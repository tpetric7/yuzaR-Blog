<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>yuzaR-Blog</title>
    <link>https://yuzar-blog.netlify.app/</link>
    <atom:link href="https://yuzar-blog.netlify.app/index.xml" rel="self" type="application/rss+xml"/>
    <description>Data Science with R
</description>
    <generator>Distill</generator>
    <lastBuildDate>Tue, 13 Dec 2022 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Quantile Regression as an useful Alternative for Ordinary Linear Regression</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-12-01-quantileregression</link>
      <description>


&lt;h1 id="this-post-as-a-video"&gt;This post as a video&lt;/h1&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk
about. It’s only 14 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/Gtz8ca_4hVg" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id="why-do-we-need-quantile-regression-qr"&gt;Why do we need Quantile
Regression (QR)?&lt;/h1&gt;
&lt;p&gt;Particularly, QR:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;is robust to outliers and influential points&lt;/li&gt;
&lt;li&gt;does not assume a constant variance (known as homoskedasticity) for
the response variable or the residuals&lt;/li&gt;
&lt;li&gt;does not assume normality&lt;/li&gt;
&lt;li&gt;but the main advantage of QR over linear regression (LR) is that QR
explores different values of the response variable, instead of only the
average, and delivers therefore a more complete picture of the
relationships between variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, let’s:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;take problematic data,&lt;/li&gt;
&lt;li&gt;build both, linear and quantile models, and see&lt;/li&gt;
&lt;li&gt;whether QR can solve problems and be a truly &lt;strong&gt;Useful
Alternative for Ordinary Linear Regression&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1
id="solve-outliers-problem-median-regression-only-5th-quantile-or-2nd-quartile"&gt;1.
Solve outliers problem: Median Regression (only 5th quaNtile, or 2nd
quaRtile)&lt;/h1&gt;
&lt;p&gt;We’ll first see how both models deal with outliers. For that we’ll
create a small data set with ONE obvious outlier and use
&lt;code&gt;geom_smooth()&lt;/code&gt; function to create a linear model and
&lt;code&gt;geom_quantile()&lt;/code&gt; function for a quick quantile regression,
with only 5th quantile, which makes it a &lt;strong&gt;median-based
regression&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# create data
library(tidyverse)
d &amp;lt;- tibble(
  predictor = c(  1,   2,   3,  4,   5,   6,   7),
  outcome   = c(1.5, 2.3, 2.8,  4.1, 5.3, 0, 6.8)
)

# plot ordinary and median regressions
ggplot(d, aes(predictor, outcome))+
  geom_point()+
  geom_smooth(method = lm, se = F,color = &amp;quot;red&amp;quot;, )+
  geom_quantile(quantiles = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-2-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;This plot shows, that linear model tries to please all points and
misses most of them, which results in a bad fit. In contrast, the Median
Regression ignores the outlier and visually fits the rest of the data
much better. But how do we know that Median Regression is indeed
better?&lt;/p&gt;
&lt;p&gt;Well, if we create an ordinary and quantile regressions, we can
compare the amount of information they loose. The Akaike’s Information
Criterion (AIC) measures such loss of information. Namely, the lower the
AIC, the better the model. Thus, a lower AIC of QR indicates a smaller
loss of information from the data, as compared to LR, making QR a better
model. Moreover, since the slope of LR is not significant, while the
slope of QR is, using a wrong model could cost you an important
discovery. So, no Nobel Price for you!&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# model median (2nd quantile) regression
lr &amp;lt;- lm(outcome ~ predictor, data = d)
library(quantreg)
mr &amp;lt;- rq(outcome ~ predictor, data = d, tau = .5)

# compare models
AIC(lr, mr) # =&amp;gt; the lower AIC the better&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   df      AIC
lr  3 34.90235
mr  2 27.09082&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sjPlot) # I made a video on this 📦
theme_set(theme_bw())
plot_models(lr, mr, show.values = TRUE, 
            m.labels = c(&amp;quot;Linear model&amp;quot;, &amp;quot;Median model&amp;quot;), 
            legend.title = &amp;quot;Model type&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;By the way, we can use &lt;code&gt;ols_plot_resid_lev()&lt;/code&gt; function
from {olsrr} package and see that we indeed have an outlier.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(olsrr)
ols_plot_resid_lev(lr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-4-1.png" width="672" /&gt;
# 2. Solve heteroscedasticity&lt;/p&gt;
&lt;p&gt;Now let’s take a real world heteroscedastic data and see whether
median regression handles it better. Engel dataset from {quantreg}
package explores the relationship between household food expenditure and
household income. Similarly to previous example, the median and mean
fits are quite different, which can be explained by the strong effect of
the two unusual points with high income and low food expenditure.
Probably just greedy people.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# get heteroscedastic data
data(engel)
ggplot(engel, aes(income, foodexp))+
  geom_point()+
  geom_smooth(method = lm, se = F, color = &amp;quot;red&amp;quot;)+
  geom_quantile(color = &amp;quot;blue&amp;quot;, quantiles = 0.5)+
  geom_quantile(color = &amp;quot;gray&amp;quot;, alpha = 0.3, 
                  quantiles = seq(.05, .95, by = 0.05))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;In order to better justify the use of QR, we can check
heteroskedasticity via Breusch-Pagan test. Our test detects
heteroscedasticity, so that we again need an alternative to linear
regression. And, a lower AIC of median-based regression again shows a
better fit, as compared to the mean-based regression.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# compare models
lr   &amp;lt;- lm(foodexp ~ income, data = engel)

library(performance) # I made a video on this 📦
check_heteroscedasticity(lr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning: Heteroscedasticity (non-constant error variance) detected (p &amp;lt; .001).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;qm50 &amp;lt;- rq(foodexp ~ income, data = engel, tau = 0.5)

AIC(lr, qm50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     df      AIC
lr    3 2897.351
qm50  2 2827.260&lt;/code&gt;&lt;/pre&gt;
&lt;h1
id="solve-not-normal-skewed-distribution-not-homogen-variances-across-groups-categorical-predictor"&gt;3.
Solve not-normal (skewed) distribution &amp;amp; not-homogen variances
across groups + categorical predictor&lt;/h1&gt;
&lt;p&gt;Now let’s see how both models handle not-normally distributed or
skewed data, and, at the same time, see how they handle categorical
predictors.&lt;/p&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-7-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;For that we’ll use a Wage dataset from {ISLR} package and model the
salary of 30 industrial and IT workers. And when we check the
assumptions of linear model, we’ll see, that our data has no outliers,
but is not-normality distributed and variances between groups differ, so
our data is again - heteroscedastic. And that’s a big problem, because
if SEVERAL assumption of a model fail, we CAN NOT trust the results of
such model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# get not-normal data
library(ISLR)
set.seed(1) # for reproducibility
salary &amp;lt;- Wage %&amp;gt;% 
  group_by(jobclass) %&amp;gt;% 
  sample_n(30)

lr &amp;lt;- lm(wage ~ jobclass, data = salary)

check_outliers(lr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;OK: No outliers detected.
- Based on the following method and threshold: cook (0.7).
- For variable: (Whole model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;check_normality(lr) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning: Non-normality of residuals detected (p &amp;lt; .001).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;check_homogeneity(lr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning: Variances differ between groups (Bartlett Test, p = 0.005).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(By the way, if we don’t specify any quantiles in quanlile
regression, the default 5th quantile or - median regression (tau = 0.5)
will be modeled.)&lt;/p&gt;
&lt;p&gt;And what are those results? Well, linear model reveals, that average
annual salary of IT workers is almost 37.000$ higher as compared to
industrial workers, and such big &lt;strong&gt;difference in means&lt;/strong&gt; is
significant. While median regression shows, that IT crowd earns only
19.6 thousand dollars more and this &lt;strong&gt;difference in
medians&lt;/strong&gt; is not significant.&lt;/p&gt;
&lt;p&gt;The lower AIC of the median regression again shows that QR performs
better then LR. So that, while in the case with outliers &lt;strong&gt;LR
missed an important discovery&lt;/strong&gt;, here &lt;strong&gt;LR discovered
nonsense&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# tau = .5 - or median regression is a default
mr &amp;lt;- rq(wage ~ jobclass, data = salary, tau = 0.5) 

plot_models(lr, mr, show.values = T, 
            m.labels = c(&amp;quot;Linear model&amp;quot;, &amp;quot;Median model&amp;quot;), 
            legend.title = &amp;quot;Model type&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-9-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;AIC(lr, mr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   df      AIC
lr  3 630.4943
mr  2 614.7647&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Such nonsense is often caused by small samples, and indeed, if we
take all 3000 workers from Wage dataset, we’ll see that both models show
significantly higher salary of IT crowd as compared with factory
workers. However, the median regression still shows a smaller difference
and a smaller AIC tells us that QR is still a better model, which makes
sense for not-normally distributed and heteroscedastic data. Now, let’s
finally get to the main advantage of QR. (halliluja)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lr &amp;lt;- lm(wage ~ jobclass, data = Wage)
mr &amp;lt;- rq(wage ~ jobclass, data = Wage)

plot_models(lr, mr, show.values = T, 
            m.labels = c(&amp;quot;Linear model&amp;quot;, &amp;quot;Median model&amp;quot;), 
            legend.title = &amp;quot;Model type&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-10-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;AIC(lr, mr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   df      AIC
lr  3 30774.50
mr  2 30248.76&lt;/code&gt;&lt;/pre&gt;
&lt;h1
id="model-more-then-just-mean-or-just-median---model-several-quantiles"&gt;4.
Model more then just mean or just median - model several quantiles&lt;/h1&gt;
&lt;pre class="r"&gt;&lt;code&gt;# model several quantiles
library(ggridges)
ggplot(Wage, aes(x = wage, y = jobclass, fill = factor(stat(quantile)))) +
  stat_density_ridges(
    geom = &amp;quot;density_ridges_gradient&amp;quot;, calc_ecdf = TRUE,
    quantile_lines = TRUE, quantiles = c(.1, .5, .9)
  ) +
  scale_fill_viridis_d(name = &amp;quot;Quantiles&amp;quot;)+
  xlab(&amp;quot;salary&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-11-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;While median regression delivers better results, the median is still
a single central location, similar to the mean. But since median
regression is a special case of QR, which uses only a 5th quantile, and
since QR can easily model other quantiles too, a QR allows you to easily
model low and high salaries! In other words, QR can be extended to
noncentral locations. Namely, if we take a low quantile, for example 0.1
instead of 0.5, we’ll model the difference between low income factory
and low income IT workers. Similarly, if we take a high quantile, for
example 0.9 instead of 0.5, we’ll be able to check the difference
between top salaries of industrial vs. top salaries of IT workers.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lr   &amp;lt;- lm(wage ~ jobclass, data = Wage)
qm10 &amp;lt;- rq(wage ~ jobclass, data = Wage, tau = 0.10)
qm50 &amp;lt;- rq(wage ~ jobclass, data = Wage, tau = 0.50)
qm90 &amp;lt;- rq(wage ~ jobclass, data = Wage, tau = 0.90)

plot_models(lr, qm10, qm50, qm90,
            show.values = TRUE,
            m.labels = c(&amp;quot;LR&amp;quot;, &amp;quot;QR 10%&amp;quot;, &amp;quot;QR 50%&amp;quot;, &amp;quot;QR 90%&amp;quot;), 
            legend.title = &amp;quot;Model type&amp;quot;)+
  ylab(&amp;quot;Increase in wage after switch to IT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-12-1.png" width="672" /&gt;
The results show, that for low salaries the difference between
industrial and IT jobs is smaller, then for median or high salaries. The
reason for that could be education, so that when your education level is
low, switching jobs from factory to IT would only increase your salary
by ca. 8.000 bucks, while when you have a college degree, changing to IT
will increase your salary by over 25.000 bucks. However, the reason
itself is not important. What is important here, is that, while ordinary
linear regression describes only an average change in salaries when we
switch from industrial to IT job, quantile regression uncovers what
happen after you switch jobs having low, median or high salary. In other
words, &lt;strong&gt;a new salary after switching jobs depends on the salary
before switching&lt;/strong&gt;, which makes sense. But what doesn’t make any
sense is that, an ordinary linear regression over-promises increase in
salary for low earners and under-promises increase in salary for high
earners. Thus, QR reveals a &lt;strong&gt;more complete picture of
reality&lt;/strong&gt;, and allows you to make a more informed decision.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ggridges)
ggplot(Wage, aes(x = wage, y = jobclass, fill = factor(stat(quantile)))) +
  stat_density_ridges(
    geom = &amp;quot;density_ridges_gradient&amp;quot;, calc_ecdf = TRUE,
    quantile_lines = TRUE, quantiles = seq(.1, .9, by = 0.1)
  ) +
  scale_fill_viridis_d(name = &amp;quot;Quantiles&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-13-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;qm20 &amp;lt;- rq(wage ~ jobclass, data = Wage, tau = 0.20)
qm30 &amp;lt;- rq(wage ~ jobclass, data = Wage, tau = 0.30)
qm70 &amp;lt;- rq(wage ~ jobclass, data = Wage, tau = 0.70)
qm80 &amp;lt;- rq(wage ~ jobclass, data = Wage, tau = 0.80)

plot_models(lr, qm10, qm20, qm30, qm50, qm70, qm80, qm90, show.values = TRUE)+
  theme(legend.position = &amp;quot;none&amp;quot;)+
  ylab(&amp;quot;Increase in wage after switch to IT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-14-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;But that is just a beginning! Because, similarly to low (tau = 0.1)
or high (tau = 0.9) quantiles, we can model more quantile to get
&lt;strong&gt;more useful inference&lt;/strong&gt;. And we can even …&lt;/p&gt;
&lt;h1
id="model-the-entire-conditional-distribution-of-salaries-via-all-possible-quantiles"&gt;5.
Model &lt;strong&gt;the entire conditional distribution&lt;/strong&gt; of salaries
via all possible quantiles&lt;/h1&gt;
&lt;p&gt;… by defining the sequence of quantiles, from let’s say 0.1 to 0.9,
and defining the step, in order to control how many quantiles we model.
For example using “by = 0.1” will model 9 quantiles from 0.1 to 0.9.&lt;/p&gt;
&lt;p&gt;Plotting the summary of our model (a quantile process plot) uncovers
how switching to IT affects &lt;strong&gt;the entire conditional
distribution&lt;/strong&gt; of salaries. The red lines show the mean effect
with confidence intervals estimated by linear regression. While shaded
gray area shows confidence intervals for the quantile regression
estimates. The non-overlapping confidence intervals between quantile and
linear regression can be seen as significant difference between models.
So that, linear regression significantly over-promises the increase in
salaries when you switch to IT for low and medium earners (if we ignore
the very small overlap from 0.3 to 0.6 quantiles), significantly
underestimates the increase in salary for top 10% earners, while
correctly describes the increase in salary for only a small part of
workers with already relatively high salaries.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;seq(0.1, 0.9, by = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;q &amp;lt;- rq(wage ~ jobclass, data = Wage, 
        tau = seq(0.1, 0.9, by = 0.1))

summary(q) %&amp;gt;% 
  plot(parm = &amp;quot;jobclass2. Information&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-15-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h1 id="multivariable-regression"&gt;6. Multivariable regression&lt;/h1&gt;
&lt;p&gt;So, I think a univariable QR is already much more useful then LR. But
that’s not all, multivariable QR is even more useful, because it can
uncover which variables are important for low or for high values of the
response variable.&lt;/p&gt;
&lt;p&gt;Let’s have a look at two multivariable examples.&lt;/p&gt;
&lt;h2 id="american-salaries"&gt;1) American salaries&lt;/h2&gt;
&lt;p&gt;In the first example we’ll continue to model salaries, but instead of
only a “jobclass” predictor, we’ll add “age” and “race” predictors.&lt;/p&gt;
&lt;p&gt;Let’s interpret the influence of “age” on salary first. The young low
earners would significantly increase their salaries as they age, because
y-axis, which shows the slope of this increase, is positive and does not
include zero. However, this realistic increase over lifetime is
significantly smaller then average, promised by the linear regression,
because red and gray confidence intervals don’t overlap. The young high
earners have much higher slope, meaning much stronger increase in salary
over lifetime, which was significantly underestimated by the linear
regression. Here again, high educational degree could cause young people
to earn a lot of money already in the beginning of their lives, and
opens better chances to increase the salary over lifetime.&lt;/p&gt;
&lt;p&gt;The interpretation of the categorical predictor “race” is even more
interesting. Since “White” people are the intercept, “Black”- and
“Asian-Americans” can be compared to “White” Americans. Here, linear
regression shows that on average for low income folks, Black people earn
significantly less then White people, because the coefficient is
negative and does not cross the zero, which is wrong. Because, in
reality, since gray confidence intervals cross the zero, there is no
significant difference between White and Black folks with low income. In
contrast, when salaries are high, Black workers earn significantly less
then White workers, even when they earn millions.&lt;/p&gt;
&lt;p&gt;The wages of Asian Americans show the opposite. Namely, while linear
regression mistakenly predicts that Asian folks get significantly more
then White folks, independently of their salary, QR shows that low
income Asian people earn significantly less or similar to White
people.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# multivariable regression
q &amp;lt;- rq(wage ~ jobclass + age + race, data = Wage, 
        tau = seq(.05, .95, by = 0.05))

summary(q) %&amp;gt;% 
  plot(c(&amp;quot;jobclass2. Information&amp;quot;, &amp;quot;age&amp;quot;, &amp;quot;race2. Black&amp;quot;, &amp;quot;race3. Asian&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-16-1.png" width="672" /&gt;
Since, in all of the panels of the plot, the quantile regression
estimates lie at some point outside the confidence intervals for the
ordinary least squares regression, we can conclude that the effects of
“jobclass”, “age” and “race” are not constant across salaries, but
depends on a height of the salary.&lt;/p&gt;
&lt;p&gt;And if that’s not enough, you can go one step further and conduct a
…&lt;/p&gt;
&lt;h3 id="nonparametric-non-linear-quantile-regression"&gt;Nonparametric
non-linear quantile regression&lt;/h3&gt;
&lt;p&gt;… for numeric predictors using {quantregGrowth} package. But before
you do that, have a look at the last example where we check the
influence of 5 predictors on the efficiency of cars.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# non-linear quantile regression
library(quantregGrowth)
set.seed(1)
o &amp;lt;-gcrq(wage ~ ps(age), 
         data = Wage %&amp;gt;% sample_n(100), tau=seq(.10,.90,l=3))

# par(mfrow=c(1,2)) # for several plots
plot(o, legend=TRUE, conf.level = .95, shade=TRUE, lty = 1, lwd = 3, col = -1, res=TRUE) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-17-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2 id="efficiency-of-cars"&gt;2) Efficiency of cars&lt;/h2&gt;
&lt;p&gt;Here, a linear regression will answer the question - which variables
affect the average car mileage? A low quantile of 0.1 will tell us which
predictors are important for not efficient cars, which drive only a few
miles per gallon of gas. A high quantile of 0.9 will tell us which
predictors are important for highly efficient cars, which drive a lot of
miles per gallon of gas. We’ll also conduct a median regression in order
to compare it to LR and for a &lt;strong&gt;more complete presentation of the
results&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let’s start with that. The negative coefficient of “horsepower”
indicates significant decrease in efficiency of cars with increasing
horsepower. Both, mean-based and median-based models agree on that.
However, while linear regression reports “Engine displacement” to be
not-important for efficiency, median regression shows that it is
important. Moreover, quantile regression reports that increasing
acceleration significantly reduces mileage of not-efficient cars and has
no effect on highly efficient cars, while linear regression can’s say
anything about low or highly efficient cars.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cars &amp;lt;- Auto %&amp;gt;% 
  select(mpg, cylinders, displacement, horsepower, acceleration, origin)

l   &amp;lt;- lm(mpg ~ ., data = cars)
q10 &amp;lt;- rq(mpg ~ ., data = cars, tau = .1)
q50 &amp;lt;- rq(mpg ~ ., data = cars, tau = .5)
q90 &amp;lt;- rq(mpg ~ ., data = cars, tau = .9)

library(gtsummary) # I made a video on this 📦
tbl_merge(
    tbls = list(
      tbl_regression(l) %&amp;gt;% bold_p(),
      tbl_regression(q10, se = &amp;quot;nid&amp;quot;) %&amp;gt;% bold_p(), 
      tbl_regression(q50, se = &amp;quot;nid&amp;quot;) %&amp;gt;% bold_p(),
      tbl_regression(q90, se = &amp;quot;nid&amp;quot;) %&amp;gt;% bold_p()
),
    tab_spanner = c(&amp;quot;OLS&amp;quot;, &amp;quot;QR 10%&amp;quot;, &amp;quot;QR 50%&amp;quot;, &amp;quot;QR 90%&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;div id="iluhdqiuau" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;"&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#iluhdqiuau .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#iluhdqiuau .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#iluhdqiuau .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#iluhdqiuau .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#iluhdqiuau .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#iluhdqiuau .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#iluhdqiuau .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#iluhdqiuau .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#iluhdqiuau .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#iluhdqiuau .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#iluhdqiuau .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#iluhdqiuau .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#iluhdqiuau .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#iluhdqiuau .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#iluhdqiuau .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#iluhdqiuau .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#iluhdqiuau .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#iluhdqiuau .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#iluhdqiuau .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#iluhdqiuau .gt_row_group_first td {
  border-top-width: 2px;
}

#iluhdqiuau .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#iluhdqiuau .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#iluhdqiuau .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#iluhdqiuau .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#iluhdqiuau .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#iluhdqiuau .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#iluhdqiuau .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#iluhdqiuau .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#iluhdqiuau .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#iluhdqiuau .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-left: 4px;
  padding-right: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#iluhdqiuau .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#iluhdqiuau .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#iluhdqiuau .gt_left {
  text-align: left;
}

#iluhdqiuau .gt_center {
  text-align: center;
}

#iluhdqiuau .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#iluhdqiuau .gt_font_normal {
  font-weight: normal;
}

#iluhdqiuau .gt_font_bold {
  font-weight: bold;
}

#iluhdqiuau .gt_font_italic {
  font-style: italic;
}

#iluhdqiuau .gt_super {
  font-size: 65%;
}

#iluhdqiuau .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 75%;
  vertical-align: 0.4em;
}

#iluhdqiuau .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#iluhdqiuau .gt_indent_1 {
  text-indent: 5px;
}

#iluhdqiuau .gt_indent_2 {
  text-indent: 10px;
}

#iluhdqiuau .gt_indent_3 {
  text-indent: 15px;
}

#iluhdqiuau .gt_indent_4 {
  text-indent: 20px;
}

#iluhdqiuau .gt_indent_5 {
  text-indent: 25px;
}
&lt;/style&gt;
&lt;table class="gt_table"&gt;
  
  &lt;thead class="gt_col_headings"&gt;
    &lt;tr&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="2" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;Characteristic&amp;lt;/strong&amp;gt;"&gt;&lt;strong&gt;Characteristic&lt;/strong&gt;&lt;/th&gt;
      &lt;th class="gt_center gt_columns_top_border gt_column_spanner_outer" rowspan="1" colspan="3" scope="colgroup" id="OLS"&gt;
        &lt;span class="gt_column_spanner"&gt;OLS&lt;/span&gt;
      &lt;/th&gt;
      &lt;th class="gt_center gt_columns_top_border gt_column_spanner_outer" rowspan="1" colspan="3" scope="colgroup" id="QR 10%"&gt;
        &lt;span class="gt_column_spanner"&gt;QR 10%&lt;/span&gt;
      &lt;/th&gt;
      &lt;th class="gt_center gt_columns_top_border gt_column_spanner_outer" rowspan="1" colspan="3" scope="colgroup" id="QR 50%"&gt;
        &lt;span class="gt_column_spanner"&gt;QR 50%&lt;/span&gt;
      &lt;/th&gt;
      &lt;th class="gt_center gt_columns_top_border gt_column_spanner_outer" rowspan="1" colspan="3" scope="colgroup" id="QR 90%"&gt;
        &lt;span class="gt_column_spanner"&gt;QR 90%&lt;/span&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;Beta&amp;lt;/strong&amp;gt;"&gt;&lt;strong&gt;Beta&lt;/strong&gt;&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;95% CI&amp;lt;/strong&amp;gt;&amp;lt;sup class=&amp;quot;gt_footnote_marks&amp;quot;&amp;gt;1&amp;lt;/sup&amp;gt;"&gt;&lt;strong&gt;95% CI&lt;/strong&gt;&lt;sup class="gt_footnote_marks"&gt;1&lt;/sup&gt;&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;p-value&amp;lt;/strong&amp;gt;"&gt;&lt;strong&gt;p-value&lt;/strong&gt;&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;Beta&amp;lt;/strong&amp;gt;"&gt;&lt;strong&gt;Beta&lt;/strong&gt;&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;95% CI&amp;lt;/strong&amp;gt;&amp;lt;sup class=&amp;quot;gt_footnote_marks&amp;quot;&amp;gt;1&amp;lt;/sup&amp;gt;"&gt;&lt;strong&gt;95% CI&lt;/strong&gt;&lt;sup class="gt_footnote_marks"&gt;1&lt;/sup&gt;&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;p-value&amp;lt;/strong&amp;gt;"&gt;&lt;strong&gt;p-value&lt;/strong&gt;&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;Beta&amp;lt;/strong&amp;gt;"&gt;&lt;strong&gt;Beta&lt;/strong&gt;&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;95% CI&amp;lt;/strong&amp;gt;&amp;lt;sup class=&amp;quot;gt_footnote_marks&amp;quot;&amp;gt;1&amp;lt;/sup&amp;gt;"&gt;&lt;strong&gt;95% CI&lt;/strong&gt;&lt;sup class="gt_footnote_marks"&gt;1&lt;/sup&gt;&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;p-value&amp;lt;/strong&amp;gt;"&gt;&lt;strong&gt;p-value&lt;/strong&gt;&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;Beta&amp;lt;/strong&amp;gt;"&gt;&lt;strong&gt;Beta&lt;/strong&gt;&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;95% CI&amp;lt;/strong&amp;gt;&amp;lt;sup class=&amp;quot;gt_footnote_marks&amp;quot;&amp;gt;1&amp;lt;/sup&amp;gt;"&gt;&lt;strong&gt;95% CI&lt;/strong&gt;&lt;sup class="gt_footnote_marks"&gt;1&lt;/sup&gt;&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&amp;lt;strong&amp;gt;p-value&amp;lt;/strong&amp;gt;"&gt;&lt;strong&gt;p-value&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class="gt_table_body"&gt;
    &lt;tr&gt;&lt;td headers="label" class="gt_row gt_left"&gt;cylinders&lt;/td&gt;
&lt;td headers="estimate_1" class="gt_row gt_center"&gt;-0.88&lt;/td&gt;
&lt;td headers="ci_1" class="gt_row gt_center"&gt;-1.7, -0.05&lt;/td&gt;
&lt;td headers="p.value_1" class="gt_row gt_center" style="font-weight: bold;"&gt;0.037&lt;/td&gt;
&lt;td headers="estimate_2" class="gt_row gt_center"&gt;-0.29&lt;/td&gt;
&lt;td headers="ci_2" class="gt_row gt_center"&gt;-1.1, 0.51&lt;/td&gt;
&lt;td headers="p.value_2" class="gt_row gt_center"&gt;0.5&lt;/td&gt;
&lt;td headers="estimate_3" class="gt_row gt_center"&gt;-1.2&lt;/td&gt;
&lt;td headers="ci_3" class="gt_row gt_center"&gt;-1.8, -0.53&lt;/td&gt;
&lt;td headers="p.value_3" class="gt_row gt_center" style="font-weight: bold;"&gt;&lt;0.001&lt;/td&gt;
&lt;td headers="estimate_4" class="gt_row gt_center"&gt;-2.3&lt;/td&gt;
&lt;td headers="ci_4" class="gt_row gt_center"&gt;-3.2, -1.3&lt;/td&gt;
&lt;td headers="p.value_4" class="gt_row gt_center" style="font-weight: bold;"&gt;&lt;0.001&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td headers="label" class="gt_row gt_left"&gt;displacement&lt;/td&gt;
&lt;td headers="estimate_1" class="gt_row gt_center"&gt;-0.01&lt;/td&gt;
&lt;td headers="ci_1" class="gt_row gt_center"&gt;-0.03, 0.01&lt;/td&gt;
&lt;td headers="p.value_1" class="gt_row gt_center"&gt;0.3&lt;/td&gt;
&lt;td headers="estimate_2" class="gt_row gt_center"&gt;-0.02&lt;/td&gt;
&lt;td headers="ci_2" class="gt_row gt_center"&gt;-0.03, -0.01&lt;/td&gt;
&lt;td headers="p.value_2" class="gt_row gt_center" style="font-weight: bold;"&gt;&lt;0.001&lt;/td&gt;
&lt;td headers="estimate_3" class="gt_row gt_center"&gt;-0.01&lt;/td&gt;
&lt;td headers="ci_3" class="gt_row gt_center"&gt;-0.03, 0.00&lt;/td&gt;
&lt;td headers="p.value_3" class="gt_row gt_center" style="font-weight: bold;"&gt;0.037&lt;/td&gt;
&lt;td headers="estimate_4" class="gt_row gt_center"&gt;-0.01&lt;/td&gt;
&lt;td headers="ci_4" class="gt_row gt_center"&gt;-0.03, 0.01&lt;/td&gt;
&lt;td headers="p.value_4" class="gt_row gt_center"&gt;0.6&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td headers="label" class="gt_row gt_left"&gt;horsepower&lt;/td&gt;
&lt;td headers="estimate_1" class="gt_row gt_center"&gt;-0.11&lt;/td&gt;
&lt;td headers="ci_1" class="gt_row gt_center"&gt;-0.14, -0.08&lt;/td&gt;
&lt;td headers="p.value_1" class="gt_row gt_center" style="font-weight: bold;"&gt;&lt;0.001&lt;/td&gt;
&lt;td headers="estimate_2" class="gt_row gt_center"&gt;-0.07&lt;/td&gt;
&lt;td headers="ci_2" class="gt_row gt_center"&gt;-0.12, -0.03&lt;/td&gt;
&lt;td headers="p.value_2" class="gt_row gt_center" style="font-weight: bold;"&gt;0.003&lt;/td&gt;
&lt;td headers="estimate_3" class="gt_row gt_center"&gt;-0.07&lt;/td&gt;
&lt;td headers="ci_3" class="gt_row gt_center"&gt;-0.10, -0.05&lt;/td&gt;
&lt;td headers="p.value_3" class="gt_row gt_center" style="font-weight: bold;"&gt;&lt;0.001&lt;/td&gt;
&lt;td headers="estimate_4" class="gt_row gt_center"&gt;-0.07&lt;/td&gt;
&lt;td headers="ci_4" class="gt_row gt_center"&gt;-0.10, -0.05&lt;/td&gt;
&lt;td headers="p.value_4" class="gt_row gt_center" style="font-weight: bold;"&gt;&lt;0.001&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td headers="label" class="gt_row gt_left"&gt;acceleration&lt;/td&gt;
&lt;td headers="estimate_1" class="gt_row gt_center"&gt;-0.39&lt;/td&gt;
&lt;td headers="ci_1" class="gt_row gt_center"&gt;-0.61, -0.17&lt;/td&gt;
&lt;td headers="p.value_1" class="gt_row gt_center" style="font-weight: bold;"&gt;&lt;0.001&lt;/td&gt;
&lt;td headers="estimate_2" class="gt_row gt_center"&gt;-0.44&lt;/td&gt;
&lt;td headers="ci_2" class="gt_row gt_center"&gt;-0.69, -0.18&lt;/td&gt;
&lt;td headers="p.value_2" class="gt_row gt_center" style="font-weight: bold;"&gt;&lt;0.001&lt;/td&gt;
&lt;td headers="estimate_3" class="gt_row gt_center"&gt;-0.54&lt;/td&gt;
&lt;td headers="ci_3" class="gt_row gt_center"&gt;-0.73, -0.34&lt;/td&gt;
&lt;td headers="p.value_3" class="gt_row gt_center" style="font-weight: bold;"&gt;&lt;0.001&lt;/td&gt;
&lt;td headers="estimate_4" class="gt_row gt_center"&gt;-0.11&lt;/td&gt;
&lt;td headers="ci_4" class="gt_row gt_center"&gt;-0.52, 0.31&lt;/td&gt;
&lt;td headers="p.value_4" class="gt_row gt_center"&gt;0.6&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td headers="label" class="gt_row gt_left"&gt;origin&lt;/td&gt;
&lt;td headers="estimate_1" class="gt_row gt_center"&gt;1.7&lt;/td&gt;
&lt;td headers="ci_1" class="gt_row gt_center"&gt;0.96, 2.4&lt;/td&gt;
&lt;td headers="p.value_1" class="gt_row gt_center" style="font-weight: bold;"&gt;&lt;0.001&lt;/td&gt;
&lt;td headers="estimate_2" class="gt_row gt_center"&gt;-0.08&lt;/td&gt;
&lt;td headers="ci_2" class="gt_row gt_center"&gt;-1.1, 0.89&lt;/td&gt;
&lt;td headers="p.value_2" class="gt_row gt_center"&gt;0.9&lt;/td&gt;
&lt;td headers="estimate_3" class="gt_row gt_center"&gt;2.1&lt;/td&gt;
&lt;td headers="ci_3" class="gt_row gt_center"&gt;1.1, 3.0&lt;/td&gt;
&lt;td headers="p.value_3" class="gt_row gt_center" style="font-weight: bold;"&gt;&lt;0.001&lt;/td&gt;
&lt;td headers="estimate_4" class="gt_row gt_center"&gt;1.9&lt;/td&gt;
&lt;td headers="ci_4" class="gt_row gt_center"&gt;0.72, 3.1&lt;/td&gt;
&lt;td headers="p.value_4" class="gt_row gt_center" style="font-weight: bold;"&gt;0.002&lt;/td&gt;&lt;/tr&gt;
  &lt;/tbody&gt;
  
  &lt;tfoot class="gt_footnotes"&gt;
    &lt;tr&gt;
      &lt;td class="gt_footnote" colspan="13"&gt;&lt;sup class="gt_footnote_marks"&gt;1&lt;/sup&gt; CI = Confidence Interval&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tfoot&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;se = "nid"&lt;/code&gt; argument produces 95% confidence
intervals and p-values, which allows to build this useful table. And if
you want to learn how to produce similar publication ready tables for
data summaries, results of statistical tests or models, check out my
video on {gtsummary} package.&lt;/p&gt;
&lt;h1 id="some-further-useful-things"&gt;7. Some further useful things&lt;/h1&gt;
&lt;h2 id="confidence-intervals"&gt;Confidence intervals&lt;/h2&gt;
&lt;p&gt;There are several ways to compute confidence intervals for quantile
regression. This can be specified using the &lt;code&gt;"se ="&lt;/code&gt; option
in the &lt;code&gt;summary()&lt;/code&gt; or &lt;code&gt;tbl_regression()&lt;/code&gt;
functions. The default value is &lt;code&gt;se="rank"&lt;/code&gt;, however, it does
not deliver p-values, while other options “nid”, “iid” (not good), “ker”
and “boot” do (type &lt;code&gt;?summary.rq&lt;/code&gt; for details). However,
using “boot” is recommended only with large data-sets.&lt;/p&gt;
&lt;h2 id="equality-of-slopes"&gt;Equality of slopes&lt;/h2&gt;
&lt;p&gt;Khmaladze [1981] introduced the tests of equality of slopes across
quantiles. Or &lt;code&gt;anova()&lt;/code&gt; can compare two (better) or more
slopes.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;KhmaladzeTest(wage ~ jobclass, data = Wage, 
              tau = seq(.05, .95, by = 0.05))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Test of H_0: location 

Joint Test Statistic: 0.06793593 

Component Test Statistics: 0.06793593 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(qm10, qm50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Quantile Regression Analysis of Deviance Table

Model: wage ~ jobclass
Joint Test of Equality of Slopes: tau in {  0.1 0.5  }

  Df Resid Df F value   Pr(&amp;gt;F)   
1  1     5999  6.6577 0.009896 **
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(qm20, qm30, qm50, qm70)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Quantile Regression Analysis of Deviance Table

Model: wage ~ jobclass
Joint Test of Equality of Slopes: tau in {  0.2 0.3 0.5 0.7  }

  Df Resid Df F value Pr(&amp;gt;F)
1  3    11997  0.8401 0.4717&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="speed-up-the-model"&gt;Speed up the model&lt;/h2&gt;
&lt;p&gt;The default calculation method is &lt;code&gt;method = "br"&lt;/code&gt;. For
more than a few thousand observations it is worthwhile considering
&lt;code&gt;method = "fn"&lt;/code&gt;. For extremely large data sets use
&lt;code&gt;method = "pfn"&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="contrasts-in-median-regression"&gt;Contrasts in median
regression&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;mr &amp;lt;- rq(wage ~ education, data = Wage, tau = 0.5)

emmeans::emmeans(mr, pairwise ~ education, weights = &amp;quot;prop&amp;quot;, adjust = &amp;quot;bonferroni&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$emmeans
 education          emmean   SE   df lower.CL upper.CL
 1. &amp;lt; HS Grad         81.3 1.79 2995     77.8     84.8
 2. HS Grad           94.1 1.13 2995     91.9     96.3
 3. Some College     104.9 1.34 2995    102.3    107.5
 4. College Grad     118.9 1.79 2995    115.4    122.4
 5. Advanced Degree  141.8 2.46 2995    136.9    146.6

Confidence level used: 0.95 

$contrasts
 contrast                             estimate   SE   df t.ratio
 1. &amp;lt; HS Grad - 2. HS Grad               -12.8 2.12 2995  -6.043
 1. &amp;lt; HS Grad - 3. Some College          -23.6 2.23 2995 -10.589
 1. &amp;lt; HS Grad - 4. College Grad          -37.6 2.53 2995 -14.853
 1. &amp;lt; HS Grad - 5. Advanced Degree       -60.5 3.04 2995 -19.867
 2. HS Grad - 3. Some College            -10.8 1.75 2995  -6.191
 2. HS Grad - 4. College Grad            -24.8 2.12 2995 -11.702
 2. HS Grad - 5. Advanced Degree         -47.7 2.71 2995 -17.585
 3. Some College - 4. College Grad       -14.0 2.24 2995  -6.244
 3. Some College - 5. Advanced Degree    -36.9 2.80 2995 -13.143
 4. College Grad - 5. Advanced Degree    -22.9 3.05 2995  -7.511
 p.value
  &amp;lt;.0001
  &amp;lt;.0001
  &amp;lt;.0001
  &amp;lt;.0001
  &amp;lt;.0001
  &amp;lt;.0001
  &amp;lt;.0001
  &amp;lt;.0001
  &amp;lt;.0001
  &amp;lt;.0001

P value adjustment: bonferroni method for 10 tests &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_model(mr, type = &amp;quot;pred&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$education&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-20-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2 id="median-regression-with-interactions"&gt;Median regression with
interactions&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;mr &amp;lt;- rq(wage ~ education*jobclass, data = Wage, tau = 0.5)

emmeans::emmeans(mr, pairwise ~ jobclass|education, weights = &amp;quot;prop&amp;quot;, adjust = &amp;quot;fdr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$emmeans
education = 1. &amp;lt; HS Grad:
 jobclass       emmean   SE   df lower.CL upper.CL
 1. Industrial    81.3 1.60 2990     78.2     84.4
 2. Information   86.7 3.31 2990     80.2     93.2

education = 2. HS Grad:
 jobclass       emmean   SE   df lower.CL upper.CL
 1. Industrial    93.5 1.07 2990     91.4     95.6
 2. Information   95.2 2.07 2990     91.2     99.3

education = 3. Some College:
 jobclass       emmean   SE   df lower.CL upper.CL
 1. Industrial   102.9 1.65 2990     99.6    106.1
 2. Information  105.9 1.69 2990    102.6    109.2

education = 4. College Grad:
 jobclass       emmean   SE   df lower.CL upper.CL
 1. Industrial   118.9 2.83 2990    113.3    124.4
 2. Information  118.9 2.31 2990    114.3    123.4

education = 5. Advanced Degree:
 jobclass       emmean   SE   df lower.CL upper.CL
 1. Industrial   134.7 6.82 2990    121.3    148.1
 2. Information  141.8 2.83 2990    136.2    147.3

Confidence level used: 0.95 

$contrasts
education = 1. &amp;lt; HS Grad:
 contrast                       estimate   SE   df t.ratio p.value
 1. Industrial - 2. Information    -5.41 3.68 2990  -1.472  0.1412

education = 2. HS Grad:
 contrast                       estimate   SE   df t.ratio p.value
 1. Industrial - 2. Information    -1.74 2.33 2990  -0.750  0.4536

education = 3. Some College:
 contrast                       estimate   SE   df t.ratio p.value
 1. Industrial - 2. Information    -3.06 2.36 2990  -1.294  0.1959

education = 4. College Grad:
 contrast                       estimate   SE   df t.ratio p.value
 1. Industrial - 2. Information     0.00 3.66 2990   0.000  1.0000

education = 5. Advanced Degree:
 contrast                       estimate   SE   df t.ratio p.value
 1. Industrial - 2. Information    -7.07 7.38 2990  -0.958  0.3383&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_model(mr, type = &amp;quot;int&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-21-1.png" width="672" /&gt;
## {lqmm} package: Fitting Linear Quantile Mixed Models&lt;/p&gt;
&lt;h3 id="random-intercept-model"&gt;Random intercept model&lt;/h3&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(lqmm)
data(Orthodont)
rim &amp;lt;- lqmm(distance ~ age, random = ~ 1, group = Subject,
tau = c(0.1,0.5,0.9), data = Orthodont)
summary(rim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Call: lqmm(fixed = distance ~ age, random = ~1, group = Subject, tau = c(0.1, 
    0.5, 0.9), data = Orthodont)

tau = 0.1

Fixed effects:
                Value Std. Error lower bound upper bound  Pr(&amp;gt;|t|)
(Intercept) 16.733609   0.774097   15.178002     18.2892 &amp;lt; 2.2e-16
age          0.522199   0.070113    0.381301      0.6631 1.346e-09
               
(Intercept) ***
age         ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

tau = 0.5

Fixed effects:
                Value Std. Error lower bound upper bound  Pr(&amp;gt;|t|)
(Intercept) 16.811968   0.768394   15.267822     18.3561 &amp;lt; 2.2e-16
age          0.618802   0.076946    0.464174      0.7734 1.648e-10
               
(Intercept) ***
age         ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

tau = 0.9

Fixed effects:
                Value Std. Error lower bound upper bound  Pr(&amp;gt;|t|)
(Intercept) 16.826789   0.769971   15.279475      18.374 &amp;lt; 2.2e-16
age          0.796190   0.085502    0.624368       0.968 2.031e-12
               
(Intercept) ***
age         ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

AIC:
[1] 476.0 (df = 4) 432.2 (df = 4) 485.4 (df = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="random-slope-model"&gt;Random slope model&lt;/h3&gt;
&lt;pre class="r"&gt;&lt;code&gt;rsm &amp;lt;- lqmm(distance ~ age, random = ~ age, group = Subject,
tau = c(0.1,0.5,0.9), cov = &amp;quot;pdDiag&amp;quot;, data = Orthodont)
summary(rsm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Call: lqmm(fixed = distance ~ age, random = ~age, group = Subject, 
    covariance = &amp;quot;pdDiag&amp;quot;, tau = c(0.1, 0.5, 0.9), data = Orthodont)

tau = 0.1

Fixed effects:
               Value Std. Error lower bound upper bound  Pr(&amp;gt;|t|)    
(Intercept) 16.74285    0.57417    15.58901     17.8967 &amp;lt; 2.2e-16 ***
age          0.57087    0.14996     0.26951      0.8722 0.0003921 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

tau = 0.5

Fixed effects:
               Value Std. Error lower bound upper bound  Pr(&amp;gt;|t|)    
(Intercept) 16.76179    0.56423    15.62792     17.8957 &amp;lt; 2.2e-16 ***
age          0.65987    0.12215     0.41440      0.9053 1.922e-06 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

tau = 0.9

Fixed effects:
               Value Std. Error lower bound upper bound  Pr(&amp;gt;|t|)    
(Intercept) 16.79738    0.56463    15.66271     17.9320 &amp;lt; 2.2e-16 ***
age          0.62056    0.12088     0.37764      0.8635 4.884e-06 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

AIC:
[1] 529.0 (df = 5) 510.9 (df = 5) 475.0 (df = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="final-thoughs"&gt;Final thoughs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the QR can be applied in any case where relationships for
different levels of response variable are needed to be addressed
differently&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the more data you have, the more details QR can capture from the
conditional distribution of response&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;splitting a sample into several small dataset (low values of
outcome, high values of the outcome) and using LR on them reduces
statistical power. Besides, the results could differ depending on where
the cut point (e.g. for low values) is set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the interquantile range can be easily modeled and plotted with QR
(i.e., .25, .50, .75), like a &lt;strong&gt;fancy box-plot for continuous
variables&lt;/strong&gt; :)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(1)
o &amp;lt;-gcrq(wage ~ ps(age), 
         data = Wage %&amp;gt;% sample_n(1000), tau=seq(.25,.75,l=3))

# par(mfrow=c(1,2)) # for several plots
plot(o, legend=TRUE, conf.level = .95, shade=TRUE, lty = 1, lwd = 3, col = -1, res=TRUE) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c4ab6233_files/figure-html/unnamed-chunk-24-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h1 id="references-and-further-readings"&gt;References and further
readings&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The best introduction on QR!!! &lt;a
href="https://nguyenvantien0405.files.wordpress.com/2014/03/quantile_regressiolingxin-hao.pdf"
class="uri"&gt;https://nguyenvantien0405.files.wordpress.com/2014/03/quantile_regressiolingxin-hao.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I loved this paper too! But, be careful about their
interpretation using “gap”, it is confusing and might be incorrect, as
shown in the next reference: &lt;a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4166511/pdf/nihms529550.pdf"
class="uri"&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4166511/pdf/nihms529550.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Commentary to the reference above with some corrections, among
which the most important one - is that we can interpret the coefficients
of QR as we do with OLS (page 9): &lt;a
href="https://srcd.onlinelibrary.wiley.com/doi/10.1111/cdev.13141"
class="uri"&gt;https://srcd.onlinelibrary.wiley.com/doi/10.1111/cdev.13141&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf"
class="uri"&gt;http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="http://www.econ.uiuc.edu/~roger/research/rq/QReco.pdf"
class="uri"&gt;http://www.econ.uiuc.edu/~roger/research/rq/QReco.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>98fde0a80ea80a621f42e465d8b56a72</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <category>models</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-12-01-quantileregression</guid>
      <pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-12-01-quantileregression/thumbnail_quantile_regression.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Robust Regression (don't depend on influential data!)</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-09-02-robustregression</link>
      <description>


&lt;h1 id="this-post-as-a-video"&gt;This post as a video&lt;/h1&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk
about. It’s less then 5 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/M_7MOkAm9WU" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src="file14d3c38340cbc_files/figure-html/unnamed-chunk-2-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h1 id="make-sure-you-have-unusual-data"&gt;Make sure you have unusual
data&lt;/h1&gt;
&lt;p&gt;For the sake of simplicity let’s take only 5 observations with one
obvious outlier.&lt;/p&gt;
&lt;p&gt;First of all, how do we know that we have influential observations?
Well, plotting the raw data sometimes helps, but if you have a lot of
data and many predictors, &lt;strong&gt;the best way to find unusual data
is&lt;/strong&gt; to conduct a linear regression and to run &lt;strong&gt;residuals
diagnostics&lt;/strong&gt;. &lt;code&gt;ols_plot_resid_lev()&lt;/code&gt; function from
{olsrr} package displays all contaminations of data on a single plot. In
our case it finds observation 4 to be an obvious outlier.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 1. create data
library(tidyverse)
d &amp;lt;- tibble(
  predictor = c(  1,   2,   3,  4,   5),
  outcome   = c(0.8, 2.3, 2.8,  0, 5.3)
)

plot(d)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c38340cbc_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 2. run ordinary least squares regression
m &amp;lt;- lm(outcome ~ predictor, data= d)

# 3.make residual diagnostics
library(olsrr)
ols_plot_resid_lev(m)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c38340cbc_files/figure-html/unnamed-chunk-3-2.png" width="672" /&gt;&lt;/p&gt;
&lt;h1 id="perform-robust-regression"&gt;Perform robust regression&lt;/h1&gt;
&lt;p&gt;Now, we’ll use &lt;code&gt;lmrob()&lt;/code&gt; function form {robustbase}
package to conduct the robust regression and have a look at the
&lt;code&gt;summary()&lt;/code&gt; and residuals plot of the model, because they
explain how robust regression actually works.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 4. conduct robust regression
library(robustbase)
rm &amp;lt;- lmrob(outcome ~ predictor, data = d)

summary(rm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lmrob(formula = outcome ~ predictor, data = d)
 \--&amp;gt; method = &amp;quot;MM&amp;quot;
Residuals:
       1        2        3        4        5 
-0.09832  0.31533 -0.27102 -4.15738  0.05627 

Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept) -0.18804    0.18312  -1.027     0.38    
predictor    1.08635    0.03763  28.873 9.12e-05 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Robust residual standard error: 0.6541 
Multiple R-squared:  0.9791,    Adjusted R-squared:  0.9721 
Convergence in 5 IRWLS iterations

Robustness weights: 
     1      2      3      4      5 
0.9979 0.9789 0.9844 0.0000 0.9993 
Algorithmic parameters: 
       tuning.chi                bb        tuning.psi 
        1.548e+00         5.000e-01         4.685e+00 
       refine.tol           rel.tol         scale.tol 
        1.000e-07         1.000e-07         1.000e-10 
        solve.tol       eps.outlier             eps.x 
        1.000e-07         2.000e-02         9.095e-12 
warn.limit.reject warn.limit.meanrw 
        5.000e-01         5.000e-01 
     nResample         max.it       best.r.s       k.fast.s 
           500             50              2              1 
         k.max    maxit.scale      trace.lev            mts 
           200            200              0           1000 
    compute.rd fast.s.large.n 
             0           2000 
                  psi           subsampling                   cov 
           &amp;quot;bisquare&amp;quot;         &amp;quot;nonsingular&amp;quot;         &amp;quot;.vcov.avar1&amp;quot; 
compute.outlier.stats 
                 &amp;quot;SM&amp;quot; 
seed : int(0) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sjPlot)
plot_residuals(m)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c38340cbc_files/figure-html/unnamed-chunk-4-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h1 id="how-and-why-does-robust-regression-works"&gt;How and why does
robust regression works?&lt;/h1&gt;
&lt;p&gt;Namely, a robust regression gives different &lt;strong&gt;robustness
weights&lt;/strong&gt;, from 0 to 1, to every observation based on it’s
residual. Where a residual is simply the differences between observed
and predicted values of data. So, the smaller the residual, the larger
the weight. For example, observations 1 and 3 have the smallest
residuals and therefore the highest weight, which means - they have the
strongest influence on our model. And while all observations with a
non-zero residual get down-weighted at least a little, our outlier gets
down-weighting the most … to … actually zero, so that our outlier has
zero influence on our model, which in fact makes our model ROBUST.&lt;/p&gt;
&lt;p&gt;The assignment of weight happens by &lt;strong&gt;Iteratively ReWeighting
Least Squares (IRWLS)&lt;/strong&gt;, thus we have to make sure, robust
regression algorithm converged. In our case, the model converged in only
a few iterations.&lt;/p&gt;
&lt;p&gt;But why don’t we just remove outliers and run a normal linear model,
right? Well, in most of the cases it’s a bad idea, because we’ll loose
information. For example in our case of 5 observations, we’d loose 20%
of data. In contrast, robust regression still squizzes some knowledge
out of unusual data, but lowers their weight which does not let unusual
data to influence our regression to much. Now lets …&lt;/p&gt;
&lt;h1 id="compare-both-models-and-choose-the-best"&gt;Compare both models and
choose the best&lt;/h1&gt;
&lt;p&gt;First, &lt;code&gt;plot_model()&lt;/code&gt; command from {sjPlot} package easily
visualizes predictions of both models.&lt;/p&gt;
&lt;p&gt;The ordinary linear model shows no trend. However, the absence of a
trend may only be caused by the outlier N°4, which drags the line down
and widens the confidence intervals, making us less confident in our
results. In contrast, a robust regression ignores the outlier and shows
a clear trend with a narrow confidence intervals.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 5. visualize both models
plot_model(m,  type = &amp;quot;pred&amp;quot;, show.data = T)
plot_model(rm, type = &amp;quot;pred&amp;quot;, show.data = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Moreover, &lt;code&gt;tab_model()&lt;/code&gt; command from {sjPlot} package
shows that a robust model has much higher coefficient of determination
&lt;span class="math inline"&gt;\(R^2\)&lt;/span&gt;, which means that robust model
fits the data much better then the ordinary model. And finally, we can
see that the results can be dramatically different. Namely, a slope is
significant in a robust regression, while not significant in the
ordinary linear model, indicating that ordinary model (1) was soo
heavily biased by the outlier, (2) that it produced a wrong result, (3)
which made me miss an important discovery (4) and never win a Nobel
Price ;)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$predictor&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$predictor&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c38340cbc_files/figure-html/figures-side2-1.png" width="50%" /&gt;&lt;img src="file14d3c38340cbc_files/figure-html/figures-side2-2.png" width="50%" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 6. compare coefficients and goodness of fit of both models
tab_model(m)&lt;/code&gt;&lt;/pre&gt;
&lt;table style="border-collapse:collapse; border:none;"&gt;
&lt;tr&gt;
&lt;th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; "&gt;
 
&lt;/th&gt;
&lt;th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; "&gt;
outcome
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; "&gt;
Predictors
&lt;/td&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  "&gt;
Estimates
&lt;/td&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  "&gt;
CI
&lt;/td&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  "&gt;
p
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; "&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
0.23
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
-6.52 – 6.98
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
0.921
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; "&gt;
predictor
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
0.67
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
-1.37 – 2.71
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
0.372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;"&gt;
Observations
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3"&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;"&gt;
R&lt;sup&gt;2&lt;/sup&gt; / R&lt;sup&gt;2&lt;/sup&gt; adjusted
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3"&gt;
0.268 / 0.024
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;pre class="r"&gt;&lt;code&gt;tab_model(rm)&lt;/code&gt;&lt;/pre&gt;
&lt;table style="border-collapse:collapse; border:none;"&gt;
&lt;tr&gt;
&lt;th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; "&gt;
 
&lt;/th&gt;
&lt;th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; "&gt;
outcome
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; "&gt;
Predictors
&lt;/td&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  "&gt;
Estimates
&lt;/td&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  "&gt;
CI
&lt;/td&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  "&gt;
p
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; "&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
-0.19
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
-0.77 – 0.39
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
0.380
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; "&gt;
predictor
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
1.09
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
0.97 – 1.21
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;
&lt;strong&gt;&amp;lt;0.001&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;"&gt;
Observations
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3"&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;"&gt;
R&lt;sup&gt;2&lt;/sup&gt; / R&lt;sup&gt;2&lt;/sup&gt; adjusted
&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3"&gt;
0.979 / 0.972
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;So, having robust regression in your statistical toolbox will already
step up your data-science game, but robust regression does not save you
from violations of other model assumptions, which you definitely need to
check, otherwise you might again get a completely wrong result.
Fortunately, there is only &lt;strong&gt;one function&lt;/strong&gt; which
&lt;strong&gt;checks and visualizes all the assumptions of any model at
once&lt;/strong&gt; which you can learn more about from &lt;a
href="https://youtu.be/EPIxQ5i5oxs"&gt;this video&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id="works-also-in-a-complicated-model"&gt;Works also in a complicated
model&lt;/h1&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(carData)
cm &amp;lt;- lm(prestige ~ income * education, data = Duncan)
ols_plot_resid_lev(cm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c38340cbc_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;crm &amp;lt;- lmrob(prestige ~ income * education, data = Duncan)

library(performance)
compare_performance(cm, crm, rank = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Comparison of Model Performance Indices

Name | Model |    R2 | R2 (adj.) |   RMSE |  Sigma | Performance-Score
----------------------------------------------------------------------
crm  | lmrob | 0.885 |     0.876 | 13.506 | 10.170 |            75.00%
cm   |    lm | 0.829 |     0.816 | 12.895 | 13.509 |            25.00%&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="there-are-further-packages-for-robust-regression-but"&gt;There are
further packages for robust regression, but…&lt;/h1&gt;
&lt;p&gt;MASS::rlm() does not provide &lt;span class="math inline"&gt;\(R^2\)&lt;/span&gt;
while robust::lmRob() does not provide info on outliers and has a
smaller &lt;span class="math inline"&gt;\(R^2\)&lt;/span&gt;. There are also other
options to conduct a non-parametric regression, like least trimmed
squares, quantile regression or bootstrapped regression. But, while all
of them have merits, I personally decided to always use
robustbase::lmrob() if I have unsual data.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rm2 &amp;lt;- MASS::rlm(outcome ~ predictor, data= d)
rm3 &amp;lt;- robust::lmRob(outcome ~ predictor, data= d)

plot_model(rm2, type = &amp;quot;pred&amp;quot;, show.data = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$predictor&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_model(rm3, type = &amp;quot;pred&amp;quot;, show.data = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$predictor&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3c38340cbc_files/figure-html/figures-side4-1.png" width="50%" /&gt;&lt;img src="file14d3c38340cbc_files/figure-html/figures-side4-2.png" width="50%" /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll
improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>eac1c282851219989ca8486736109c62</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <category>visualization</category>
      <category>models</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-09-02-robustregression</guid>
      <pubDate>Fri, 02 Dec 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-09-02-robustregression/thumbnail_robust.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {glmulti} find the best model!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-05-31-glmulti</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk
about. It’s less then 14 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/Im293ClFen4" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="why-do-we-need-glmulti"&gt;Why do we need {glmulti}?&lt;/h2&gt;
&lt;p&gt;The goal of ANY model is to explain a dependent variable by several
independent variables, sometimes called predictors. But which predictors
are useful(?) and how many should we include into our model(?), is
usually unknown. These questions are important, because if we take to
many predictors, we’ll overfit the model and explain the noise in the
data instead of uncovering true relationships. While, if we include only
a few predictors into our model, we’ll underfit the model and probably
miss some potentially important relationships. Thus, we need to find
&lt;strong&gt;THE BEST&lt;/strong&gt; model, with an &lt;strong&gt;optimal set of
predictors which explains maximum of our dependent variable, without
explaining the noise&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-05-31-glmulti/fit.png" /&gt;&lt;/p&gt;
&lt;h2 id="stepwise-variable-selection-approach"&gt;Stepwise variable
selection approach&lt;/h2&gt;
&lt;p&gt;One of the most common solutions for finding &lt;strong&gt;THE
BEST&lt;/strong&gt; model is &lt;strong&gt;a stepwise variable selection.&lt;/strong&gt;
But it’s not the best solution out there, and here is why. Stepwise
selection applies two main techniques: forwards and backwards selection.
Forwards selection starts with an empty model, adds one predictor,
compares two models, one with and another without this predictor, takes
the best model of the two, adds another predictor etc… Backwards
selection starts with the most complicated model, which includes all
predictors and interactions, and reduces the number of terms one by one.
But there are two problems with it. First, &lt;strong&gt;forwards and
backwards approaches would often not converge to the same
model&lt;/strong&gt;, like in our example. And secondly, even if they converge
to the same model, this model might not be the optimal one (gray circle
on the picture below). These problems occur simply because stepwise
selection doesn’t look at &lt;strong&gt;all possible models at the same
time&lt;/strong&gt;. They just remove or add terms one by one, compare two
models, take the best model of the two, remove or add another term
etc..&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-05-31-glmulti/stepwise_problem.png" /&gt;&lt;/p&gt;
&lt;p&gt;Load all needed packages at once, to avoid interruptions.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(car)        # extracts model results
library(MASS)       # provides &amp;quot;birthwt&amp;quot; dataset
library(ISLR)       # provides &amp;quot;Wage&amp;quot; dataset
library(tictoc)     # checks running time
library(sjPlot)     # visualizes model results
library(glmulti)    # finds the BEST model
library(flextable)  # beautifies tables
library(tidyverse)  # provides a lot of useful stuff !!! 
library(performance)# checks and compares quality of models

theme_set(theme_light(base_size = 12)) # beautifies plots
theme_update(panel.grid.minor = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# prepare selection
full_model &amp;lt;- glm(mpg ~ (hp + drat + wt + qsec + gear)^2, 
                 data = mtcars, family = gaussian)

null_model &amp;lt;- glm(mpg ~ 1, data = mtcars, family = gaussian)

# run stepwise selection
optimal_model_backward &amp;lt;- step(full_model, direction = &amp;quot;backward&amp;quot;,
                        scope = list(upper = full_model, lower = null_model))

optimal_model_forward &amp;lt;- step(null_model, direction = &amp;quot;forward&amp;quot;,
                        scope = list(upper = full_model, lower = null_model))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# compare two final models
anova(optimal_model_backward, optimal_model_forward, test = &amp;quot;Chisq&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Analysis of Deviance Table

Model 1: mpg ~ hp + drat + wt + qsec + gear + hp:drat + hp:wt + hp:qsec + 
    hp:gear + drat:wt + drat:qsec + wt:qsec + wt:gear
Model 2: mpg ~ wt + hp + qsec + gear + wt:hp
  Resid. Df Resid. Dev Df Deviance Pr(&amp;gt;Chi)   
1        18      51.32                        
2        26     112.06 -8  -60.743  0.00638 **
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;compare_performance(optimal_model_backward, optimal_model_forward)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Comparison of Model Performance Indices

Name                   | Model | AIC (weights) | AICc (weights) | BIC (weights) |    R2 |  RMSE | Sigma
-------------------------------------------------------------------------------------------------------
optimal_model_backward |   glm | 135.9 (0.989) |  165.9 (&amp;lt;.001) | 157.9 (0.203) | 0.954 | 1.266 | 1.689
optimal_model_forward  |   glm | 144.9 (0.011) |  149.6 (&amp;gt;.999) | 155.2 (0.797) | 0.900 | 1.871 | 2.076&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="brute-force-approach-with-glmulti"&gt;“Brute force” approach with
{glmulti}&lt;/h2&gt;
&lt;p&gt;In contrast, {glmulti} R package builds &lt;strong&gt;all possible models
with all possible combinations of predictors and, optionally, even their
pairwise interactions&lt;/strong&gt;. Such approach was called “brute
force”.&lt;/p&gt;
&lt;p&gt;{glmulti} then compares the amount of useful information models
provide. Such model comparison is done with the help of information
criteria (IC), for example Akaike’s IC (aic) or Bayesian IC (bic).
Information criteria are used instead of other metrics, such as &lt;span
class="math inline"&gt;\(R^2\)&lt;/span&gt;, because they show the “fitness” of
the model, where this fitness is penalized by the number of predictors a
model incorporates. In contrast to information criteria, &lt;span
class="math inline"&gt;\(R^2\)&lt;/span&gt; will always increase with the
increasing number of terms and will eventually overfit the model. And as
mentioned before, an overfitted model is bad, because it describes the
noise rather than genuine relationships between variables. Consequently,
we can’t trust the coefficients and p-values of overfitted models.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-05-31-glmulti/bic_vs_r_squared.jpeg" style="width:50.0%" /&gt;&lt;/p&gt;
&lt;p&gt;This picture originates from &lt;a
href="https://www.igi-global.com/gateway/chapter/235052"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But that’s not all, overfitting produces another problem. Each sample
has its own unique quirks. Consequently, overfitted model that fits the
random quirks of one sample is unlikely to fit the random quirks of
another sample. That makes overfitted model less generalizable outside
the original dataset, and therefore less useful.&lt;/p&gt;
&lt;p&gt;That’s why we need to create &lt;strong&gt;all possible models&lt;/strong&gt;,
instead of using stepwise selection, and we need to compare models using
&lt;strong&gt;Information Criteria&lt;/strong&gt;, instead of &lt;span
class="math inline"&gt;\(R^2\)&lt;/span&gt;. And while “Brute force” approach is
great, the number of models to be considered can easily become
exorbitant. However, there are several possibilities to reduce the
number of models and to decrease calculation time. Let’s get into the
Code and see how to do that.&lt;/p&gt;
&lt;h2 id="how-to-compute-glmulti-to-find-the-best-model"&gt;How to compute
glmulti to find the best model&lt;/h2&gt;
&lt;p&gt;The code is similar to any other model, you use in R:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first you have the &lt;strong&gt;formula&lt;/strong&gt; with the dependent
variable on the left side of the tilde (~), and all possible predictors
on the right side of the tilde. For this example we’ll study the salary
of 3000 american workers with 5 predictors: jobclass, education, age,
health and health-insurance&lt;/li&gt;
&lt;li&gt;then we’ll tell R which &lt;strong&gt;dataset&lt;/strong&gt; to use. In this
case we’ll use the “Wage” dataset from ISLR package&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;crit&lt;/strong&gt; specifies the &lt;strong&gt;Information
Criterion&lt;/strong&gt; to be used. Default is the Akaike IC (aic). Other
options are the Bayesian IC (bic), quasi-AIC for overdispersed or count
data (qaic and qaicc) and the small-sample corrected AIC (aicc), which I
personally prefer, because for big samples it always gets the same
result as Akaike’s IC, while with small samples it performs better&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;level&lt;/strong&gt; - argument is important! It specifies weather
all possible models supposed to be build without interactions (level =
1) or with interactions (level = 2)&lt;/li&gt;
&lt;li&gt;argument - &lt;strong&gt;method&lt;/strong&gt; - explores the candidate set of
models. Method = “d” counts the number of candidate models without
calculating anything. For our example of 5 predictors we’ll have 32
models without interactions and 1921 models with interactions. If method
= “h”, an &lt;strong&gt;exhaustive screening&lt;/strong&gt; is undertaken, which
means that all possible models will be created. If method = “g”, the
genetic algorithm is employed (recommended for large candidate
sets)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;glmulti(wage   ~ jobclass + education + age + health + health_ins,
        data   = Wage, 
        crit   = aicc,       # AICC corrected AIC for small samples
        level  = 1,          # 2 with interactions, 1 without  
        method = &amp;quot;d&amp;quot;,        # &amp;quot;d&amp;quot;, or &amp;quot;h&amp;quot;, or &amp;quot;g&amp;quot;
        family = gaussian, 
        fitfunction = glm,   # Type of model (LM, GLM etc.)
        confsetsize = 100)   # Keep 100 best models&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Initialization...
TASK: Diagnostic of candidate set.
Sample size: 3000
4 factor(s).
1 covariate(s).
0 f exclusion(s).
0 c exclusion(s).
0 f:f exclusion(s).
0 c:c exclusion(s).
0 f:c exclusion(s).
Size constraints: min =  0 max = -1
Complexity constraints: min =  0 max = -1
Your candidate set contains 32 models.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 32&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;glmulti(wage   ~ jobclass + education + age + health + health_ins,
        data   = Wage, 
        crit   = aicc,       # AICC corrected AIC for small samples
        level  = 2,          # 2 with interactions, 1 without  
        method = &amp;quot;d&amp;quot;,        # &amp;quot;d&amp;quot;, or &amp;quot;h&amp;quot;, or &amp;quot;g&amp;quot;
        family = gaussian, 
        fitfunction = glm,   # Type of model (LM, GLM, GLMER etc.)
        confsetsize = 100)   # Keep 100 best models&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Initialization...
TASK: Diagnostic of candidate set.
Sample size: 3000
4 factor(s).
1 covariate(s).
0 f exclusion(s).
0 c exclusion(s).
0 f:f exclusion(s).
0 c:c exclusion(s).
0 f:c exclusion(s).
Size constraints: min =  0 max = -1
Complexity constraints: min =  0 max = -1
Your candidate set contains 1921 models.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1921&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;you then specify the distribution &lt;strong&gt;family&lt;/strong&gt; and
the&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fitfunction&lt;/strong&gt;, where any function similar to
&lt;strong&gt;lm, glm or glmer&lt;/strong&gt; can be used&lt;/li&gt;
&lt;li&gt;lastly, &lt;strong&gt;confsetsize&lt;/strong&gt; argument allows you to keep a
particular number of the best models, so called - &lt;strong&gt;confident set
of best models&lt;/strong&gt;. One hundred - is a default value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, now let’s run the exhaustive algorithm and see how much time it
takes to compute 1921 regressions and to find the BEST model for our 5
predictors with interactions. “tic()” and “toc()” functions from
{tictoc} package would record running time for us.&lt;/p&gt;
&lt;p&gt;Fortunately, the exhaustive method took only 19 seconds. Not bad at
all, if you ask me. However, I usually have way more then five
predictors, which could cause performance problems. That’s why we need
to talk about the…&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tic()

h_model &amp;lt;- glmulti(wage ~ jobclass + education + age + health + health_ins,
          data   = Wage, 
          crit   = aicc,       # AICC corrected AIC for small samples
          level  = 2,          # 2 with interactions, 1 without  
          method = &amp;quot;h&amp;quot;,        # &amp;quot;d&amp;quot;, or &amp;quot;h&amp;quot;, or &amp;quot;g&amp;quot;
          family = gaussian, 
          fitfunction = glm,   # Type of model (LM, GLM, GLMER etc.)
          confsetsize = 100)   # Keep 100 best models

toc() # 19 sec elapsed: 1921 models &lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="performance-impovement-techniques"&gt;Performance impovement
techniques&lt;/h2&gt;
&lt;h3 id="remove-unnecessary-terms"&gt;1. Remove unnecessary terms&lt;/h3&gt;
&lt;p&gt;And the first one is to remove all unnecessary predictors or
interactions. For example a &lt;em&gt;weight&lt;/em&gt; and &lt;em&gt;body mass index
(BMI)&lt;/em&gt; provide very similar information - the statisticians would
say - they are highly multicollinear. Anyway, if both, &lt;em&gt;weight&lt;/em&gt;
and &lt;em&gt;BMI&lt;/em&gt; are included, they would dramatically increase the
number of models without providing any value. Check this out, adding
only two additional categorical predictors (maritl &amp;amp; region) into
the Wage model above increases the number of models to over 2.5 millions
(2604485 to be exact, see below). And while it’s unimaginable to run so
many models in our life time, &lt;strong&gt;genetic algorithm&lt;/strong&gt;
provides a solution for it.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;glmulti(wage ~ jobclass + education + age + health + health_ins + maritl + region,
        data   = Wage, 
        crit   = aicc,       # AICC corrected AIC for small samples
        level  = 2,          # 2 with interactions, 1 without  
        method = &amp;quot;d&amp;quot;,        # &amp;quot;d&amp;quot;, or &amp;quot;h&amp;quot;, or &amp;quot;g&amp;quot;
        family = gaussian, 
        fitfunction = glm,   # Type of model (LM, GLM, GLMER etc.)
        confsetsize = 100,
        plotty=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Initialization...
TASK: Diagnostic of candidate set.
Sample size: 3000
6 factor(s).
1 covariate(s).
0 f exclusion(s).
0 c exclusion(s).
0 f:f exclusion(s).
0 c:c exclusion(s).
0 f:c exclusion(s).
Size constraints: min =  0 max = -1
Complexity constraints: min =  0 max = -1
Your candidate set contains 2604485 models.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2604485&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="use-genetic-algorithm"&gt;2. Use genetic algorithm&lt;/h3&gt;
&lt;p&gt;Particularly, having 6 numeric predictors with interactions, the
“brute force” approach needs almost 3 hours, while genetic algorithm
runs only 40-80 seconds and produces almost identical results (with
sometimes slightly worse IC value).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tic()

test_h &amp;lt;- glmulti(mpg ~ hp + drat + wt + qsec + gear, 
                 data   = mtcars, 
                 method = &amp;quot;h&amp;quot;,       # Exhaustive approach
                 crit   = aic,      # AICC corrected AIC for small samples
                 level  = 2,         # 2 with interactions, 1 without
                 family = gaussian,
                 fitfunction = glm,  # Type of model (LM, GLM, GLMER etc.)
                 confsetsize = 100)  # Keep 100 best models

toc() # 32768 models &amp;quot;h&amp;quot; takes 104-109 seconds
# 6 numeric predictors with interactions produces 2.097.152 models, and &amp;quot;h&amp;quot; method takes 9715.466 seconds or ca. 2.7 hours

tic()

test_g &amp;lt;- glmulti(mpg ~ hp + drat + wt + qsec + gear, 
                 data   = mtcars, 
                 method = &amp;quot;g&amp;quot;,       # genetic algorithm approach
                 crit   = aic,      # AICC corrected AIC for small samples
                 level  = 2,         # 2 with interactions, 1 without
                 family = gaussian,
                 fitfunction = glm,  # Type of model (LM, GLM, GLMER etc.)
                 confsetsize = 100)  # Keep 100 best models

toc() # 32768 models &amp;quot;g&amp;quot; takes 40-59 seconds
# 6 numeric predictors with interactions produces 2.097.152 models, and &amp;quot;g&amp;quot; method takes 40-80 seconds or ca. 1 minute&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, if genetic algorithm is sooo cool, why not use genetic algorithm
all the time? Well, interestingly enough, with categorical predictors,
having a lot of categories, genetic algorithms may perform slower as
compared to the exhaustive one. For instance, our Wage-model, which has
lots of categorical predictors took only 19 second with the exhaustive
screening, while needed 117 seconds till genetic algorithm converged,
so, almost 6 times longer. Moreover, genetic algorithm might have
convergence problem and might run indefinitely long, without you having
any idea of WHEN, or IF it ever stops. And lastly, exhaustive method
almost always delivers better IC values. That’s why I’d recommend to
&lt;strong&gt;produce all possible models (aka. using exhaustive screening,
aka. applying “brute force” approach) whenever possible&lt;/strong&gt; and
only use genetic algorithm for a high number of numeric predictors.&lt;/p&gt;
&lt;h3 id="specify-marginality-and-exclude-arguments-optional"&gt;3. Specify
“marginality” and “exclude” arguments (optional) –&amp;gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;marginality = TRUE&lt;/strong&gt;, argument considers only
marginal models. That would reduce the number of model from 2604485 to
2350602. I did not really understand what the martinality exactly does,
that is why I just prefer to leave it out.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Argument &lt;strong&gt;exclude&lt;/strong&gt; excludes (main effects or
interactions) from the candidate models, e.g. c(“mass:height”) … it
somehow did not work in my code and I could not find out why. I hope it
will work on your machine!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;using multiple cores while computing might help.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="what-the-hell-is-the-best-model-then"&gt;What the hell is THE BEST
model then ???&lt;/h2&gt;
&lt;p&gt;By the way, remember, in the beginning of the video I said, that
stepwise selection is not the best method, implying that {glmulti}
approach is better? Well, let’s compare the results of exhaustive and
genetic algorithms, to the results of forward and backwards selections
and see which is a &lt;strong&gt;TRULY BEST&lt;/strong&gt; model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimal_model_glmulti_exhaustive &amp;lt;- test_h@objects[[1]]
optimal_model_glmulti_genetic    &amp;lt;- test_g@objects[[1]]
compare_performance(optimal_model_glmulti_exhaustive, optimal_model_glmulti_genetic, optimal_model_backward, optimal_model_forward)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Comparison of Model Performance Indices

Name                             | Model | AIC (weights) | AICc (weights) | BIC (weights) |    R2 |  RMSE | Sigma
-----------------------------------------------------------------------------------------------------------------
optimal_model_glmulti_exhaustive |   glm | 134.7 (0.391) |  141.0 (0.497) | 146.5 (0.496) | 0.932 | 1.547 | 1.750
optimal_model_glmulti_genetic    |   glm | 134.7 (0.391) |  141.0 (0.497) | 146.5 (0.496) | 0.932 | 1.547 | 1.750
optimal_model_backward           |   glm | 135.9 (0.215) |  165.9 (&amp;lt;.001) | 157.9 (0.002) | 0.954 | 1.266 | 1.689
optimal_model_forward            |   glm | 144.9 (0.002) |  149.6 (0.007) | 155.2 (0.006) | 0.900 | 1.871 | 2.076&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimal_model_glmulti_exhaustive$formula&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mpg ~ 1 + wt + qsec + gear + drat:hp + qsec:wt + gear:wt
&amp;lt;environment: 0x7f8d7a176370&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimal_model_glmulti_genetic$formula&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mpg ~ 1 + wt + qsec + gear + drat:hp + qsec:wt + gear:wt
&amp;lt;environment: 0x7f8d3b0d1948&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimal_model_backward$formula&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mpg ~ hp + drat + wt + qsec + gear + hp:drat + hp:wt + hp:qsec + 
    hp:gear + drat:wt + drat:qsec + wt:qsec + wt:gear
&amp;lt;environment: 0x7f8d5cd34558&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimal_model_forward$formula&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mpg ~ wt + hp + qsec + gear + wt:hp
&amp;lt;environment: 0x7f8d5cd34558&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see {glmulti} approach produced lower AIC and much lower
BIC Information criteria, and interestingly enough, the &lt;span
class="math inline"&gt;\(R^2\)&lt;/span&gt; produced by {glmulti} is right in
between the &lt;span class="math inline"&gt;\(R^2\)&lt;/span&gt;s of forwards and
backwards selections, suggesting that {glmulti} models are neither
underfitted not overfitted. Moreover, in our example both exhaustive and
genetic algorithms have identical result (will not always be the case)
and showed three interactions (drat:hp + qsec:wt + gear:wt) to be
important, while backwards selection found 8 interactions to be
important, which to me sound like overfitting, which is in line with
it’s highest &lt;span class="math inline"&gt;\(R^2\)&lt;/span&gt;, and forwards
selection found only one interaction, which looks like underfitting,
which is in line with it’s lowest &lt;span
class="math inline"&gt;\(R^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So, I hope I could convince you that &lt;strong&gt;{glmulti} approach is
superior to the stepwise selection&lt;/strong&gt; approach and produces a
&lt;strong&gt;truly BEST model&lt;/strong&gt;.&lt;/p&gt;
&lt;!-- ```{r eval=FALSE} --&gt;
&lt;!-- tic() --&gt;
&lt;!-- g_model &lt;- glmulti(wage ~ jobclass + education + age + health + health_ins, --&gt;
&lt;!--           data   = Wage,  --&gt;
&lt;!--           crit   = aicc,       # AICC corrected AIC for small samples --&gt;
&lt;!--           level  = 2,          # 2 with interactions, 1 without   --&gt;
&lt;!--           method = "g",        # "d", or "h", or "g" --&gt;
&lt;!--           family = gaussian,  --&gt;
&lt;!--           fitfunction = glm,   # Type of model (LM, GLM, GLMER etc.) --&gt;
&lt;!--           confsetsize = 100)   # Keep 100 best models --&gt;
&lt;!-- toc() # 117 sec elapsed: After 440 generations:  --&gt;
&lt;!-- ``` --&gt;
&lt;h2 id="some-exotic-applications-glmer-or-multinom"&gt;Some exotic
applications: GLMER or multinom&lt;/h2&gt;
&lt;p&gt;And while {glmulti} works fine with the classic functions like LM and
GLM, it can also fit some exotic models, such as “multinomial” models
via Neural Networks from {nnet} package.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(nnet)

multinom_glmulti &amp;lt;- glmulti(
  education ~ wage + jobclass + health, 
  data   = Wage, 
  level  = 2, 
  method = &amp;quot;h&amp;quot;
  fitfunction = multinom)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the predictions of the best multinomial model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(effects::allEffects(multinom_glmulti@objects[[1]]),
     lines = list(multiline = T),
     confint = list(style = &amp;quot;auto&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3cc5c2752_files/figure-html/unnamed-chunk-14-1.png" width="1344" /&gt;&lt;/p&gt;
&lt;p&gt;And lastly, despite the fact there is no straightforward fitting
function for the mixed-effects models, such as GLMER from {lme4}
package, we can easily write our own wrapper-function and use it inside
of {glmulti}:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;glmer.glmulti&amp;lt;-function(formula, data, random = &amp;quot;&amp;quot;, ...){
   glmer(paste(deparse(formula),random),
         data    = data, REML = F, ...)
}

mixed_model &amp;lt;- glmulti(
  y = response ~ predictor_1 + predictor_2 + predictor_3,
  random  = &amp;quot;+(1|random_effect)&amp;quot;,
  crit    = aicc,
  data    = data,
  family  = binomial,
  method  = &amp;quot;h&amp;quot;,
  fitfunc = glmer.glmulti,
  marginality = F,
  level   = 2 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s have a look at the results of our BEST model and interpret
them.&lt;/p&gt;
&lt;h2 id="extract-results"&gt;Extract results&lt;/h2&gt;
&lt;p&gt;The output of a {glmulti} analysis is an object containing the
&lt;strong&gt;confidence set of models (100 best models by default)&lt;/strong&gt;.
Standard R regression functions like “summary()”, “coef()” or “plot()”
can all be used to make a multi-model inference. But let’s start with
the brief summary of the results which can be obtained with via
“print()” command:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;print(h_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;glmulti.analysis
Method: h / Fitting: glm / IC used: aicc
Level: 2 / Marginality: FALSE
From 100 models:
Best IC: 29793.4306133546
Best model:
[1] &amp;quot;wage ~ 1 + jobclass + education + health + health_ins + age + &amp;quot;  
[2] &amp;quot;    education:jobclass + health_ins:education + education:age + &amp;quot;
[3] &amp;quot;    health:age + health_ins:age&amp;quot;                                 
Evidence weight: 0.0786680339413555
Worst IC: 29801.3206286612
6 models within 2 IC units.
74 models to reach 95% of evidence weight.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… were we see the most important information, such as fitting
function, the information criteria used to rank the models, the formula
of the best model and even the number of models which as good as the
best model. There are 6 models, which we can also see if we plot our
object:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(h_model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3cc5c2752_files/figure-html/unnamed-chunk-18-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;This plot shows the IC values for all 100 models from the confidence
set. A horizontal line separates 6 best models, that are less than 2 IC
units away from &lt;strong&gt;THE BEST&lt;/strong&gt; model. But what predictors and
interactions do those 6 models contain? Using {weightable} function, we
can easily display them:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;weightable(h_model)[1:6,] %&amp;gt;% 
  regulartable() %&amp;gt;%       # beautifying tables
  autofit()&lt;/code&gt;&lt;/pre&gt;
&lt;template id="f9179015-dd37-4bca-971f-a193f7fa9a57"&gt;&lt;style&gt;
.tabwid table{
  border-spacing:0px !important;
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  border-color: transparent;
  caption-side: top;
}
.tabwid-caption-bottom table{
  caption-side: bottom;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td, .tabwid th {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
.katex-display {
    margin: 0 0 !important;
}
&lt;/style&gt;&lt;div class="tabwid"&gt;&lt;style&gt;.cl-1e0e954c{}.cl-1e035ab0{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1e083a44{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1e083a58{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1e08597a{width:12.024in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1e085984{width:0.965in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1e08598e{width:1.092in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1e085998{width:12.024in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1e085999{width:0.965in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1e0859a2{width:1.092in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1e0859a3{width:12.024in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1e0859ac{width:0.965in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1e0859b6{width:1.092in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}&lt;/style&gt;&lt;table class='cl-1e0e954c'&gt;&lt;thead&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;th class="cl-1e08597a"&gt;&lt;p class="cl-1e083a44"&gt;&lt;span class="cl-1e035ab0"&gt;model&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;th class="cl-1e085984"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;aicc&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;th class="cl-1e08598e"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;weights&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-1e085998"&gt;&lt;p class="cl-1e083a44"&gt;&lt;span class="cl-1e035ab0"&gt;wage ~ 1 + jobclass + education + health + health_ins + age + education:jobclass + health_ins:education + education:age + health:age + health_ins:age&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e085999"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;29,793.43&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e0859a2"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;0.07866803&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-1e085998"&gt;&lt;p class="cl-1e083a44"&gt;&lt;span class="cl-1e035ab0"&gt;wage ~ 1 + jobclass + education + health + health_ins + age + education:jobclass + health_ins:education + education:age + health:age&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e085999"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;29,793.68&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e0859a2"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;0.06952606&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-1e085998"&gt;&lt;p class="cl-1e083a44"&gt;&lt;span class="cl-1e035ab0"&gt;wage ~ 1 + jobclass + education + health_ins + age + education:jobclass + health_ins:education + education:age + health:age&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e085999"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;29,794.40&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e0859a2"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;0.04836431&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-1e085998"&gt;&lt;p class="cl-1e083a44"&gt;&lt;span class="cl-1e035ab0"&gt;wage ~ 1 + jobclass + education + health_ins + age + education:jobclass + health_ins:education + education:age + health:age + health_ins:age&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e085999"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;29,794.43&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e0859a2"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;0.04776916&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-1e085998"&gt;&lt;p class="cl-1e083a44"&gt;&lt;span class="cl-1e035ab0"&gt;wage ~ 1 + jobclass + education + health + health_ins + age + education:jobclass + health_ins:education + jobclass:age + education:age + health:age + health_ins:age&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e085999"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;29,795.31&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e0859a2"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;0.03070167&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-1e0859a3"&gt;&lt;p class="cl-1e083a44"&gt;&lt;span class="cl-1e035ab0"&gt;wage ~ 1 + jobclass + education + health + health_ins + age + education:jobclass + health_ins:jobclass + health_ins:education + education:age + health:age + health_ins:age&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e0859ac"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;29,795.36&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-1e0859b6"&gt;&lt;p class="cl-1e083a58"&gt;&lt;span class="cl-1e035ab0"&gt;0.02990738&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;/template&gt;
&lt;div class="flextable-shadow-host" id="f37c2c88-fed9-4be9-8685-88101d7623b3"&gt;&lt;/div&gt;
&lt;script&gt;
var dest = document.getElementById("f37c2c88-fed9-4be9-8685-88101d7623b3");
var template = document.getElementById("f9179015-dd37-4bca-971f-a193f7fa9a57");
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
&lt;/script&gt;

&lt;p&gt;Here we see the formulas, Information Criteria and the Akaike weights
of our 6 best models. The Akaike weight for a particular model shows the
probability that the model is the best model out of all models
considered. To say it in a simple lingo - the model with the highest
weight minimizes the loss of information. So, while the “best” model has
the highest weight, its weight in this example is not substantially
larger than that of the second model (and also the third, fourth, and so
on). So, we shouldn’t be all too certain here that the top model is
really &lt;strong&gt;the best model&lt;/strong&gt; in the set. Several models are
almost equally plausible. So, &lt;strong&gt;which model should we take
then?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If all 6 models are great, but have different combinations of
predictors and interactions, figuring out which terms are important may
help to choose the best model. Fortunately for us, the
&lt;code&gt;plot()&lt;/code&gt; command with &lt;code&gt;type="s"&lt;/code&gt; argument displays
the relative importance of model terms across all models. The importance
value for a particular predictor or interaction is equal to the sum of
the weights for the models in which the variable appears. So, &lt;strong&gt;a
variable that shows up in lots of models with large weights will receive
a high importance value&lt;/strong&gt;. A vertical line is drawn at 80% (where
terms to the right of the line are part of 80% of the models), which is
sometimes used as a cutoff to differentiate between very important and
less important variables. This threshold is somewhat arbitrary though,
so that we are free to set it at … let’s say 50% and include all the
predictors and interactions with the importance above 50% into the final
model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(h_model, type = &amp;quot;s&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3cc5c2752_files/figure-html/unnamed-chunk-20-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Interestingly, the very first model contains the
&lt;em&gt;age:health_ins&lt;/em&gt; interaction, which has ca. 50% importance. And
it would be totally fine to go with that. But, since we have so many
terms with the importance around 80%, I am happy to use only those,
including &lt;em&gt;education:health_insurance&lt;/em&gt; interaction and predictor
&lt;em&gt;health&lt;/em&gt;, because they are far enough from the rest. And if I
look at 6 best models, I’ll see that the second model has exactly those
terms. The third model is a bit worse because it does not contain
variable health, but since health is part of the most important
interaction - &lt;em&gt;age:health&lt;/em&gt;, I’d prefer to include it. So, now we
did not blindly trust the algorithm and took it’s &lt;strong&gt;BEST
MODEL&lt;/strong&gt;, but examined the results carefully and made a grounded
decision to take the second model as &lt;strong&gt;OUR BEST
MODEL&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now, we can easily interpret and visualize and check assumptions of
&lt;strong&gt;OUR BEST&lt;/strong&gt; model as we always do:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;best_model &amp;lt;- h_model@objects[[2]]

car::Anova(best_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Analysis of Deviance Table (Type II tests)

Response: wage
                     LR Chisq Df Pr(&amp;gt;Chisq)    
jobclass                 4.91  1   0.026764 *  
education              626.21  4  &amp;lt; 2.2e-16 ***
health                  28.16  1  1.117e-07 ***
health_ins             158.79  1  &amp;lt; 2.2e-16 ***
age                     90.94  1  &amp;lt; 2.2e-16 ***
jobclass:education      16.14  4   0.002838 ** 
education:health_ins    10.22  4   0.036890 *  
education:age           12.11  4   0.016572 *  
health:age              10.13  1   0.001459 ** 
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_model(best_model, type = &amp;quot;int&amp;quot;) %&amp;gt;% 
  plot_grid()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file14d3cc5c2752_files/figure-html/unnamed-chunk-21-1.png" width="1440" /&gt;&lt;/p&gt;
&lt;p&gt;And if you want to learn &lt;strong&gt;how to test ALL model-assumptions
using only one function&lt;/strong&gt;, check out {performance} package:&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/BNTn_f43U04" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="problems"&gt;Problems&lt;/h2&gt;
&lt;p&gt;So, while {glmulti} is an amazing package, but it is not perfect and
here are three things I found challenging:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;rJava package needed. If you can’t easily install rJava package
from RStudio, chances are your computed does not have Java installed.
Doing this can take some time and nerves.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;some arguments are poorly described (e.g. “marginality”), or
simply do not work (e.g. “exclude”). Please, let me know in the comments
below, if you managed to use “exclude”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;and while the &lt;code&gt;coef()&lt;/code&gt; and &lt;code&gt;predict()&lt;/code&gt;
commands are useful multi-model inference tools for models without
interactions and only with numeric predictors and could provide
multi-model averaged estimates, confidence intervals and predictions, I
find them less intuitive for the models with several interactions and
with many categorical predictors.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="further-readings-and-references"&gt;Further readings and
references&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://www.jstatsoft.org/article/view/v034i12"
class="uri"&gt;https://www.jstatsoft.org/article/view/v034i12&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a
href="https://cran.r-project.org/web/packages/glmulti/glmulti.pdf"
class="uri"&gt;https://cran.r-project.org/web/packages/glmulti/glmulti.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://www.igi-global.com/gateway/chapter/235052"
class="uri"&gt;https://www.igi-global.com/gateway/chapter/235052&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll
improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>e293af68b10d93947ec753cf0794c593</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <category>machine learning</category>
      <category>R package reviews</category>
      <category>models</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-05-31-glmulti</guid>
      <pubDate>Fri, 02 Dec 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-05-31-glmulti/thumbnail_glmulti.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {gtsummary} Publication-Ready Tables of Data, Stat-Tests and Models!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-10-31-gtsummary</link>
      <description>{gtsummary} package helps to easily produce publication-ready &amp; beautifully formatted summary tables of Data, Statistical Tests and Models! It calculates tons of statistics and has a beautiful design by default, but you can customize every aspect of your table and export it as a picture or MS Word format.</description>
      <category>videos</category>
      <category>statistics</category>
      <category>models</category>
      <category>machine learning</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-10-31-gtsummary</guid>
      <pubDate>Tue, 29 Nov 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-10-31-gtsummary/thumbnail_gtsummary.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Publication-Ready Tables of Particular Statistical Tests and Models with {gtsummary}</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-11-25-gtsummary2</link>
      <description>Find a review of incredibly useful {gtsummary} package in a separate blog-post. Here I'll just collect all the possible Statsitcal Tests and Models, {gtsummary} can help with.</description>
      <category>videos</category>
      <category>statistics</category>
      <category>models</category>
      <category>machine learning</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-11-25-gtsummary2</guid>
      <pubDate>Tue, 29 Nov 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-11-25-gtsummary2/thumbnail_gtsummary2.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {rsample} Effective Resampling for Machine Learning!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-10-27-rsample</link>
      <description>Let's learn how to use three most important resampling techniques: train-test split, cross-validation and bootstrapping. We'll start with the question...</description>
      <category>videos</category>
      <category>statistics</category>
      <category>models</category>
      <category>machine learning</category>
      <category>tidymodels</category>
      <category>R package reviews</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-10-27-rsample</guid>
      <pubDate>Sun, 06 Nov 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-10-27-rsample/thumbnail_rsample.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>4 Reasons Non-Parametric Bootstrapped Regression (with tidymodels) is Better thаn Ordinary Regression</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-08-31-bootstrappingregressions</link>
      <description>If the assumptions of parametric models can be satisfied, parametric models are the way to go. However, there are often many assumptions and to satisfy them all is rarely possible. Data transformation or using non-parametric methods are two solutions for that. In this post we'll learn the Non-Parametric Bootstrapped Regression as an alternative for the Ordinary Linear Regression in case when assumptions are violated.</description>
      <category>videos</category>
      <category>statistics</category>
      <category>visualization</category>
      <category>models</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-08-31-bootstrappingregressions</guid>
      <pubDate>Fri, 07 Oct 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-08-31-bootstrappingregressions/thumbnail_bootstrapped_regression.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Repeated Measures ANOVA (One-Way) | How to Conduct, Visualise and Interpret</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-01-30-rmanova</link>
      <description>Can sport increase our selfesteem? Well, one experiment measured self-esteem of 10 people on three different time points and used Repeated Measures ANOVA to answer this question. So, let's learn how to produce this statistically rich plot using only one simple command and how to interpret all these results.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-01-30-rmanova</guid>
      <pubDate>Wed, 05 Oct 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-01-30-rmanova/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo| Many Models with Nested (Grouped) Data Easily</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-09-12-manymodels</link>
      <description>In this blog-post, we'll learn how to produce grouped / nested models, with an amazing "map()" function from {purrr} package in R. We'll use linear models in this example for the sake of simplicity, but you can apply any model you want (robust, logistic, poisson etc.). We'll see, how to effectively store and use the information from multiple models. And while in this blog-post we'll produce "only" 10 models, you can produce any number of models you want.</description>
      <category>videos</category>
      <category>statistics</category>
      <category>visualization</category>
      <category>models</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-09-12-manymodels</guid>
      <pubDate>Thu, 22 Sep 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-09-12-manymodels/thumbnail_many_models.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {sjPlot} How to Easily Visualize Data And Model Results</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-08-01-sjplot</link>
      <description>One picture is worth a thousand words. That's why visualizing data and model results is a crutial skill for any data scientist. {sjPlot} package became my favorite tool for visualization. That's why I want to share with you some simple but very effective commands which will make you more productive today. So, let's visualize Wage dataset, visualize bunch of models and see what people earn and what factors determine the salary.</description>
      <category>videos</category>
      <category>statistics</category>
      <category>R package reviews</category>
      <category>visualization</category>
      <category>models</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-08-01-sjplot</guid>
      <pubDate>Fri, 12 Aug 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-08-01-sjplot/thumbnail_sjPlot.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {report} How To Report Statistical Results!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-06-18-report</link>
      <description>If you ever wandered how to correctly describe the results of statistical tests and models, this blog is for you. In a few minutes you'll learn how to report the results of correlations, t-tests, Generalised Linear Models, Mixed-Effects models, Bayesian Models and even more 😉 So, let's start with a simple t-test.</description>
      <category>videos</category>
      <category>statistics</category>
      <category>R package reviews</category>
      <category>models</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-06-18-report</guid>
      <pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-06-18-report/thumbnail_report.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Tidy Data and Why We Need It!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-04-22-tidydata</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk
about and you have visual examples. It’s ca. 12 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/KW1laBLEiw0" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Well, if I had to summarize the whole idea of tidy data into one
sentence, I’d say: &lt;strong&gt;“Whatever changes in your data, put it into a
column.”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Why columns? Because columns are &lt;strong&gt;the easiest way to store
similar data&lt;/strong&gt;&lt;em&gt;.&lt;/em&gt; Important is that the data in every
column is &lt;strong&gt;similar, but not identical&lt;/strong&gt;, so, &lt;strong&gt;the
data vary&lt;/strong&gt;. For example &lt;em&gt;age&lt;/em&gt; varies from 1 to 100,
&lt;em&gt;gender&lt;/em&gt; varies from male to female. That’s actually why
&lt;strong&gt;a column is always a variable&lt;/strong&gt;. And a &lt;strong&gt;VARIABLE
IS what we need to make any type of analysis possible&lt;/strong&gt;. Let me
make tidy data even easier for you.&lt;/p&gt;
&lt;h2 id="principles-of-tidy-data"&gt;Principles of tidy data&lt;/h2&gt;
&lt;p&gt;There are only &lt;strong&gt;3 simple principles for tidy data&lt;/strong&gt;
(postulated by the father of tidy data Hadley Wickham):&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;strong&gt;each column is a variable&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;each row is an observation&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;each cell is a single value&lt;/strong&gt; or only one peace of
information&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-04-22-tidydata/tidy-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;For the sake of simplicity, let’s say that any dataset which does not
follow these three rules is &lt;strong&gt;messy&lt;/strong&gt;. And the problem with
messy data is that &lt;strong&gt;it requires different strategies&lt;/strong&gt; and
&lt;strong&gt;tons of work&lt;/strong&gt; to extract different variables, in order
to enable different statistical analyses.&lt;/p&gt;
&lt;h2 id="does-messy-data-exist"&gt;Does messy data exist?&lt;/h2&gt;
&lt;p&gt;These three rules of tidy data seem so obvious that you might wonder
whether messy datasets even exist. Well, unfortunately, most real world
data is messy, because there are soo many opportunities to mess things
up, and people are usually very creative. Leo Tolstoy once said:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;“Happy families are all alike; while every unhappy family is
unhappy in its own way”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;“Like families, tidy datasets are all alike; but every messy
dataset is messy in its own way.” - Hadley Wickham&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Does this mean, that messy data is bad? Absolutely not. Messy
datasets might be very convenient for data collection and for having a
good overview of the whole dataset, containing all the information we
have, including explanatory columns, commentaries, colors etc. .
However, while useful for you, it is useless for
&lt;strong&gt;statistics&lt;/strong&gt;, which &lt;strong&gt;needs only variables and
observations&lt;/strong&gt;. So, you might end up having two tables, one for
you, and the other one for statistical analysis. Let me show you the
most common cases of messy data and how to fix them. The first one is
when…&lt;/p&gt;
&lt;h3
id="one-variable-is-stored-in-multiple-columns-or-when-column-headers-are-actually-values-not-variable-names"&gt;1.
One variable is stored in multiple columns, or when column headers are
actually values, not variable names&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-04-22-tidydata/tidy-9.png" /&gt;&lt;/p&gt;
&lt;p&gt;Different &lt;em&gt;timepoints&lt;/em&gt;, for example &lt;em&gt;years&lt;/em&gt; or
&lt;em&gt;days&lt;/em&gt;, are usually stored in different columns. And while it
might be convenient for recording data, it’s hardly possible to analyse
it. Why? Well, if we have time, we usually want to study change in
something over time, right? But a variable &lt;strong&gt;time&lt;/strong&gt; does
not exist if years are spread across different columns. Moreover, if we
see this table for the first time, we have no idea what those
&lt;strong&gt;numbers&lt;/strong&gt; are, so they are also not a variable, because
they are not in a single named column. Making this &lt;strong&gt;wide dataset
longer&lt;/strong&gt; creates two new variables which immedeatly allows to
study the &lt;strong&gt;change in tuberculosis cases over time&lt;/strong&gt; for
every country.&lt;/p&gt;
&lt;p&gt;So, every combination of a country and a year BECOMES &lt;strong&gt;a
single observation&lt;/strong&gt; of tuberculosis cases, and with that -
&lt;strong&gt;a single row&lt;/strong&gt;. But if we overdo that, we can end up with
a second common problem, where…&lt;/p&gt;
&lt;h3 id="multiple-variables-are-stored-in-one-column"&gt;2. Multiple
variables are stored in one column&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-04-22-tidydata/tidy-8.png" /&gt;&lt;/p&gt;
&lt;p&gt;As mentioned before, the observations within one column suppose to
belong together, for example gender with categories “females” and
“males”, or countries like Brazil and China. However, it’s important to
separate several &lt;strong&gt;categories&lt;/strong&gt; of the same variable, from
the column &lt;strong&gt;key&lt;/strong&gt;, which stores &lt;strong&gt;two different
variables&lt;/strong&gt; &lt;em&gt;cases and population&lt;/em&gt; in one column.
&lt;em&gt;Cases and population&lt;/em&gt; do not belong together, and thus can not
be analysed. To solve this problem we simply &lt;strong&gt;make a long table
wider&lt;/strong&gt;. Now, having two variables we can calculate the rate of
tuberculosis by dividing a column cases by the column population. Which
would not be possible if both values would be stored in the same cell,
being the third common problem…&lt;/p&gt;
&lt;h3 id="more-then-one-value-in-one-cell"&gt;3. More then ONE value in ONE
cell&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-04-22-tidydata/tidy-17.png" /&gt;&lt;/p&gt;
&lt;p&gt;This one can be very sneaky, because it pretends to convey a lot of
useful information, like a range of values from zero to five (0-5),
undecided values, like 2 or 3 or some borderline values, like &amp;lt;3 or
&amp;gt;99. To fix this problem, follow the third principle of tidy data and
always put only one value in one cell. And, please, &lt;strong&gt;don’t use
any special characters for numeric values&lt;/strong&gt;, because it will
produce the next most common problem, where…&lt;/p&gt;
&lt;h3
id="different-types-of-data-numbers-text-are-stored-in-the-same-column"&gt;4.
Different types of data (numbers &amp;amp; text) are stored in the same
column&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-04-22-tidydata/missing_data1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data can be either numbers or text (categories)&lt;/strong&gt;. Any
text or special characters (like @, €, *, ^, &amp;gt;, &amp;lt;, +, (), ” “, .
etc.) inside of a numeric column converts the whole variable into text.
Words like”unknown” or “missing” are the most common examples of text
inside of numeric columns. &lt;strong&gt;If the value is missing, it’s better
to leave the cell empty&lt;/strong&gt;. By the way, ironically, missing values
can sometimes cause the most damage, so it’s really important to …&lt;/p&gt;
&lt;h3 id="understand-what-a-missing-value-really-is"&gt;5. Understand what a
missing value really is&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-04-22-tidydata/missing_data.png" /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;real missing value&lt;/strong&gt; represents a measurement or
observation that should have been made, but wasn’t. One of the
non-intentional mistakes is to put &lt;em&gt;zeros&lt;/em&gt; (0) into cells with
missing values. Think about measuring blood pressure of a cat for
example. If cat’s owner doesn’t bring the cat to the clinic at Monday,
the blood pressure record &lt;strong&gt;should remain empty&lt;/strong&gt;. However,
if we put zero instead of a missing value on Monday, that would mean
that blood pressure of our cat WAS measured and it WAS zero - so our
Monday cat was either dead or a zombie. An example of a real zero is if
you measured virus load of that cat on Tuesday, but did not find any
virus. In this case a zero means that our cat is absolutely healthy and
it’s important to record that 0! So, &lt;strong&gt;a zero conveys a lot of
information, while a&lt;/strong&gt; &lt;strong&gt;missing value conveys NO
information and should therefore remain empty&lt;/strong&gt;. But the most
complicated form of messy data occurs when (see the picture above)…&lt;/p&gt;
&lt;h3 id="variables-are-stored-in-both-rows-and-columns."&gt;6. Variables are
stored in both rows and columns.&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-04-22-tidydata/tidied_up_data.png" /&gt;&lt;/p&gt;
&lt;p&gt;For example, days of the week is our “day” variable which we would
like to have in order to study blood pressure of cats over time, so we’d
need to put the days into a single column. Then, if we want to estimate
an average for Monday we can’t do that, because Monday contains two
different values, blood pressure and virus load. Thus we need to split
the column “test” into two different columns and just move the values.
In this tidy data values inside of every columns belong together.
Finally, let me quickly show you a few more…&lt;/p&gt;
&lt;h3
id="examples-of-messyness-and-the-checklist-to-follow-in-order-to-keep-the-data-tidy"&gt;7.
Examples of messyness and the checklist to follow in order to keep the
data tidy&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-04-22-tidydata/messy_data.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;remove empty rows&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;remove empty and constant columns&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;remove merged cells&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;use only the first raw as your header&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;store same data in the same table, for instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if control and treatment are in two tables, put them below each
other and create a new variable - “group”, because this is exactly what
we want - compare &lt;strong&gt;groups&lt;/strong&gt;, then…&lt;/li&gt;
&lt;li&gt;if multiple excel sheets contain similar information, for example
multiple years, combine them into one table and create a variable -
“years”, because we often want to study something over time&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;a thing to remember is that statistics has no good taste, thus
simple is better then beautiful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;so, any visual effects like colors, italic or bold font etc. don’t
provide any information, because statistics is blind, thus if colors are
important, turn them into variables&lt;/li&gt;
&lt;li&gt;if you want to show that some of the observations aren’t very good
(e.g. calibration error), create a new column “error” and use 1 every
time you aren’t sure, and 0 every time you are sure about the
measurement. That would later allow to exclude these ones easily if we
would want to.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;use short, simple, but still clear column names, instead of long
explanational names, because when you start to work with them, it will
hurt. Too short names, like “d” or “s” are also bad, because they don’t
communicate any information, while “days” or “species” do.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;check for similar but not identical categories (mostly created by
typos), because “cat”, “cat_” with a space and “cat.” with a dot could
be considered three different categories. The solution for that is to go
to Excel Table &amp;gt; Data &amp;gt; Filter, and to check all categorical
variables&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;remove all the columns and data which do not participate in the
analyses. As mentioned above, you may keep two tables, one for yourself,
with all the explanations and colors, and one new minimalistic table for
the analyses&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;but please don’t remove rows or columns only because they have
some missing values (empty cells), otherwise we would lose a lot of
existing information&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;don’t summarize, calculate or explain something on the side or
below the table, because a software will try to incorporate this
information in form of variables or observations&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if you plan to use R software don’t code categorical variables
into numbers, for example instead 1 and 2 for sex, write “female” and
“male”. For SPSS or other software you might need to code them, and last
but not least …&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if you think your table will become too long or that tidying up
the data is too much work … &lt;strong&gt;stop thinking that&lt;/strong&gt; :),
because approximately 80% of time in data analysis is spent on cleaning
and preparing the data &lt;a href="#fn1" class="footnote-ref"
id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; for calculations&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;So, these were just the most common messyness examples I encountered.
But the human creativity is not be underestimated. Thus, as you can see
- it is much easier to learn what to do, namely &lt;strong&gt;only three
principles of tidy data&lt;/strong&gt;, then what not to do.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://vita.had.co.nz/papers/tidy-data.pdf"
class="uri"&gt;https://vita.had.co.nz/papers/tidy-data.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dasu T, Johnson T (2003). Exploratory Data Mining and Data Cleaning.
Wiley-IEEE.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll
improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Dasu T, Johnson T (2003). Exploratory Data Mining and
Data Cleaning. Wiley-IEEE.&lt;a href="#fnref1"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>d96681a1558b263f1642c3de040596c8</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-04-22-tidydata</guid>
      <pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-04-22-tidydata/tidydata_2.jpeg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>R demo | ANOVA (One-Way ) | Fisher's, Welch's, Bayesian, Robust</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-04-03-anova</link>
      <description>How does education influence our salary? ANOVA which is just the abbreviation for Analysis Of Variances you see on the thumbnail answeres this question with Frequentists and Bayesian tests. It also privides two different effect sizes, compares education levels pairwisely and even corrects p-values for multiple comparisons. ALL OF THAT is done by this simple command. So, in this blog-post you'll learn how to produce the statistically rich plot, you'll understand when to conduct Welch's ANOVA and when Fisher's ANOVA and you'll know how to interpret every little detail on this plot. Lets get into it.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-04-03-anova</guid>
      <pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-04-03-anova/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Kruskal-Wallis test | How to conduct, visualize, interpret &amp; more 😉</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-04-13-kw</link>
      <description>If we have ordinal or not-normally distributed data, ANOVA might produce a wrong result. That's why we need Kruskal-Wallis test. Kruskal-Wallis test you see on the screen answers two question (1) whether at least one group is different from other groups and (2) between which groups exactly this difference is. So, let's learn how to get and interpret all these results.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-04-13-kw</guid>
      <pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-04-13-kw/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Cochran’s Q Test + Pairwise McNemar Tests (post-hoc)</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-03-04-cochran</link>
      <description>Cochran test is an extension of the McNemar test for comparing MORE than two PAIRED categorical samples in which the same individuals appear in each sample. If Cochran test is significant, we'd need to compare samples among each other pairwisely with McNemar tests. So, let's do that.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-03-04-cochran</guid>
      <pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-03-04-cochran/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Mann-Whitney U Test = Wilcoxon Rank Sum Test | How to conduct, visualise &amp; interpret 🥳 What happens if we use a wrong test 😱</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-03-16-mwutest</link>
      <description>Comparing two groups with not-normally disctributed or ordinal data is the reason we need Mann-Whitney U Test instead of t-Test. So, today we'll learn (1) how to conduct and visualize Mann-Whitney U Test you saw on the thumbnail with one simple command, (2) how to interpret all statistical results on that plot and (3) why this test is sometimes called Wilcoxon Rank Sum Test and why we shouldn't use this name</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-03-16-mwutest</guid>
      <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-03-16-mwutest/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Correlation Matrix | Danger or opportunity?</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-01-05-correlationmatrixinr</link>
      <description>Having several numeric variables, we often wanna know which of them are correlated and how. Correlation Matrix seems to be a good solution for it. But drawing conclusions from plain correlation coeffitients and p-values is dangerous, if we don't visualize the data. Let's learn a better way to produce a correlation matrix.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-01-05-correlationmatrixinr</guid>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-01-05-correlationmatrixinr/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>R demo | Two-Samples t-Test | Student's &amp; Welch's | How to conduct, visualise, interpret | What happens if we use a wrong test 😱</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-03-11-ttest</link>
      <description>Two-samples t-test can answer useful questions, for example - where can we get more money, working in a factory or in the IT-industry? So, let's learn (1) how to make sure t-test is a CORRECT test for our data, (2) how to get all these results with one simple command, (3) how to interpret all these results and (4) finally see what happens if we choose a wrong test.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-03-11-ttest</guid>
      <pubDate>Tue, 22 Mar 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-03-11-ttest/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Paired Samples t-Test | How to conduct, visualise and interpret</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-01-22-pairedsamplesttestinr</link>
      <description>Can one week of training significantly improve your number of sit-ups? Well, Paired t-Test can answer this question by comparing your performance Before and After this week. So, let's learn how to produce this statistically rich plot using only one simple command, how to interpret all these results and see what happens if we use a wrong test.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-01-22-pairedsamplesttestinr</guid>
      <pubDate>Fri, 11 Mar 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-01-22-pairedsamplesttestinr/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | McNemar Test | How to Conduct, Visualise and Interpret</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-02-20-mcnemar</link>
      <description>If you need to compare two PAIRED categorical samples, McNemar test is a correct choise for you. Though, people often use Chi-Square test instead. Thus, in this blog-post we'll first conduct, visualize and interpret McNemac test you see on the picture to your right using only one simple command and then see what happens if we use Chi-Square test for paired data.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-02-20-mcnemar</guid>
      <pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-02-20-mcnemar/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Friedman Test | How to Conduct, Visualise and Interpret</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-02-08-friedman</link>
      <description>The Friedman Test is a non-parametric brother of Repeated Measures ANOVA, which does much better job when data is not-normally distributed (which happens pretty often ;). Friedman test is also superior to Repeated Measures ANOVA when our data is ordinal (e.g., scales from 1 to 10). Friedman Test can also be a non-parametric father of the Paired Wilcoxon test, because it can compare more then two groups.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-02-08-friedman</guid>
      <pubDate>Sun, 20 Feb 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-02-08-friedman/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Paired Samples Wilcoxon Signed Rank Test</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr</link>
      <description>Can a speed-reading exercise make you a faster reader? Well, Wilcoxon Signed Rank Test displayed here is a correct test to answer this question. So, in this video we'll learn how to choose a correct test and what happens if we use a wrong test, why Wilcoxon test is called Signed Rank and how to produce and interpret this statistically rich plot using only one simple command.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr</guid>
      <pubDate>Wed, 26 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Correlation Analysis in R | Pearson, Spearman, Robust, Bayesian | How to conduct, visualise and interpret</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-12-29-correlationinr</link>
      <description>Having two numeric variables, we often wanna know whether they are correlated and how. One simple command {ggscatterstats} can answer both questions by visualizing the data and conducting frequentists and bayesian correlation analysis at the same time. So, let's learn how to do that, how to interpret all those results and how to choose the right correlation method in the first place.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-12-29-correlationinr</guid>
      <pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-12-29-correlationinr/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>One-sample Student’s t-test and One-sample Wilcoxon test: or how to compare your work to the work of others.</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others</link>
      <description>Imagine you get 7 out of 10 to-dos from your list done on average. Are you then more productive then others? One-sample t-test and One-sample Wilcoxon test can answer this question. So, in this blog-post you'll learn how to conduct and visualize these tests with only one simple command, how to interpret all these results and how to choose the right test in the first place. Let's get straight into it.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others</guid>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>R demo | Chi-Square Test | how to conduct, visualize &amp; interpret | + pairwise post-hoc tests</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-12-14-how-to-conduct-chi-square-test-in-r</link>
      <description>Chi-Square Test checks the independence between two categorical variables, where variables can have two or more categories. Need to do Chi-Square test? It can actually be done with only one line of code. There is no better way than {ggbarstats} function from {ggstatsplot} package 📦. In this short blog-post you'll learn how to conduct, visualize and interpret Chi-Square test &amp; pairwise post-hoc tests in R.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-12-14-how-to-conduct-chi-square-test-in-r</guid>
      <pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-12-14-how-to-conduct-chi-square-test-in-r/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>R package reviews {dlookr} diagnose, explore and transform your data</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data</link>
      <description>Raw data need to be diagnosed for existing problems, explored for new hypotheses and repaired in order to increase data quality and output. The {dlookr} package makes these steps fast and easy. {dlookr} generates automated reports and performs compex operations, like imputing missing values or outliers, with simple functions. Moreover, {dlookr} collaborates perfectly with {tidyverse} packages, like {dplyr} and {ggplot2} to name just a few!</description>
      <category>EDA</category>
      <category>videos</category>
      <category>data wrangling</category>
      <category>R package reviews</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data</guid>
      <pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data/dlookr_thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Deep Exploratory Data Analysis (EDA) in R</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress</link>
      <description>Exploratory Data Analysis is an important first step on the long way to the final result, be it a statistical inference in a scientific paper or a machine learning algorithm in production. This long way is often bumpy, highly iterative and time consuming. However, EDA might be the most important part of data analysis, because it helps to generate hypothesis, which then determine THE final RESULT. Thus, in this post I'll provide the simplest and most effective ways to explore data in R, which will significantly speed up your work. Moreover, we'll go one step beyond EDA by starting to test our hypotheses with simple statistical tests.</description>
      <category>EDA</category>
      <category>videos</category>
      <category>data wrangling</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress/DEDA_thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>How to impute missing values with Machine Learning in R</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r</link>
      <description>Imputation simply means - replacing a missing value with a value that makes sense. But how can we get such values? Well, we'll use Machine Learning algorithms, because they have a high prediction power. So, in this post we'll learn how to impute missing values easily and effectively.</description>
      <category>videos</category>
      <category>data wrangling</category>
      <category>visualization</category>
      <category>machine learning</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r/thumbnail_missing_values.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Null Hypothesis, Alternative Hypothesis and Hypothesis Testing</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good</link>
      <description>Hypothesis testing is one of the most important concepts in (frequentiest) statistics and science. However, most people who test hypotheses are scientists, but not statisticians. That's why scientists often do not test hypotheses properly, without any bad intensionс. So, in this blog-post we'll break down hypothesis testing in small parts and try to properly understand every of them.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>What is p-value and why we need it</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation</link>
      <description>Why do we need p-values? Well, they help to **make decisions** and **answer the question whether we found something new or not**. But despite the fact that **p-values are** actually **useful**, they are **far from perfect**! And while everyone uses p-values, understanding them (and using them correctly) is very hard. The definition of the p-value from the book is often correct but rarely intuitive. Intuitive explanations are often not entirely correct. So, in this blog-post (and video) we’ll start with an intuitive (and not entirely correct) definition and will gradually build up the understanding of the p-value step by step. Thus, I don’t recommend to skip any part of this blog (or video). We’ll also talk about how to use and interpret p-values correctly in order to **make better decisions and better science**.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>R package reviews {DataExplorer} explore your data!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data</link>
      <description>What is the best way to explore the data quick? I think it's visualization. And what it the best way to visualize the data quick? I think it's - {DataExplorer} package, because it can visualize all your data in seconds using only one function! Check this out...</description>
      <category>R package reviews</category>
      <category>EDA</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data</guid>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data/2.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Survival analysis 2: parametric survival models</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models</link>
      <description>The non-parametric Kaplan-Meier method (KM) can not describe survival probability by a smooth function, which means it can not predict anything. The parametric models (e.g. Exponential, Weibull etc.) can! Besides, in case where parametric models are appropriate, they are more exact, more effective and more informative than KM or Cox. However, unfortunately, this step is often left out due to the rear use of parametric models. In this post we’ll try to close this gap.</description>
      <category>survival analysis</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models</guid>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models/thumbnail_survival_2.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {performance} check how good your model is! </title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is</link>
      <description>There are several indicators of model quality, e.g. $R^2$ or AIC, and several assumption for every model which supposed to be checked, e.g. normality of residuals, multicollinearity etc.. R provides solutions for every indicator or assumption you can imagine. However, they are usually spread around different packages and functions. {performance} package brings all of quality indicators and all of the assumption under one roof. Thus, for me it became the one-stop solution for modelling.</description>
      <category>R package reviews</category>
      <category>videos</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is</guid>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/14.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Survival analysis 1: a gentle introduction into Kaplan-Meier Curves</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves</link>
      <description>Survival time analysis is necessary in any study which investigates the time to a particular outcome of interest. Cancer studies in the medicine and the first failure of the car in the engineering field (failure time analysis) are good examples. The outcome of interest could be death, remission to relapse, progression, or failure. Point in time of reaching that outcome is generally called the event. Thank goodness, not every “event” is fatal 😃, but can sometimes even be a favorable outcome such as discharge from hospital. And thus, survival analysis is also a generic term, because it is not only about survival.</description>
      <category>survival analysis</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves</guid>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves/thumbnail_survival_1.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {janitor} clean your data!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data</link>
      <description>Data Scientists spend up to 80% of their time cleaning and preparing data for analysis. " Happy families are all alike; every unhappy family is unhappy in its own way" — Leo Tolstoy. "Like families, tidy datasets are all alike but every messy dataset is messy in its own way" - Hadley Wickham. Thats when "janitor" helps to clean the mess.</description>
      <category>R package reviews</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data</guid>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data/11.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>How to visualize models, their assumptions and post-hocs</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs</link>
      <description>A picture is worth a thousand words! This article shows how to visualize results of 16 different models in R: from a simple linear model to a multiple-additive-non-linear-mixed-effects model. Among them are logistic, multinomial, additive and survival models with and without interactions. **Goal: minimum R code &amp; maximum output!** We'll also go a bit beyond only model visualization. So, don't miss the bonuses 😉.</description>
      <category>visualization</category>
      <category>videos</category>
      <category>models</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs</guid>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/thumbnail_visualize_models.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>How to create a blog or a website in R with {Distill} package</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package</link>
      <description>If you're not online, you don't exist. A personal webpage or a blog became the business card of the digital century. It shows who you are and what you are capable of. Thus: show, don't tell.</description>
      <category>R &amp; the Web</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package</guid>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/images/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
  </channel>
</rss>
