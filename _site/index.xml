<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>yuzaR-Blog</title>
    <link>https://yuzar-blog.netlify.app/</link>
    <atom:link href="https://yuzar-blog.netlify.app/index.xml" rel="self" type="application/rss+xml"/>
    <description>Data Science with R
</description>
    <generator>Distill</generator>
    <lastBuildDate>Sat, 30 Jan 2021 00:00:00 +0000</lastBuildDate>
    <item>
      <title>R package reviews {dlookr} diagnose, explore and transform your data</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data</link>
      <description>Raw data need to be diagnosed for existing problems, explored for new hypotheses and repaired in order to increase data quality and output. The {dlookr} package makes these steps fast and easy. {dlookr} generates automated reports and performs compex operations, like imputing missing values or outliers, with simple functions. Moreover, {dlookr} collaborates perfectly with {tidyverse} packages, like {dplyr} and {ggplot2} to name just a few!</description>
      <category>EDA</category>
      <category>videos</category>
      <category>data wrangling</category>
      <category>R package reviews</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data</guid>
      <pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data/dlookr_thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Deep Exploratory Data Analysis (EDA) in R</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress</link>
      <description>Exploratory Data Analysis is an important first step on the long way to the final result, be it a statistical inference in a scientific paper or a machine learning algorithm in production. This long way is often bumpy, highly iterative and time consuming. However, EDA might be the most important part of data analysis, because it helps to generate hypothesis, which then determine THE final RESULT. Thus, in this post I'll provide the simplest and most effective ways to explore data in R, which will significantly speed up your work. Moreover, we'll go one step beyond EDA by starting to test our hypotheses with simple statistical tests.</description>
      <category>EDA</category>
      <category>videos</category>
      <category>data wrangling</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress/DEDA_thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>How to impute missing values with Machine Learning in R</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r</link>
      <description>Imputation simply means - replacing a missing value with a value that makes sense. But how can we get such values? Well, we'll use Machine Learning algorithms, because they have a high prediction power. So, in this post we'll learn how to impute missing values easily and effectively.</description>
      <category>videos</category>
      <category>data wrangling</category>
      <category>visualization</category>
      <category>machine learning</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r/thumbnail_impute_na.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Null Hypothesis, Alternative Hypothesis and Hypothesis Testing</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/nFlTr_1GXXg" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="the-essence-of-hypothesis-testing"&gt;The essence of Hypothesis Testing&lt;/h2&gt;
&lt;p&gt;Hypothesis testing can be summarized in only 5 points:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;Collect &lt;strong&gt;data&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Clearly define &lt;strong&gt;Null (&lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;) and Alternate (&lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;) Hypotheses&lt;/strong&gt;. That‚Äôs the most important point!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Get &lt;strong&gt;p-value&lt;/strong&gt; through something called a &lt;strong&gt;statistical test&lt;/strong&gt;. Every modern software will calculate it for you.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Define your &lt;strong&gt;rejection threshold&lt;/strong&gt;, e.g.¬†&lt;strong&gt;p-value &amp;lt; 0.05&lt;/strong&gt; (or &amp;lt;5%) and finally&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make a &lt;strong&gt;conclusion&lt;/strong&gt;, e.g.¬†if p = 0.03, we reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; in favor of &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That‚Äôs actually enough to get you started with Hypothesis Testing. But if you want to know what exactly &lt;strong&gt;Null (&lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;) and Alternate (&lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;) Hypotheses&lt;/strong&gt; are, why we &lt;strong&gt;only reject the null but never accept it&lt;/strong&gt;, what is &lt;strong&gt;p-value&lt;/strong&gt; or &lt;strong&gt;why do we need hypothesis testing&lt;/strong&gt; at all, continue reading. So‚Ä¶&lt;/p&gt;
&lt;h2 id="why-do-we-actually-need-hypothesis-testing"&gt;Why do we actually need hypothesis testing?&lt;/h2&gt;
&lt;p&gt;The short answer: &lt;strong&gt;TO MAKE SENSE OF THE DATA.&lt;/strong&gt; The long answer ‚Ä¶ where should I start?&lt;/p&gt;
&lt;p&gt;I‚Äôll start with the first time I have got my own data. I was super motivated, because this data seemed to be a golden ticket to writing a thesis and finally getting a degree. The only thing I needed to do is &lt;strong&gt;to analyze this data&lt;/strong&gt;. I couldn‚Äôt wait to start working, so I opened my Excel table and started to look at it. However, the more I looked at it, the more puzzled I became. &lt;strong&gt;How the heck does one actually analyzes data?&lt;/strong&gt; I asked my boss, and she said:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‚ÄúJust test some hypothesis.‚Äù&lt;/li&gt;
&lt;li&gt;‚ÄúCool! Thanks! That helps!‚Äù - I said and slowly started to panic üò±, because I had no idea how hypothesis testing works. So, I had no choice but to figure it out! What a pain!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, after a bit of research I realized that the &lt;strong&gt;hypothesis testing&lt;/strong&gt; is not a pain at all, but &lt;strong&gt;is actually a huge help for analyzing data!&lt;/strong&gt; And here is why. Having a data - we want to make a &lt;strong&gt;claim or a statement&lt;/strong&gt;. For example: sport reduces weight. This &lt;strong&gt;claim IS our hypothesis&lt;/strong&gt;. Having several claims creates a compelling story for a thesis or a scientific paper. There is only one problem with that: why should anyone believe our story? This could be a science fiction story. Thus, in order to make it a real science, not science fiction, we need to make it believable and solid. How? Well, we &lt;strong&gt;test every of our hypothesis against the null hypothesis.&lt;/strong&gt; Wait, what the hell is the null hypothesis?&lt;/p&gt;
&lt;h2 id="null-hypothesis-and-alternative-hypothesis"&gt;Null Hypothesis and Alternative Hypothesis&lt;/h2&gt;
&lt;p&gt;The null hypothesis is ‚Ä¶ absolutely, &lt;strong&gt;nothing! Empty, boring, zero, null!&lt;/strong&gt;. For instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;when &lt;strong&gt;nothing&lt;/strong&gt; happens,&lt;/li&gt;
&lt;li&gt;when there is &lt;strong&gt;no effect&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;when there is &lt;strong&gt;no difference&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;when &lt;strong&gt;nothing new&lt;/strong&gt; was learned&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, if our research hypothesis is - &lt;strong&gt;sport reduces weight&lt;/strong&gt;, then our null hypothesis is - &lt;strong&gt;sport DOES NOT reduce weight&lt;/strong&gt;. So, we actually need &lt;strong&gt;ONLY these TWO hypothesis&lt;/strong&gt; to make any of our claims solid! And since our research hypothesis is the only &lt;strong&gt;Alternative&lt;/strong&gt; to the null hypothesis, it is often called &lt;strong&gt;the Alternative Hypothesis&lt;/strong&gt;. Let‚Äôs have 3 examples and clearly &lt;strong&gt;define both Null and Alternative hypotheses.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="theoretical-examples"&gt;3 theoretical examples&lt;/h3&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;if you measure the effect of sports on muscles:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;muscle gain (or measurable effect)&lt;/strong&gt; is your research or alternative hypothesis &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NO muscle gain (NO effect)&lt;/strong&gt; is your null hypothesis &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/effect.jpg" /&gt;&lt;/p&gt;
&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;if you don‚Äôt believe previous studies, or accepted value, for example that the average weight loss from a fancy diet is 3 kilos per week, you make an experiment in order to test this accepted value. Then:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;average weight loss ‚â† 3 kg/week&lt;/strong&gt; is our &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;average weight loss = 3 kg/week&lt;/strong&gt; is our &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/weight_loss.jpg" /&gt;&lt;/p&gt;
&lt;ol start="3" style="list-style-type: decimal"&gt;
&lt;li&gt;if you study any difference between anything (groups, treatments etc.):&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NO difference&lt;/strong&gt; is our &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;THERE IS a difference&lt;/strong&gt; is our &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/difference.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NO difference&lt;/strong&gt; expressed mathematically means - the difference is equal to &lt;em&gt;zero&lt;/em&gt;. Writing this null hypothesis down in a simple formula makes it even easier to understand:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[ average \ 1 - average \ 2 = 0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and if we add &lt;span class="math inline"&gt;\(average \ 2\)&lt;/span&gt; to both sides of the formula, we‚Äôll get:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[ average \ 1 = average \ 2\]&lt;/span&gt; ‚Ä¶which, again, means that if the difference between samples &lt;span class="math inline"&gt;\(= 0\)&lt;/span&gt;, than the averages of both groups are the same.&lt;/p&gt;
&lt;p&gt;For the &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt; it would be:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[ average \ 1 - average \ 2 ‚â† 0\]&lt;/span&gt; &lt;span class="math display"&gt;\[ average \ 1 ‚â† average \ 2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;‚Ä¶which is saying that samples (or their averages) differ. So, &lt;strong&gt;the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt; are always mathematical opposites!&lt;/strong&gt; Which makes the &lt;strong&gt;hypothesis testing really simple&lt;/strong&gt;:&lt;/p&gt;
&lt;h3 id="summary-for-h_0-and-h_a"&gt;Summary for &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;in order to answer a question or make a solid claim from our data &lt;strong&gt;we only need two hypotheses&lt;/strong&gt;, &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; and its only alternative - &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;your question (claim or statement) is in fact your &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the best part of the &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt; is that we can create tons of &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;, because we can ask thousands of different questions, or make thousands of different claims.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;but &lt;strong&gt;we always have only one &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the beauty of the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; is that it is always &lt;strong&gt;the mathematical opposite&lt;/strong&gt; of the &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;, doesn‚Äôt matter what the alternative is. The &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt; is always what &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; is NOT&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;and the best part of the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; is that we do not need any preliminary data for it. It‚Äôs always there, e.g.¬†&lt;strong&gt;NO difference&lt;/strong&gt; or &lt;strong&gt;NO effect&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ok, we just learned about what &lt;strong&gt;hypotheses&lt;/strong&gt; are, but how do we test them? And what‚Äôs the point of &lt;strong&gt;testing&lt;/strong&gt; anyway? Well‚Ä¶&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The only goal&lt;/strong&gt; of hypothesis testing is to &lt;strong&gt;(1) reject or (2) fail to reject the null hypothesis&lt;/strong&gt;. There are really &lt;strong&gt;only these two possible outcomes&lt;/strong&gt;. We can &lt;strong&gt;never accept the null&lt;/strong&gt; hypothesis. Why? Let‚Äôs figure out on some examples?&lt;/p&gt;
&lt;h2 id="never-accept-null-hypothesis-only-try-to-refect"&gt;Never accept null hypothesis! Only try to refect!&lt;/h2&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;Imagine you travel to a completely new country for hiking. You discover a beautiful lake and start to fish. After 3 hours of fishing you didn‚Äôt catch anything and you even failed to have a single bite. Can you accept the null hypothesis that &lt;strong&gt;there is NO fish in that lake&lt;/strong&gt;? Of course not! You only failed to reject it during 3 hours. May be you just need to fish a little longer. So, you fish for another hour and finally catch your first fish. In that case the null hypothesis that &lt;strong&gt;there is NO fish in that lake&lt;/strong&gt; is destroyed by the ‚Äúfishy‚Äù evidence. That‚Äôs why &lt;strong&gt;we cannot accept the null hypothesis until we reject it; we can only fail to reject it&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now imagine you explore life on an alien planet. Your &lt;strong&gt;null hypothesis (&lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;)&lt;/strong&gt; is that &lt;strong&gt;there is NO life on this planet&lt;/strong&gt;. You drive around a few hours and look for life. If you see any alien, you can happily reject the null hypothesis in favor of the alternative, because further believing the null hypotheses that &lt;strong&gt;there is NO life on the planet&lt;/strong&gt; after you just saw one - seems ridiculous. But if you don‚Äôt see any aliens, can you definitively say that there is no alien life on this planet, or accept the null hypotheses? Again, no! Because if we would have explored longer we might have found an alien life. And since we are not sure about the absence of life on this new planet &lt;strong&gt;we cannot accept the null hypothesis; we can only fail to reject it&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The last intuitive example comes from trial courts. If you are accused of a crime, you presumed to be &lt;strong&gt;NOT-guilty&lt;/strong&gt;. So, the null hypothesis is that - &lt;strong&gt;you are NOT guilty&lt;/strong&gt;. The court tries to reject it. If there is a clear evidence against you, like a videotape, it would be ridiculous to still believe the null hypothesis, that &lt;strong&gt;you are NOT guilty&lt;/strong&gt;, because the evidence is recorded, so, the judges can reject the null hypothesis that &lt;strong&gt;you are NOT guilty&lt;/strong&gt; in favor of the alternative hypothesis that &lt;strong&gt;you are actually guilty&lt;/strong&gt;. But if there is not enough evidence against you, for example no tape or no fingerprints, the judges cannot say &lt;strong&gt;you are guilty&lt;/strong&gt;. But they also can NOT say &lt;strong&gt;you are NOT guilty&lt;/strong&gt;! Imagine, you did actually commit a crime. But nobody can prove it, because there is absolutely no evidence. May be if Sherlock Holmes looked for evidence, he would have found some. But until then, judges can not accept that - &lt;strong&gt;you are NOT guilty&lt;/strong&gt;, they only can fail to reject the presumption that - &lt;strong&gt;you are NOT guilty&lt;/strong&gt;. In other words, judges can‚Äôt say &lt;strong&gt;you are innocent&lt;/strong&gt;, but rather &lt;strong&gt;you are STILL NOT guilty&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Everything is clear in the case of aliens on a new planet or fish in the lake. If you found any evidence against the null hypothesis, you simply reject it. But comparing averages of two groups is a bit more complex? Remember our very first hypothesis - sport reduces weight? Let‚Äôs test that one.&lt;/p&gt;
&lt;h2 id="how-far-is-far-enough"&gt;How far is far enough?&lt;/h2&gt;
&lt;p&gt;We first collect some data from people who did not exercise for one year. Some people in this group gained a bit of weight, because they eat a lot, while some people lost a bit of weight, because they were sick or on a diet. Despite this mild random variable &lt;strong&gt;ON AVERAGE the weight change in this group was zero&lt;/strong&gt;. And that‚Äôs our &lt;strong&gt;Null Hypothesis - NO change in weight.&lt;/strong&gt; We‚Äôll call this group - a control group.&lt;/p&gt;
&lt;p&gt;Then we‚Äôll find another group or people who has exercised for one year and measure their weight before and after one year of exercise. Well, imagine we have an average loss in weight of only 1 kilo. It‚Äôs not 0 change, but it‚Äôs too close to 0 for being taken seriously. It could be just due to the same random variation we have had in the control group. Then, imagine we sampled a different group where people lost 3 kilos on average. Here I am starting to feel uncomfortable to think that the change is close to 0, and I start to think whether I should reject the null hypothesis. Finally, the last group of people who exercised for one year lost 10 kilos on average. Now, it‚Äôs soo far away from 0 that the &lt;strong&gt;null hypothesis - that there is NO change in weight&lt;/strong&gt; seems ridiculous. Therefore we‚Äôll reject it in favor of the alternative hypothesis - &lt;strong&gt;sport reduces weight&lt;/strong&gt;. Interestingly, for the weight loss of 10 kilograms everyone would agree that sport works, while for a loss of only 1 kilo people would agree that sport does not work. But for 3 kilos some people would say yes while some would say no. And all their opinions would be highly subjective and not concrete, making us not confident about our conclusion any more. Well, &lt;strong&gt;hypothesis testing offers a concrete way to decide&lt;/strong&gt; when we reject and when we fail to reject the null hypothesis. And that is where &lt;strong&gt;p-value&lt;/strong&gt; comes into play and help us solve this problem numerically.&lt;/p&gt;
&lt;div id="hello" class="greeting message" style="color: blue;"&gt;
&lt;p&gt;The &lt;strong&gt;p&lt;/strong&gt; stands for the &lt;strong&gt;probability&lt;/strong&gt;. And it refers to the probability that we would have gotten the results we did &lt;strong&gt;just by chance&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In order to understand it better, let‚Äôs test the last hypothesis (comming from on an amazing YouTuber - MrNystrom):&lt;/p&gt;
&lt;p&gt;If you throw a fair coin, you have only 2 possible outcomes: tales or heads. The chances to get tails are 50%. Clear and boring result. The chances to get 2 tails in a row are 25%, because we have 4 possible outcomes: 1/4 = 25%. It‚Äôs pretty likely to get 2 tails in a row. So, nothing unusual here. The coin must be fair. However, 3 tails in a row starts to feel strange. The chances to get 3 tails in a row are low, 1/8 = 12.5%, but still possible. But the chances to get 4 tails in a row are only 6.25%. And if we get 4 tails in a row we start to doubt whether the coin is actually fair. And if you get 6 tails in a row despite only 1.5% chances (probability) to get them, the null hypothesis that &lt;strong&gt;this is a fair coin&lt;/strong&gt; would seem ridiculous and you‚Äôll reject it, because it‚Äôs simply too unlikely to happen randomly (or by chance).&lt;/p&gt;
&lt;p&gt;A widely accepted, but by no means the best, cut off for a p-value is 5%. That means, that if your p-value (your probability that would have gotten the results you did &lt;strong&gt;just by chance&lt;/strong&gt;) is below 0.05, you can reject the null hypothesis, while if a p-value is above 0.05, you fail to reject the null. So, &lt;strong&gt;p-values provide concrete boundaries for making a decision about hypothesis testing and are therefore useful.&lt;/strong&gt; However, p-values are one of the most misunderstood and misused concepts in statistics. Thus, p-values deserve a separate video. Here I would like to point out only one, but the most frequent misuse of p-values, which, if eliminated could dramatically increase the quality of science.&lt;/p&gt;
&lt;p&gt;Researchers always want to reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; in favour of the research hypothesis, because that would mean - that they found something interesting or new. But if the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; can not be rejected, it seems like a tragedy and failure. All the efforts of making experiments, collecting and analyzing data seem useless and there is nothing to be published. However, &lt;strong&gt;it‚Äôs not true! We always can say - we learned nothing new.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;goal of science is NOT to find an effect or a difference, but to figure out whether there is an effect or difference&lt;/strong&gt;. If there is NO effect, it is a perfectly valid and equally important result. But some scientists insist on finding something significant and continue looking for it till they found it, a phenomenon known as - &lt;strong&gt;p-hacking&lt;/strong&gt;. Well, p-hacking also deserves a separate video. Until then I‚Äôd love to finish with the quote of Cassie Kozyrkov:&lt;/p&gt;
&lt;h2 id="stop-trying-to-learn-something"&gt;Stop trying to learn something!&lt;/h2&gt;
&lt;div id="hello" class="greeting message" style="color: blue;"&gt;
&lt;p&gt;‚Äú&lt;strong&gt;You should get into the habit of learning nothing more often, because if you insist on learning something beyond the data every time you test hypotheses, you will learn something stupid.&lt;/strong&gt;‚Äù&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="whats-next"&gt;What‚Äôs next?&lt;/h2&gt;
&lt;p&gt;You should definitely get an intuitive understanding of p-values!&lt;/p&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=VK-rnA3-41c&amp;amp;t=1s&amp;amp;ab_channel=MathandScience" class="uri"&gt;https://www.youtube.com/watch?v=VK-rnA3-41c&amp;amp;t=1s&amp;amp;ab_channel=MathandScience&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=0oc49DyA3hU&amp;amp;ab_channel=StatQuestwithJoshStarmer" class="uri"&gt;https://www.youtube.com/watch?v=0oc49DyA3hU&amp;amp;ab_channel=StatQuestwithJoshStarmer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/hackernoon/statistical-inference-in-one-sentence-33a4683a6424" class="uri"&gt;https://medium.com/hackernoon/statistical-inference-in-one-sentence-33a4683a6424&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@kozyrkov?source=post_page-----33a4683a6424-------------------" class="uri"&gt;https://medium.com/@kozyrkov?source=post_page-----33a4683a6424-------------------&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=-MKT3yLDkqk" class="uri"&gt;https://www.youtube.com/watch?v=-MKT3yLDkqk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=5koKb5B_YWo&amp;amp;ab_channel=StatQuestwithJoshStarmer" class="uri"&gt;https://www.youtube.com/watch?v=5koKb5B_YWo&amp;amp;ab_channel=StatQuestwithJoshStarmer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=zR2QLacylqQ&amp;amp;ab_channel=zedstatistics" class="uri"&gt;https://www.youtube.com/watch?v=zR2QLacylqQ&amp;amp;ab_channel=zedstatistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=tLM7xS6t4FE&amp;amp;ab_channel=SciShow" class="uri"&gt;https://www.youtube.com/watch?v=tLM7xS6t4FE&amp;amp;ab_channel=SciShow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>4475a79d2a2548787f9f4e63174e7589</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>What is p-value and why we need it (in progress)</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/Dldutns6Tig" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;Since p-values are mostly used for testing hypothesis, you should definitely check out the previous post - &lt;a href="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/"&gt;hypothesis testing&lt;/a&gt;, where I superficially introduced the p-value already.&lt;/p&gt;
&lt;h2 id="p-value-definition-n1-its-a-probability"&gt;P-value definition N¬∞1: it‚Äôs a probability&lt;/h2&gt;
&lt;p&gt;The most intuitive explanation of p-values I have found came from Andrew Vickers‚Äô book ‚ÄúWhat is a p-value anyway? 34 Stories to Help You Actually Understand Statistics‚Äù: &lt;strong&gt;‚ÄúP-value is the probability of the toothbrush being dry if you‚Äôve just cleaned your teeth.‚Äù&lt;/strong&gt; Let‚Äôs start with that.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;P&lt;/strong&gt; stands for &lt;strong&gt;probability&lt;/strong&gt;. And it refers to &lt;strong&gt;the probability to observe the results we did just by chance&lt;/strong&gt;. In order to understand this definition a little better, let‚Äôs have an example:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/coin.png" /&gt;&lt;/p&gt;
&lt;p&gt;If we throw a fair coin, we have only &lt;strong&gt;2 possible outcomes&lt;/strong&gt;: heads or tales. The chances to get &lt;strong&gt;tails are obviously 50%&lt;/strong&gt;. Then we throw our coin a second time. The chances to get &lt;strong&gt;2 tails in a row are 25%&lt;/strong&gt;, because we have 4 possible outcomes after two throws, with only one of those four outcomes having &lt;strong&gt;two tails in a row&lt;/strong&gt;, so 1/4 = 0.25 = 25%. It‚Äôs actually pretty likely to get 2 tails in a row. So, nothing unusual here. The coin must be fair. But 3 tails in a row starts to feel strange. The chances to get &lt;strong&gt;3 tails in a row are low, only 12.5%&lt;/strong&gt;, but still possible. However, if we get 3 tails in a row we start to doubt whether the coin is actually fair. And if we get &lt;strong&gt;6 tails in a row despite only 1.5% probability to get them&lt;/strong&gt;, the null hypothesis - &lt;em&gt;that this is a fair coin&lt;/em&gt; - would seem ridiculous and we‚Äôll reject it, because it‚Äôs simply &lt;strong&gt;too unlikely to happen randomly (or by chance)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/6_tails.png" /&gt;&lt;/p&gt;
&lt;p&gt;This example, which defines the p-value as &lt;strong&gt;ONE particular&lt;/strong&gt; probability of having &lt;strong&gt;only tales&lt;/strong&gt;, is clear, intuitive, but &lt;strong&gt;only ONE sided&lt;/strong&gt;. It isn‚Äôt wrong, but &lt;strong&gt;ONE-sided p-values&lt;/strong&gt; coming from &lt;strong&gt;only ONE probability&lt;/strong&gt; is not what people use. The &lt;strong&gt;‚Äúreal p-values‚Äù&lt;/strong&gt; are &lt;strong&gt;TWO sided&lt;/strong&gt;. Then why did we learned about one-sided p-values at all, you might ask??? Well, it was totally necessary for a better understanding of the second definition of p-values. And this is actually one of those definitions from the book, where people start to run away from statistics :) namely:&lt;/p&gt;
&lt;div id="hello" class="greeting message" style="color: blue;"&gt;
&lt;p&gt;P.S.: &lt;strong&gt;One-sided p-values are rare and dangerous&lt;/strong&gt;, so, please don‚Äôt use them to avoid confusion, especially if you just started to learn about p-values.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="p-value-definition-n2-three-probabilities-learned-from-josh-starmer"&gt;P-value definition N¬∞2: three probabilities (learned from &lt;a href="https://www.youtube.com/watch?v=5Z9OIYA8He8&amp;amp;ab_channel=StatQuestwithJoshStarmer"&gt;Josh Starmer!&lt;/a&gt;)&lt;/h2&gt;
&lt;p&gt;P-value is the probability to get &lt;strong&gt;(1) the data we have collected, (2) as extreme or (3) more extreme data&lt;/strong&gt; just by chance, assuming our null hypothesis is true.&lt;/p&gt;
&lt;p&gt;Two tailed p-values are determined by adding up &lt;strong&gt;three probabilities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/3_probs.png" /&gt;&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;Remember our coin example? The probability of having &lt;em&gt;two tails&lt;/em&gt; in a row is 25% (or 0.25), because there are 4 equally possible outcomes after flipping a coin 2 times. These 4 possible outcomes is our &lt;em&gt;distribution.&lt;/em&gt; And &lt;strong&gt;the data we have got&lt;/strong&gt; - &lt;em&gt;two tails&lt;/em&gt; - is only one side of this distribution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Likewise, the probability of getting &lt;em&gt;two heads&lt;/em&gt; is also 0.25, and since &lt;em&gt;two heads&lt;/em&gt; are &lt;strong&gt;as extreme&lt;/strong&gt; as &lt;em&gt;two tales&lt;/em&gt;, we have to add the probability of &lt;em&gt;two tails&lt;/em&gt; to the probability of &lt;em&gt;two heads&lt;/em&gt;: 1/4 + 1/4 = 2/4 = 0.5. Now we have a &lt;strong&gt;two sided probability&lt;/strong&gt; but the p-values still need one last part.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start="3" style="list-style-type: decimal"&gt;
&lt;li&gt;The third and the last part of the p-value is - the probability of observing &lt;strong&gt;something rarer or more extreme&lt;/strong&gt;. In this case it is 0, because &lt;strong&gt;no other outcomes are rarer then two tails or two heads&lt;/strong&gt;. Thus, if we add this 0 to the probability of having either two tales or two heads, we‚Äôll get a final p-value of 0.5. And since our p-value is way above the significance threshold of 0.05, we failed to reject the null hypothesis, so that our null hypothesis is still TRUE - our coin must be fare.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus: the p-value is the probability of getting the &lt;strong&gt;(1) data we collected (two tails = 0.25), as extreme (two heads = 0.25) or more extreme data (0) is&lt;/strong&gt;: 0.25 + 0.25 + 0 = 0.5.&lt;/p&gt;
&lt;p&gt;Now let‚Äôs calculate the p-value for a slightly more complicated outcome: &lt;strong&gt;3 heads and 1 tail&lt;/strong&gt;. This is one of the 16 possible outcomes, when we flip the coin 4 times. The null hypothesis would be that our data - 3 heads and 1 tail - is nothing special, but belongs to this exact distribution.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/3h1t.png" /&gt;&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;The probability we randomly get &lt;strong&gt;3 heads and 1 tail&lt;/strong&gt; is 4/16 = 0.25. That would be &lt;strong&gt;our data&lt;/strong&gt; and with that &lt;strong&gt;the first part of the p-value&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The probability that we randomly get something equally rare, namely &lt;strong&gt;3 tails and 1 head&lt;/strong&gt;, is also 4/16 = 0.25, which will be &lt;strong&gt;the second part of the p-values&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The probability that we randomly get something rarer or more extreme, which in our case are &lt;strong&gt;4 heads and 4 tails&lt;/strong&gt; is 2/16 = 0.125, and are &lt;strong&gt;the last part of our p-value&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Adding them all together would result into a p-value of 0.625, so we would fail to reject the null hypothesis - that our data (3 heads and 1 tail) is nothing special, and would need to conclude that 3 heads and 1 tail belong to this distribution.&lt;/p&gt;
&lt;h2 id="p-value-definition-n3-cumulative-probability-area-under-the-curve"&gt;P-value definition N¬∞3: cumulative probability (area under the curve)&lt;/h2&gt;
&lt;p&gt;Calculating a p-value from discrete numbers is kind of easy, but how do we do it for continuous numbers? Plotting our discrete data will help to understand that. If we plot our values from the last example, we would find the &lt;strong&gt;most probable&lt;/strong&gt; outcomes (&lt;span style="color: green;"&gt;green&lt;/span&gt;) in the middle, a bit &lt;strong&gt;less probable&lt;/strong&gt; (black and &lt;span style="color: blue;"&gt;blue&lt;/span&gt;) on both sides of the middle, and finally &lt;strong&gt;the least probable&lt;/strong&gt; values (&lt;span style="color: red;"&gt;red&lt;/span&gt;) very far away from the middle. This will become &lt;strong&gt;the probability distribution of our discrete values&lt;/strong&gt;. We can also cover plotted values with a &lt;strong&gt;curve&lt;/strong&gt; to better visualize this distribution. Now, if we mark our discrete values with discrete numbers on the x-axis, we‚Äôll see that we‚Äôll be able to add any &lt;strong&gt;continuous value&lt;/strong&gt; in between those discrete numbers, e.g.¬†0.3 or 0.333. And for every of these &lt;strong&gt;continuous value&lt;/strong&gt; we can calculate &lt;strong&gt;probability&lt;/strong&gt; in the exact same way as we just did for &lt;em&gt;2 tails&lt;/em&gt;, or &lt;em&gt;3 heads and 1 tail&lt;/em&gt;. That‚Äôs how we arrive at the &lt;strong&gt;continuous probability&lt;/strong&gt;. The p-values for such continuous probability are also calculated in the same way. But instead of adding up only 3 discrete probabilities (our data, as extreme and more extreme data), we add up 2 discrete parts (our data and similarly extreme data) and 1 continuous part (more extreme data from both tails of the distribution):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;and since we calculate probabilities of many points &lt;strong&gt;under the curve&lt;/strong&gt; and&lt;/li&gt;
&lt;li&gt;add all of them together, or &lt;strong&gt;ACCUMULATE those probabilities under the curve&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;we‚Äôll get the &lt;strong&gt;AREA under the curve&lt;/strong&gt; - which then &lt;strong&gt;IS&lt;/strong&gt; our p-value as a &lt;strong&gt;cumulative probability&lt;/strong&gt;, including all the values which are as extreme or more extreme than our data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/distribution.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;An example of such curve could be the weight of people. For instance, the average weight of German males is 80 kilos with 95% of them weighting between 70 and 90 kilos (these numbers are totally made up ;). 2.5% of them will weight &amp;lt;70 and 2.5% will weight &amp;gt;90 kilos. If we get a new data point - namely a weight of a new person, we can calculate the p-value for that point. Our null hypothesis is - that the persons weight is &lt;strong&gt;NOT different&lt;/strong&gt; from our distribution. If this person weights 75 kilos, the area under the curve (or a p-value!), which includes all the values &lt;strong&gt;as extreme or more extreme&lt;/strong&gt; than 75, will be ca. 0.6 (or 60%), which is fairly big. So, we would fail to reject the null hypothesis, concluding that the person could be a German male, or is at least not different from German males. However, if the weight is 65 kilos, the area under the curve is quite small, let‚Äôs say 2%. Here, we can reject the null hypothesis and conclude that this person must belong to a different population, e.g.¬†women, or a man from a different country.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So you see the p-value is not a simple probability, but 3 probabilities (parts) added together!&lt;/strong&gt; A simple probability is the number of outcomes of interest, divided by the total number of outcomes (it‚Äôs not even the one-sided p-value). While &lt;strong&gt;a p-value is the probability that random chance generated the&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;data we observed, or the data that is&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;equally rare or 3. rarer for &lt;strong&gt;discrete&lt;/strong&gt; values, or&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;equally extreme or 3. more extreme for &lt;strong&gt;continuous&lt;/strong&gt; values,&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;if the null hypothesis is true.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="p-value-definition-n4-original-fishers-definition"&gt;P-value definition N¬∞4: original Fisher‚Äôs definition&lt;/h2&gt;
&lt;p&gt;All right, &lt;strong&gt;p-value is a cumulative probability&lt;/strong&gt;. And since a probability is a &lt;strong&gt;continues&lt;/strong&gt; measure from 0 to 1 (or from 0% to 100%), the &lt;strong&gt;p-value also IS any number between 0 &amp;amp; 1&lt;/strong&gt;. If we look for a difference between two groups, we can see a p-value &lt;strong&gt;as a measure of similarity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/similarity.png" /&gt;&lt;/p&gt;
&lt;p&gt;For examples if two samples are identical, there is &lt;strong&gt;100% similarity and 0% difference&lt;/strong&gt;. So, our p-value is equal to 1. If &lt;strong&gt;similarity is only 60%, then the difference is 40%&lt;/strong&gt;, which is much bigger then 0, but is still small. But &lt;strong&gt;if the similarity drops to 5% or below, then the difference of 95% is often considered significant&lt;/strong&gt;, and we can confidently reject the null hypothesis - that there is NO difference between samples, in favour of the alternative hypothesis - that such difference exists.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But wait!!!&lt;/strong&gt;, does it make any sense to &lt;strong&gt;dichotomize a continuous number&lt;/strong&gt; (from 0 to 1) into only two categories - &lt;strong&gt;significant and not-significant? Of coarse, not!&lt;/strong&gt; The p-value is not black and white, but everything in between ‚Ä¶ like 50 shades of gray ‚Ä¶&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/Fifty_Shades_of_Grey_poster.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;‚Ä¶ ups, sorry, not that 50 shades of gray ‚Ä¶ that one :) &lt;span class="citation"&gt;(&lt;a href="#ref-Sterne2001" role="doc-biblioref"&gt;Sterne, Smith, and Cox 2001&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/interpretation.png" /&gt;&lt;/p&gt;
&lt;p&gt;Hmm, but if dichotomizing continuous p-values into two categories - &lt;strong&gt;significant and not-significant&lt;/strong&gt; - doesn‚Äôt make much sense, &lt;strong&gt;why is the whole world doing exactly that&lt;/strong&gt;? ‚ÄúThe basic explanation is neither philosophical nor scientific, but sociologic - &lt;strong&gt;everyone uses them&lt;/strong&gt;.‚Äù &lt;span class="citation"&gt;(&lt;a href="#ref-Goodman2019" role="doc-biblioref"&gt;Goodman 2019&lt;/a&gt;)&lt;/span&gt;. But in order to answer this question more thoroughly, we‚Äôd need to briefly explore the history of p-values ‚Ä¶&lt;/p&gt;
&lt;p&gt;Ronald Fisher, the father of modern statistics, saw the &lt;strong&gt;P value as an index measuring the strength of evidence against the null hypothesis&lt;/strong&gt; (see the picture above). Fisher himself proposed: &lt;strong&gt;‚Äúif P-value is between 0.1 and 0.9 there is certainly no reason to suspect the hypothesis tested. If it‚Äôs below 0.02 it is strongly indicated that the hypothesis fails to account for the whole of the facts.‚Äù&lt;/strong&gt; He sometimes used the threshold for the p-value of 0.05 (&amp;lt; 5%) himself to reject the null hypothesis, but &lt;strong&gt;without any clear reason&lt;/strong&gt;. Moreover, he recommended to treat a &lt;strong&gt;p-value around 0.05 as inconclusive, where we‚Äôd need to repeat the experiment.&lt;/strong&gt; The reason Fisher used p-values at all was - to reduce the probability of the &lt;strong&gt;type I error&lt;/strong&gt; - the probability to find something what is not there - &lt;strong&gt;false positive&lt;/strong&gt;, e.g.¬†that somebody has cancer, while being totally healthy. The most typical type I error happens when we measure a difference between two groups and find this difference to be significant, while both samples came from the same population:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/type_1_errors.png" /&gt;&lt;/p&gt;
&lt;p&gt;Having a stiff threshold of 0.05 can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Good&lt;/strong&gt;: because from all H0 you test, in the long run you will falsely reject at most 5% of the correct ones, but it‚Äôs also&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;BAD&lt;/strong&gt;: because it statistically guarantees that 1 in 20 healthy people, will ‚Äúhave‚Äù cancer just by chance, or in 1 out of 20 statistical tests we‚Äôll find nonsense just by chance.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, &lt;strong&gt;lowering the p-value to, let‚Äôs say 0.01, reduces the probability of finding nonsense and ensures we find something what is really there&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Along the &lt;strong&gt;type I error&lt;/strong&gt;, Neyman and Pearson also advocated the &lt;strong&gt;type II error&lt;/strong&gt;, that could be made in interpreting the results of an experiment. The type II error is the probability to miss something what is there - &lt;strong&gt;false negative&lt;/strong&gt;, e.g.¬†failing to detect cancer, when cancer is already in the body.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/type_2_errors.png" /&gt;&lt;/p&gt;
&lt;p&gt;Neyman and Pearson‚Äôs idea of &lt;strong&gt;hypothesis testing&lt;/strong&gt; was actually really good, because it supposed to reduce the number of mistakes by &lt;strong&gt;keeping the rates of both type I and type II errors low&lt;/strong&gt;. However, only the easy part of their approach ‚Äî that the null hypothesis can be rejected if P &amp;lt; 0.05 (type I error rate of 5%) ‚Äî has been widely adopted and stuck in the medical research, causing current dichotomy of results into &lt;strong&gt;significant&lt;/strong&gt; or &lt;strong&gt;not significant&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Two serious consequences of this are that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;potentially clinically important differences observed in small studies are denoted as non-significant and ignored, while&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;all significant findings are assumed to result from real treatment effects.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Well, how do we deal with type II errors? &lt;strong&gt;Increasing the sample size will increase the power of the experiment and therefore decrease the probability of type II error&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So, now we know that hypothesis testing can produce two types of mistakes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The type I error IS an ERROR, because p-values help to maintain nonsense&lt;/strong&gt;. While&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The type II error IS an ERROR, because p-values help to miss something important&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, &lt;strong&gt;we should stop dichotomising our results into significant and not significant&lt;/strong&gt;! But, if after reading this you still want to continue dichotomising your results into &lt;strong&gt;significant&lt;/strong&gt; or &lt;strong&gt;not significant&lt;/strong&gt;, please, answer a following question: &lt;strong&gt;is the difference between a P-value of 0.051 and 0.049 statistically significant?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The answer is - a clear NO! However, if using a p-value continuously totally blows your mind, and you still need some pragmatic decision making tool, consider using a hybrid approach, which singles out 5 instead of only 2 categories of &lt;strong&gt;the strength of the evidence against the null hypothesis in favor of the alternative:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P &amp;gt; 0.10 The data shows &lt;strong&gt;NO evidence&lt;/strong&gt; against the null hypothesis&lt;/li&gt;
&lt;li&gt;0.05 &amp;lt; P &amp;lt; 0.10 &lt;strong&gt;Weak evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as a dot: ‚Äò.‚Äô&lt;/li&gt;
&lt;li&gt;0.01 &amp;lt; P &amp;lt; 0.05 &lt;strong&gt;Moderate evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as one star: ‚Äò*‚Äô&lt;/li&gt;
&lt;li&gt;0.001 &amp;lt; P &amp;lt; 0.01 &lt;strong&gt;Strong evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as two stars: ‚Äò**‚Äô&lt;/li&gt;
&lt;li&gt;P &amp;lt; 0.001 &lt;strong&gt;Very strong evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as three stars: ‚Äò***‚Äô&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="dont-follow-any-cut-off-the-cut-off-should-follow-you"&gt;Don‚Äôt follow any cut-off, the cut-off should follow you!&lt;/h2&gt;
&lt;p&gt;Any cut-off splits the probability into two parts:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level of confidence&lt;/strong&gt; - beta (Œ≤), e.g.¬†95%, tells how confident are we in our decision and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level of significance&lt;/strong&gt; - alpha (‚ç∫), e.g.¬†5%, tells us when to &lt;strong&gt;reject or fail to reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;&lt;/strong&gt;. Namely, (1) if p-value &amp;lt;= ‚ç∫, we reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;; (2) if p-value &amp;gt; ‚ç∫, we fail to reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both &lt;strong&gt;‚ç∫&lt;/strong&gt; &amp;amp; &lt;strong&gt;Œ≤&lt;/strong&gt; add up to 1 and tell you the same thing - how sure are you about your decision:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for 95% confidence: ‚ç∫ = 1 - Œ≤ = 1 - 0.95 = 0.05&lt;/li&gt;
&lt;li&gt;for 99% confidence: ‚ç∫ = 1 - Œ≤ = 1 - 0.99 = 0.01&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;cut-off itself is up to you&lt;/strong&gt; and is &lt;strong&gt;highly dependent on the experiment&lt;/strong&gt;. It might sound fuzzy, but it actually gives you a freedom to adjust the decision-making to reality. On the other hand, clinging to the cut-off of 0.05 may actually increase the rates of both type I and type II errors. Let‚Äôs have a look at two examples:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;Imagine you study a new treatment for a deadly disease and your p-value for the difference between control and treatment groups is 0.15. If you strictly follow the 0.05 cut-off, you‚Äôll fail to reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; and would conclude that the treatment does not work. The p-value of 0.15 makes you ‚Äú&lt;em&gt;only&lt;/em&gt;‚Äù 85% sure that such difference exists, making this difference ‚Äú&lt;em&gt;not significant enough&lt;/em&gt;.‚Äù However, if you don‚Äôt &lt;span style="color: red;"&gt;blindly&lt;/span&gt; follow the 0.05, you could &lt;span style="color: blue;"&gt;look&lt;/span&gt; at your experiment in a completely new way by saying: &lt;strong&gt;I am 85% confident that new treatment works and we found a new way of treating a deadly disease&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Similarly, if I was 85% confident of winning the lottery I would definitely play! However, if I‚Äôd strictly follow the cut-off of 0.05, I would ‚Äú&lt;em&gt;be not confident enough&lt;/em&gt;‚Äù that I will win (which is ridiculous) and I won‚Äôt play.&lt;/p&gt;
&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;On the opposite side, if an airline company welcomes you aboard with a catchy phrase: ‚ÄúWe are 95% confident that you‚Äôll survive, so your probability of crashing with us is below 5% (p &amp;lt; 0.05),‚Äù would you go aboard? I personally would run away! The airline company would chase me though and scrim: "Stop running away! We &lt;strong&gt;successfully rejected the Null Hypothesis that you will die!!!&lt;/strong&gt; But I would only be willing to fly .. if my level of confidence in survival increases to 99.9999% or if my chances to die would become 1 to a million (‚ç∫ = 0,000001) flights.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, you see?, the freedom to set up your own level of significance (the cut-off) depending on the situation makes you a better scientist, while blindly following the cut-off of 0.05 would either keep you pure or just kill you.&lt;/p&gt;
&lt;div id="hello" class="greeting message" style="color: blue;"&gt;
&lt;p&gt;P.S.: the dramatic effect of the last sentence serves solely educational purposes ‚Ä¶ while hoping to significantly (p &amp;lt; 0.05 üòâ) increase the &lt;strong&gt;probability&lt;/strong&gt; that you‚Äôll remember the material :)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Does it mean that any threshold is evil? Well, no! &lt;strong&gt;The threshold of 0.05 means that if there is no difference between Group 1 and Group 2, and if we did this exact experiment 100 times, then only 5 of these experiments would result in a wrong decision.&lt;/strong&gt; These would be 5 &lt;strong&gt;false positives&lt;/strong&gt; (type I error). Lowering the threshold to let‚Äôs say 0.001 would reduce the number of false positives to 1 out of 1000 experiments. &lt;strong&gt;But the costs of doing 1000 experiments are often much higher then it is worth.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="a-single-article-you-need-to-read-moving-to-a-world-beyond-p0.05"&gt;A single article you need to read: ‚ÄúMoving to a World Beyond ‚Äúp&amp;lt;0.05‚Äù "&lt;/h2&gt;
&lt;p&gt;If you want to know what statisticians themselves think about p-value, read the following article: &lt;a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913"&gt;Moving to a World Beyond ‚Äúp&amp;lt;0.05‚Äù&lt;/a&gt; &lt;span class="citation"&gt;(&lt;a href="#ref-Wasserstein2019" role="doc-biblioref"&gt;Wasserstein, Schirm, and Lazar 2019&lt;/a&gt;)&lt;/span&gt;. Here I would just sum up some recommendations the authors provided. These recommendations would enable you to find &lt;strong&gt;fewer false alarms (false positives, type I error), and to overlook fewer discoveries (false negatives, type II error)&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id="what-to-do"&gt;What to do:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;report precise continuous p-values&lt;/strong&gt; without reference to arbitrary thresholds. For example, P = 0.023 rather than P &amp;lt; 0.05.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;lower the 0.05 ‚Äú&lt;em&gt;statistical significance&lt;/em&gt;‚Äù threshold for claims of novel discoveries to a 0.005 threshold and refer to p-values between 0.05 and 0.005 as ‚Äú&lt;em&gt;suggestive&lt;/em&gt;.‚Äù&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;remember, &lt;strong&gt;the p-value as only one aspect of a complete data analysis&lt;/strong&gt;. Thus, supplement the p-value with visualizations of confidence intervals on effect sizes or likelihood ratios, justify the adequacy of the sample size and &lt;strong&gt;the reasons various statistical methods were employed&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;clarify objectives, invest into careful planning &amp;amp; design, invest into producing solid data, use more descriptive stats, use more of the regularized, robust, nonlinear and nonparametric methods for exploratory research, check the robustness of your methods and use several different data analysis techniques for testing the same hypothesis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;look for both (1) a small p-value and (2) a large effect size before declaring a result ‚Äúsignificant,‚Äù instead of dichotomized p-values alone.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;interpret the p-value in the context of sample size. A large study can detect a small, clinically unimportant finding. The larger the sample the more likely a difference to be detected. Lower the p-value threshold (e.g.¬†to 0.001) for large studies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;reduce unplanned and uncontrolled modeling/testing (p-hacking). &lt;strong&gt;Examining 20 associations will produce one result that is ‚Äúsignificant at P = 0.05‚Äù by chance alone.&lt;/strong&gt; Thus, testing multiple variables, using long questionnaires with hundreds of variables and measuring a wide range of potential outcomes, would &lt;strong&gt;guarantee several false positive findings&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;‚ÄúAccept uncertainty and embrace variation in effects‚Äù &lt;span class="citation"&gt;(&lt;a href="#ref-McShane2019" role="doc-biblioref"&gt;McShane et al. 2019&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="what-not-to-do"&gt;What not to do:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Don‚Äôt make decisions based solely on some arbitrary threshold such as p &amp;lt; 0.05. Dichotomizing p-values into ‚Äúsignificant‚Äù and ‚Äúnon significant‚Äù one loses information. A P-value is not black and white, but ‚âà 50 shades of grey üòÇ&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don‚Äôt treat &lt;strong&gt;‚Äúp = 0.051‚Äù and ‚Äúp = 0.049‚Äù as being categorically different. üôÇ As &lt;span class="citation"&gt;&lt;a href="#ref-Gelman2006" role="doc-biblioref"&gt;Gelman and Stern&lt;/a&gt; (&lt;a href="#ref-Gelman2006" role="doc-biblioref"&gt;2006&lt;/a&gt;)&lt;/span&gt; famously observed, the difference between ‚Äúsignificant‚Äù and ‚Äúnot significant‚Äù is not itself statistically significant.&lt;/strong&gt; Don‚Äôt discard a well-designed, excellently conducted, thoughtfully analyzed, and scientifically important experiment only because it failed to cross the threshold of 0.05 (e.g.¬†p-value = 0.09).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don‚Äôt believe that an association or effect exists just because it was ‚Äústatistically significant.‚Äù&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don‚Äôt believe that an association or effect is absent just because it was not ‚Äústatistically significant.‚Äù&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don‚Äôt conclude anything about practical importance based on ‚Äústatistical significance‚Äù (or lack thereof).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In fact - don‚Äôt say ‚Äústatistically significant‚Äù at all ‚Ä¶ whether expressed in words or by asterisks in a table.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don‚Äôt believe that your p-value gives the probability that your test hypothesis is TRUE. &lt;strong&gt;P-value is the probability of the data, not of (any) hypothesis!&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="what-about-bayesian-statistical-inference"&gt;What about Bayesian statistical inference?&lt;/h2&gt;
&lt;p&gt;The switch to the use of Bayesian statistical inference in medical research was proposed for several decades, but is barely possible. A major reason is that &lt;strong&gt;prior knowledge can be difficult to quantify&lt;/strong&gt;. The major reason for that is that science suppose to produce NEW knowledge. Thus, repeated experiments are necessary, but few scientist want to do them. Moreover, if the priors are flat (a wide range of values is considered to be equally likely), then the &lt;em&gt;Frequentist&lt;/em&gt; and &lt;em&gt;Bayesian&lt;/em&gt; results are similar, while computing power required for Bayesian analysis is ‚Äúsignificantly‚Äù ;) higher. Two approaches will give different results, however, if our prior opinion is not vague. Besides, we can‚Äôt ignore the fact that &lt;em&gt;Bayesian statistics&lt;/em&gt; is even less intuitive to the majority of scientists then &lt;em&gt;Frequentists statistics&lt;/em&gt;. Otherwise everybody would have already used it. Thus, until an alternative, simple and widely accepted approach to the p-values exists, banning p-values is of no help to anyone.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Most of the scientist can‚Äôt tell you what the p-value is. Most of the statisticians can, but can‚Äôt explain it to intuitively. So, don‚Äôt give yourself a hard time if you still a bit confused. It‚Äôs normal. Instead, I‚Äôd suggest we stop caring about understanding the p-value, but start caring to use it properly. And remember, &lt;strong&gt;you don‚Äôt need to understand how the car works, you only need to know how to drive&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/impossible-parking.gif" /&gt;&lt;/p&gt;
&lt;h2 id="whats-next-to-learn"&gt;What‚Äôs next to learn?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;misinterpretations of p-values (only if you are interested)&lt;/li&gt;
&lt;li&gt;effect size and&lt;/li&gt;
&lt;li&gt;power analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="further-readings-and-watchings"&gt;Further readings and watchings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;that‚Äôs an amazing video: &lt;a href="https://www.youtube.com/watch?v=tLM7xS6t4FE&amp;amp;ab_channel=SciShow" class="uri"&gt;https://www.youtube.com/watch?v=tLM7xS6t4FE&amp;amp;ab_channel=SciShow&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="http://www.stat.ualberta.ca/~hooper/teaching/misc/Pvalue.pdf" class="uri"&gt;http://www.stat.ualberta.ca/~hooper/teaching/misc/Pvalue.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-Gelman2006" class="csl-entry"&gt;
Gelman, Andrew, and Hal Stern. 2006. &lt;span&gt;‚Äú&lt;span class="nocase"&gt;The difference between "significant" and "not significant" is not itself statistically significant&lt;/span&gt;.‚Äù&lt;/span&gt; &lt;em&gt;American Statistician&lt;/em&gt; 60 (4). &lt;a href="https://doi.org/10.1198/000313006X152649"&gt;https://doi.org/10.1198/000313006X152649&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-Goodman2019" class="csl-entry"&gt;
Goodman, Steven N. 2019. &lt;span&gt;‚Äú&lt;span class="nocase"&gt;Why is Getting Rid of P-Values So Hard? Musings on Science and Statistics&lt;/span&gt;.‚Äù&lt;/span&gt; &lt;em&gt;American Statistician&lt;/em&gt; 73 (sup1). &lt;a href="https://doi.org/10.1080/00031305.2018.1558111"&gt;https://doi.org/10.1080/00031305.2018.1558111&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-McShane2019" class="csl-entry"&gt;
McShane, Blakeley B., David Gal, Andrew Gelman, Christian Robert, and Jennifer L. Tackett. 2019. &lt;span&gt;‚Äú&lt;span&gt;Abandon Statistical Significance&lt;/span&gt;.‚Äù&lt;/span&gt; &lt;em&gt;American Statistician&lt;/em&gt; 73 (sup1). &lt;a href="https://doi.org/10.1080/00031305.2018.1527253"&gt;https://doi.org/10.1080/00031305.2018.1527253&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-Sterne2001" class="csl-entry"&gt;
Sterne, Jonathan A. C., George Davey Smith, and D. R. Cox. 2001. &lt;span&gt;‚Äú&lt;span class="nocase"&gt;Sifting the evidence‚Äîwhat‚Äôs wrong with significance tests?&lt;/span&gt;‚Äù&lt;/span&gt; &lt;em&gt;BMJ&lt;/em&gt; 322 (7280). &lt;a href="https://doi.org/10.1136/bmj.322.7280.226"&gt;https://doi.org/10.1136/bmj.322.7280.226&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-Wasserstein2019" class="csl-entry"&gt;
Wasserstein, Ronald L, Allen L Schirm, and Nicole A Lazar. 2019. &lt;span&gt;‚Äú&lt;span class="nocase"&gt;Moving to a World Beyond "p &lt;span&gt;&amp;lt;&lt;/span&gt; 0.05"&lt;/span&gt;.‚Äù&lt;/span&gt; &lt;a href="https://doi.org/10.1080/00031305.2019.1583913"&gt;https://doi.org/10.1080/00031305.2019.1583913&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5>fbe358502d16d4b3c983e5217d5bc6d2</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation</guid>
      <pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>R package reviews {DataExplorer} explore your data!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data</link>
      <description>What is the best way to explore the data quick? I think it's visualization. And what it the best way to visualize the data quick? I think it's - {DataExplorer} package, because it can visualize all your data in seconds using only one function! Check this out...</description>
      <category>R package reviews</category>
      <category>EDA</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data</guid>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data/2.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Survival analysis 2: parametric survival models</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models</link>
      <description>The non-parametric Kaplan-Meier method (KM) can not describe survival probability by a smooth function, which means it can not predict anything. The parametric models (e.g. Exponential, Weibull etc.) can! Besides, in case where parametric models are appropriate, they are more exact, more effective and more informative than KM or Cox. However, unfortunately, this step is often left out due to the rear use of parametric models. In this post we‚Äôll try to close this gap.</description>
      <category>survival analysis</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models</guid>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models/thumbnail_survival_2.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {performance} check how good your model is! </title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is</link>
      <description>There are several indicators of model quality, e.g. $R^2$ or AIC, and several assumption for every model which supposed to be checked, e.g. normality of residuals, multicollinearity etc.. R provides solutions for every indicator or assumption you can imagine. However, they are usually spread around different packages and functions. {performance} package brings all of quality indicators and all of the assumption under one roof. Thus, for me it became the one-stop solution for modelling.</description>
      <category>R package reviews</category>
      <category>videos</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is</guid>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/14.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Survival analysis 1: a gentle introduction into Kaplan-Meier Curves</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves</link>
      <description>Survival time analysis is necessary in any study which investigates the time to a particular outcome of interest. Cancer studies in the medicine and the first failure of the car in the engineering field (failure time analysis) are good examples. The outcome of interest could be death, remission to relapse, progression, or failure. Point in time of reaching that outcome is generally called the event. Thank goodness, not every ‚Äúevent‚Äù is fatal üòÉ, but can sometimes even be a favorable outcome such as discharge from hospital. And thus, survival analysis is also a generic term, because it is not only about survival.</description>
      <category>survival analysis</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves</guid>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves/thumbnail_survival_1.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {janitor} clean your data!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data</link>
      <description>Data Scientists spend up to 80% of their time cleaning and preparing data for analysis. " Happy families are all alike; every unhappy family is unhappy in its own way" ‚Äî Leo Tolstoy. "Like families, tidy datasets are all alike but every messy dataset is messy in its own way" - Hadley Wickham. Thats when "janitor" helps to clean the mess.</description>
      <category>R package reviews</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data</guid>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data/11.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>How to visualize models, their assumptions and post-hocs</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs</link>
      <description>A picture is worth a thousand words! This article shows how to visualize results of 16 different models in R: from a simple linear model to a multiple-additive-non-linear-mixed-effects model. Among them are logistic, multinomial, additive and survival models with and without interactions. **Goal: minimum R code &amp; maximum output!** We'll also go a bit beyond only model visualization. So, don't miss the bonuses üòâ.</description>
      <category>visualization</category>
      <category>videos</category>
      <category>models</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs</guid>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/thumbnail_visualize_models.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>How to create a blog or a website in R with {Distill} package</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package</link>
      <description>If you're not online, you don't exist. A personal webpage or a blog became the business card of the digital century. It shows who you are and what you are capable of. Thus: show, don't tell.</description>
      <category>R &amp; the Web</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package</guid>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/images/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
  </channel>
</rss>
