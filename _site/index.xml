<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>yuzaR-Blog</title>
    <link>https://yuzar-blog.netlify.app/</link>
    <atom:link href="https://yuzar-blog.netlify.app/index.xml" rel="self" type="application/rss+xml"/>
    <description>Data Science with R
</description>
    <generator>Distill</generator>
    <lastBuildDate>Sat, 30 Jan 2021 00:00:00 +0000</lastBuildDate>
    <item>
      <title>R package reviews {dlookr} diagnose, explore and transform your data</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data</link>
      <description>Raw data need to be diagnosed for existing problems, explored for new hypotheses and repaired in order to increase data quality and output. The {dlookr} package makes these steps fast and easy. {dlookr} generates automated reports and performs compex operations, like imputing missing values or outliers, with simple functions. Moreover, {dlookr} collaborates perfectly with {tidyverse} packages, like {dplyr} and {ggplot2} to name just a few!</description>
      <category>EDA</category>
      <category>videos</category>
      <category>data wrangling</category>
      <category>R package reviews</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data</guid>
      <pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data/dlookr_thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Deep Exploratory Data Analysis (EDA) in R</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress</link>
      <description>Exploratory Data Analysis is an important first step on the long way to the final result, be it a statistical inference in a scientific paper or a machine learning algorithm in production. This long way is often bumpy, highly iterative and time consuming. However, EDA might be the most important part of data analysis, because it helps to generate hypothesis, which then determine THE final RESULT. Thus, in this post I'll provide the simplest and most effective ways to explore data in R, which will significantly speed up your work. Moreover, we'll go one step beyond EDA by starting to test our hypotheses with simple statistical tests.</description>
      <category>EDA</category>
      <category>videos</category>
      <category>data wrangling</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress/DEDA_thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>How to impute missing values with Machine Learning in R</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r</link>
      <description>Imputation simply means - replacing a missing value with a value that makes sense. But how can we get such values? Well, we'll use Machine Learning algorithms, because they have a high prediction power. So, in this post we'll learn how to impute missing values easily and effectively.</description>
      <category>videos</category>
      <category>data wrangling</category>
      <category>visualization</category>
      <category>machine learning</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r/thumbnail_impute_na.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Null Hypothesis, Alternative Hypothesis and Hypothesis Testing</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good</link>
      <description>Hypothesis testing is one of the most important concepts in (frequentiest) statistics and science. However, most people who test hypotheses are scientists, but not statisticians. That's why scientists often do not test hypotheses properly, without any bad intensionс. So, in this blog-post we'll break down hypothesis testing in small parts and try to properly understand every of them.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>What is p-value and why we need it</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/Dldutns6Tig" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;Since p-values are mostly used for testing hypothesis, you should definitely check out the previous post - &lt;a href="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/"&gt;hypothesis testing&lt;/a&gt;, where I superficially introduced the p-value already.&lt;/p&gt;
&lt;h2 id="p-value-definition-n1-its-a-probability"&gt;P-value definition N°1: it’s a probability&lt;/h2&gt;
&lt;p&gt;The most intuitive explanation of p-values I have found came from Andrew Vickers’ book “What is a p-value anyway? 34 Stories to Help You Actually Understand Statistics”: &lt;strong&gt;“P-value is the probability of the toothbrush being dry if you’ve just cleaned your teeth.”&lt;/strong&gt; Let’s start with that.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;P&lt;/strong&gt; stands for &lt;strong&gt;probability&lt;/strong&gt;. And it refers to &lt;strong&gt;the probability to observe the results we did just by chance&lt;/strong&gt;. In order to understand this definition a little better, let’s have an example:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/coin.png" /&gt;&lt;/p&gt;
&lt;p&gt;If we throw a fair coin, we have only &lt;strong&gt;2 possible outcomes&lt;/strong&gt;: heads or tales. The chances to get &lt;strong&gt;tails are obviously 50%&lt;/strong&gt;. Then we throw our coin a second time. The chances to get &lt;strong&gt;2 tails in a row are 25%&lt;/strong&gt;, because we have 4 possible outcomes after two throws, with only one of those four outcomes having &lt;strong&gt;two tails in a row&lt;/strong&gt;, so 1/4 = 0.25 = 25%. It’s actually pretty likely to get 2 tails in a row. So, nothing unusual here. The coin must be fair. But 3 tails in a row starts to feel strange. The chances to get &lt;strong&gt;3 tails in a row are low, only 12.5%&lt;/strong&gt;, but still possible. However, if we get 3 tails in a row we start to doubt whether the coin is actually fair. And if we get &lt;strong&gt;6 tails in a row despite only 1.5% probability to get them&lt;/strong&gt;, the null hypothesis - &lt;em&gt;that this is a fair coin&lt;/em&gt; - would seem ridiculous and we’ll reject it, because it’s simply &lt;strong&gt;too unlikely to happen randomly (or by chance)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/6_tails.png" /&gt;&lt;/p&gt;
&lt;p&gt;This example, which defines the p-value as &lt;strong&gt;ONE particular&lt;/strong&gt; probability of having &lt;strong&gt;only tales&lt;/strong&gt;, is clear, intuitive, but &lt;strong&gt;only ONE sided&lt;/strong&gt;. It isn’t wrong, but &lt;strong&gt;ONE-sided p-values&lt;/strong&gt; coming from &lt;strong&gt;only ONE probability&lt;/strong&gt; is not what people use. The &lt;strong&gt;“real p-values”&lt;/strong&gt; are &lt;strong&gt;TWO sided&lt;/strong&gt;. Then why did we learned about one-sided p-values at all, you might ask??? Well, it was totally necessary for a better understanding of the second definition of p-values. And this is actually one of those definitions from the book, where people start to run away from statistics :) namely:&lt;/p&gt;
&lt;div id="hello" class="greeting message" style="color: blue;"&gt;
&lt;p&gt;P.S.: &lt;strong&gt;One-sided p-values are rare and dangerous&lt;/strong&gt;, so, please don’t use them to avoid confusion, especially if you just started to learn about p-values.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="p-value-definition-n2-three-probabilities-learned-from-josh-starmer"&gt;P-value definition N°2: three probabilities (learned from &lt;a href="https://www.youtube.com/watch?v=5Z9OIYA8He8&amp;amp;ab_channel=StatQuestwithJoshStarmer"&gt;Josh Starmer!&lt;/a&gt;)&lt;/h2&gt;
&lt;p&gt;P-value is the probability to get &lt;strong&gt;(1) the data we have collected, (2) as extreme or (3) more extreme data&lt;/strong&gt; just by chance, assuming our null hypothesis is true.&lt;/p&gt;
&lt;p&gt;Two tailed p-values are determined by adding up &lt;strong&gt;three probabilities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/3_probs.png" /&gt;&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;Remember our coin example? The probability of having &lt;em&gt;two tails&lt;/em&gt; in a row is 25% (or 0.25), because there are 4 equally possible outcomes after flipping a coin 2 times. These 4 possible outcomes is our &lt;em&gt;distribution.&lt;/em&gt; And &lt;strong&gt;the data we have got&lt;/strong&gt; - &lt;em&gt;two tails&lt;/em&gt; - is only one side of this distribution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Likewise, the probability of getting &lt;em&gt;two heads&lt;/em&gt; is also 0.25, and since &lt;em&gt;two heads&lt;/em&gt; are &lt;strong&gt;as extreme&lt;/strong&gt; as &lt;em&gt;two tales&lt;/em&gt;, we have to add the probability of &lt;em&gt;two tails&lt;/em&gt; to the probability of &lt;em&gt;two heads&lt;/em&gt;: 1/4 + 1/4 = 2/4 = 0.5. Now we have a &lt;strong&gt;two sided probability&lt;/strong&gt; but the p-values still need one last part.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start="3" style="list-style-type: decimal"&gt;
&lt;li&gt;The third and the last part of the p-value is - the probability of observing &lt;strong&gt;something rarer or more extreme&lt;/strong&gt;. In this case it is 0, because &lt;strong&gt;no other outcomes are rarer then two tails or two heads&lt;/strong&gt;. Thus, if we add this 0 to the probability of having either two tales or two heads, we’ll get a final p-value of 0.5. And since our p-value is way above the significance threshold of 0.05, we failed to reject the null hypothesis, so that our null hypothesis is still TRUE - our coin must be fare.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus: the p-value is the probability of getting the &lt;strong&gt;(1) data we collected (two tails = 0.25), as extreme (two heads = 0.25) or more extreme data (0) is&lt;/strong&gt;: 0.25 + 0.25 + 0 = 0.5.&lt;/p&gt;
&lt;p&gt;Now let’s calculate the p-value for a slightly more complicated outcome: &lt;strong&gt;3 heads and 1 tail&lt;/strong&gt;. This is one of the 16 possible outcomes, when we flip the coin 4 times. The null hypothesis would be that our data - 3 heads and 1 tail - is nothing special, but belongs to this exact distribution.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/3h1t.png" /&gt;&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;The probability we randomly get &lt;strong&gt;3 heads and 1 tail&lt;/strong&gt; is 4/16 = 0.25. That would be &lt;strong&gt;our data&lt;/strong&gt; and with that &lt;strong&gt;the first part of the p-value&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The probability that we randomly get something equally rare, namely &lt;strong&gt;3 tails and 1 head&lt;/strong&gt;, is also 4/16 = 0.25, which will be &lt;strong&gt;the second part of the p-values&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The probability that we randomly get something rarer or more extreme, which in our case are &lt;strong&gt;4 heads and 4 tails&lt;/strong&gt; is 2/16 = 0.125, and are &lt;strong&gt;the last part of our p-value&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Adding them all together would result into a p-value of 0.625, so we would fail to reject the null hypothesis - that our data (3 heads and 1 tail) is nothing special, and would need to conclude that 3 heads and 1 tail belong to this distribution.&lt;/p&gt;
&lt;h2 id="p-value-definition-n3-cumulative-probability-area-under-the-curve"&gt;P-value definition N°3: cumulative probability (area under the curve)&lt;/h2&gt;
&lt;p&gt;Calculating a p-value from discrete numbers is kind of easy, but how do we do it for continuous numbers? Plotting our discrete data will help to understand that. If we plot our values from the last example, we would find the &lt;strong&gt;most probable&lt;/strong&gt; outcomes (&lt;span style="color: green;"&gt;green&lt;/span&gt;) in the middle, a bit &lt;strong&gt;less probable&lt;/strong&gt; (black and &lt;span style="color: blue;"&gt;blue&lt;/span&gt;) on both sides of the middle, and finally &lt;strong&gt;the least probable&lt;/strong&gt; values (&lt;span style="color: red;"&gt;red&lt;/span&gt;) very far away from the middle. This will become &lt;strong&gt;the probability distribution of our discrete values&lt;/strong&gt;. We can also cover plotted values with a &lt;strong&gt;curve&lt;/strong&gt; to better visualize this distribution. Now, if we mark our discrete values with discrete numbers on the x-axis, we’ll see that we’ll be able to add any &lt;strong&gt;continuous value&lt;/strong&gt; in between those discrete numbers, e.g. 0.3 or 0.333. And for every of these &lt;strong&gt;continuous value&lt;/strong&gt; we can calculate &lt;strong&gt;probability&lt;/strong&gt; in the exact same way as we just did for &lt;em&gt;2 tails&lt;/em&gt;, or &lt;em&gt;3 heads and 1 tail&lt;/em&gt;. That’s how we arrive at the &lt;strong&gt;continuous probability&lt;/strong&gt;. The p-values for such continuous probability are also calculated in the same way. But instead of adding up only 3 discrete probabilities (our data, as extreme and more extreme data), we add up 2 discrete parts (our data and similarly extreme data) and 1 continuous part (more extreme data from both tails of the distribution):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;and since we calculate probabilities of many points &lt;strong&gt;under the curve&lt;/strong&gt; and&lt;/li&gt;
&lt;li&gt;add all of them together, or &lt;strong&gt;ACCUMULATE those probabilities under the curve&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;we’ll get the &lt;strong&gt;AREA under the curve&lt;/strong&gt; - which then &lt;strong&gt;IS&lt;/strong&gt; our p-value as a &lt;strong&gt;cumulative probability&lt;/strong&gt;, including all the values which are as extreme or more extreme than our data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/distribution.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;An example of such curve could be the weight of people. For instance, the average weight of German males is 80 kilos with 95% of them weighting between 70 and 90 kilos (these numbers are totally made up ;). 2.5% of them will weight &amp;lt;70 and 2.5% will weight &amp;gt;90 kilos. If we get a new data point - namely a weight of a new person, we can calculate the p-value for that point. Our null hypothesis is - that the persons weight is &lt;strong&gt;NOT different&lt;/strong&gt; from our distribution. If this person weights 75 kilos, the area under the curve (or a p-value!), which includes all the values &lt;strong&gt;as extreme or more extreme&lt;/strong&gt; than 75, will be ca. 0.6 (or 60%), which is fairly big. So, we would fail to reject the null hypothesis, concluding that the person could be a German male, or is at least not different from German males. However, if the weight is 65 kilos, the area under the curve is quite small, let’s say 2%. Here, we can reject the null hypothesis and conclude that this person must belong to a different population, e.g. women, or a man from a different country.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So you see the p-value is not a simple probability, but 3 probabilities (parts) added together!&lt;/strong&gt; A simple probability is the number of outcomes of interest, divided by the total number of outcomes (it’s not even the one-sided p-value). While &lt;strong&gt;a p-value is the probability that random chance generated the&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;data we observed, or the data that is&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;equally rare or 3. rarer for &lt;strong&gt;discrete&lt;/strong&gt; values, or&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;equally extreme or 3. more extreme for &lt;strong&gt;continuous&lt;/strong&gt; values,&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;if the null hypothesis is true.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="p-value-definition-n4-original-fishers-definition"&gt;P-value definition N°4: original Fisher’s definition&lt;/h2&gt;
&lt;p&gt;All right, &lt;strong&gt;p-value is a cumulative probability&lt;/strong&gt;. And since a probability is a &lt;strong&gt;continues&lt;/strong&gt; measure from 0 to 1 (or from 0% to 100%), the &lt;strong&gt;p-value also IS any number between 0 &amp;amp; 1&lt;/strong&gt;. If we look for a difference between two groups, we can see a p-value &lt;strong&gt;as a measure of similarity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/similarity.png" /&gt;&lt;/p&gt;
&lt;p&gt;For examples if two samples are identical, there is &lt;strong&gt;100% similarity and 0% difference&lt;/strong&gt;. So, our p-value is equal to 1. If &lt;strong&gt;similarity is only 60%, then the difference is 40%&lt;/strong&gt;, which is much bigger then 0, but is still small. But &lt;strong&gt;if the similarity drops to 5% or below, then the difference of 95% is often considered significant&lt;/strong&gt;, and we can confidently reject the null hypothesis - that there is NO difference between samples, in favour of the alternative hypothesis - that such difference exists.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But wait!!!&lt;/strong&gt;, does it make any sense to &lt;strong&gt;dichotomize a continuous number&lt;/strong&gt; (from 0 to 1) into only two categories - &lt;strong&gt;significant and not-significant? Of coarse, not!&lt;/strong&gt; The p-value is not black and white, but everything in between … like 50 shades of gray …&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/Fifty_Shades_of_Grey_poster.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;… ups, sorry, not that 50 shades of gray … that one :) &lt;span class="citation"&gt;(&lt;a href="#ref-Sterne2001" role="doc-biblioref"&gt;Sterne, Smith, and Cox 2001&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/interpretation.png" /&gt;&lt;/p&gt;
&lt;p&gt;Hmm, but if dichotomizing continuous p-values into two categories - &lt;strong&gt;significant and not-significant&lt;/strong&gt; - doesn’t make much sense, &lt;strong&gt;why is the whole world doing exactly that&lt;/strong&gt;? “The basic explanation is neither philosophical nor scientific, but sociologic - &lt;strong&gt;everyone uses them&lt;/strong&gt;.” &lt;span class="citation"&gt;(&lt;a href="#ref-Goodman2019" role="doc-biblioref"&gt;Goodman 2019&lt;/a&gt;)&lt;/span&gt;. But in order to answer this question more thoroughly, we’d need to briefly explore the history of p-values …&lt;/p&gt;
&lt;p&gt;Ronald Fisher, the father of modern statistics, saw the &lt;strong&gt;P value as an index measuring the strength of evidence against the null hypothesis&lt;/strong&gt; (see the picture above). Fisher himself proposed: &lt;strong&gt;“if P-value is between 0.1 and 0.9 there is certainly no reason to suspect the hypothesis tested. If it’s below 0.02 it is strongly indicated that the hypothesis fails to account for the whole of the facts.”&lt;/strong&gt; He sometimes used the threshold for the p-value of 0.05 (&amp;lt; 5%) himself to reject the null hypothesis, but &lt;strong&gt;without any clear reason&lt;/strong&gt;. Moreover, he recommended to treat a &lt;strong&gt;p-value around 0.05 as inconclusive, where we’d need to repeat the experiment.&lt;/strong&gt; The reason Fisher used p-values at all was - to reduce the probability of the &lt;strong&gt;type I error&lt;/strong&gt; - the probability to find something what is not there - &lt;strong&gt;false positive&lt;/strong&gt;, e.g. that somebody has cancer, while being totally healthy. The most typical type I error happens when we measure a difference between two groups and find this difference to be significant, while both samples came from the same population:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/type_1_errors.png" /&gt;&lt;/p&gt;
&lt;p&gt;Having a stiff threshold of 0.05 can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Good&lt;/strong&gt;: because from all H0 you test, in the long run you will falsely reject at most 5% of the correct ones, but it’s also&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;BAD&lt;/strong&gt;: because it statistically guarantees that 1 in 20 healthy people, will “have” cancer just by chance, or in 1 out of 20 statistical tests we’ll find nonsense just by chance.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, &lt;strong&gt;lowering the p-value to, let’s say 0.01, reduces the probability of finding nonsense and ensures we find something what is really there&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Along the &lt;strong&gt;type I error&lt;/strong&gt;, Neyman and Pearson also advocated the &lt;strong&gt;type II error&lt;/strong&gt;, that could be made in interpreting the results of an experiment. The type II error is the probability to miss something what is there - &lt;strong&gt;false negative&lt;/strong&gt;, e.g. failing to detect cancer, when cancer is already in the body.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/type_2_errors.png" /&gt;&lt;/p&gt;
&lt;p&gt;Neyman and Pearson’s idea of &lt;strong&gt;hypothesis testing&lt;/strong&gt; was actually really good, because it supposed to reduce the number of mistakes by &lt;strong&gt;keeping the rates of both type I and type II errors low&lt;/strong&gt;. However, only the easy part of their approach — that the null hypothesis can be rejected if P &amp;lt; 0.05 (type I error rate of 5%) — has been widely adopted and stuck in the medical research, causing current dichotomy of results into &lt;strong&gt;significant&lt;/strong&gt; or &lt;strong&gt;not significant&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Two serious consequences of this are that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;potentially clinically important differences observed in small studies are denoted as non-significant and ignored, while&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;all significant findings are assumed to result from real treatment effects.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Well, how do we deal with type II errors? &lt;strong&gt;Increasing the sample size will increase the power of the experiment and therefore decrease the probability of type II error&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So, now we know that hypothesis testing can produce two types of mistakes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The type I error IS an ERROR, because p-values help to maintain nonsense&lt;/strong&gt;. While&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The type II error IS an ERROR, because p-values help to miss something important&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, &lt;strong&gt;we should stop dichotomising our results into significant and not significant&lt;/strong&gt;! But, if after reading this you still want to continue dichotomising your results into &lt;strong&gt;significant&lt;/strong&gt; or &lt;strong&gt;not significant&lt;/strong&gt;, please, answer a following question: &lt;strong&gt;is the difference between a P-value of 0.051 and 0.049 statistically significant?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The answer is - a clear NO! However, if using a p-value continuously totally blows your mind, and you still need some pragmatic decision making tool, consider using a hybrid approach, which singles out 5 instead of only 2 categories of &lt;strong&gt;the strength of the evidence against the null hypothesis in favor of the alternative:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P &amp;gt; 0.10 The data shows &lt;strong&gt;NO evidence&lt;/strong&gt; against the null hypothesis&lt;/li&gt;
&lt;li&gt;0.05 &amp;lt; P &amp;lt; 0.10 &lt;strong&gt;Weak evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as a dot: ‘.’&lt;/li&gt;
&lt;li&gt;0.01 &amp;lt; P &amp;lt; 0.05 &lt;strong&gt;Moderate evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as one star: ‘*’&lt;/li&gt;
&lt;li&gt;0.001 &amp;lt; P &amp;lt; 0.01 &lt;strong&gt;Strong evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as two stars: ‘**’&lt;/li&gt;
&lt;li&gt;P &amp;lt; 0.001 &lt;strong&gt;Very strong evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as three stars: ‘***’&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="dont-follow-any-cut-off-the-cut-off-should-follow-you"&gt;Don’t follow any cut-off, the cut-off should follow you!&lt;/h2&gt;
&lt;p&gt;Any cut-off splits the probability into two parts:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level of confidence&lt;/strong&gt; - beta (β), e.g. 95%, tells how confident are we in our decision and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level of significance&lt;/strong&gt; - alpha (⍺), e.g. 5%, tells us when to &lt;strong&gt;reject or fail to reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;&lt;/strong&gt;. Namely, (1) if p-value &amp;lt;= ⍺, we reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;; (2) if p-value &amp;gt; ⍺, we fail to reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both &lt;strong&gt;⍺&lt;/strong&gt; &amp;amp; &lt;strong&gt;β&lt;/strong&gt; add up to 1 and tell you the same thing - how sure are you about your decision:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for 95% confidence: ⍺ = 1 - β = 1 - 0.95 = 0.05&lt;/li&gt;
&lt;li&gt;for 99% confidence: ⍺ = 1 - β = 1 - 0.99 = 0.01&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;cut-off itself is up to you&lt;/strong&gt; and is &lt;strong&gt;highly dependent on the experiment&lt;/strong&gt;. It might sound fuzzy, but it actually gives you a freedom to adjust the decision-making to reality. On the other hand, clinging to the cut-off of 0.05 may actually increase the rates of both type I and type II errors. Let’s have a look at two examples:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;Imagine you study a new treatment for a deadly disease and your p-value for the difference between control and treatment groups is 0.15. If you strictly follow the 0.05 cut-off, you’ll fail to reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; and would conclude that the treatment does not work. The p-value of 0.15 makes you “&lt;em&gt;only&lt;/em&gt;” 85% sure that such difference exists, making this difference “&lt;em&gt;not significant enough&lt;/em&gt;.” However, if you don’t &lt;span style="color: red;"&gt;blindly&lt;/span&gt; follow the 0.05, you could &lt;span style="color: blue;"&gt;look&lt;/span&gt; at your experiment in a completely new way by saying: &lt;strong&gt;I am 85% confident that new treatment works and we found a new way of treating a deadly disease&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Similarly, if I was 85% confident of winning the lottery I would definitely play! However, if I’d strictly follow the cut-off of 0.05, I would “&lt;em&gt;be not confident enough&lt;/em&gt;” that I will win (which is ridiculous) and I won’t play.&lt;/p&gt;
&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;On the opposite side, if an airline company welcomes you aboard with a catchy phrase: “We are 95% confident that you’ll survive, so your probability of crashing with us is below 5% (p &amp;lt; 0.05),” would you go aboard? I personally would run away! The airline company would chase me though and scrim: "Stop running away! We &lt;strong&gt;successfully rejected the Null Hypothesis that you will die!!!&lt;/strong&gt; But I would only be willing to fly .. if my level of confidence in survival increases to 99.9999% or if my chances to die would become 1 to a million (⍺ = 0,000001) flights.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, you see?, the freedom to set up your own level of significance (the cut-off) depending on the situation makes you a better scientist, while blindly following the cut-off of 0.05 would either keep you pure or just kill you.&lt;/p&gt;
&lt;div id="hello" class="greeting message" style="color: blue;"&gt;
&lt;p&gt;P.S.: the dramatic effect of the last sentence serves solely educational purposes … while hoping to significantly (p &amp;lt; 0.05 😉) increase the &lt;strong&gt;probability&lt;/strong&gt; that you’ll remember the material :)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Does it mean that any threshold is evil? Well, no! &lt;strong&gt;The threshold of 0.05 means that if there is no difference between Group 1 and Group 2, and if we did this exact experiment 100 times, then only 5 of these experiments would result in a wrong decision.&lt;/strong&gt; These would be 5 &lt;strong&gt;false positives&lt;/strong&gt; (type I error). Lowering the threshold to let’s say 0.001 would reduce the number of false positives to 1 out of 1000 experiments. &lt;strong&gt;But the costs of doing 1000 experiments are often much higher then it is worth.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="a-single-article-you-need-to-read-moving-to-a-world-beyond-p0.05"&gt;A single article you need to read: “Moving to a World Beyond “p&amp;lt;0.05” "&lt;/h2&gt;
&lt;p&gt;If you want to know what statisticians themselves think about p-value, read the following article: &lt;a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913"&gt;Moving to a World Beyond “p&amp;lt;0.05”&lt;/a&gt; &lt;span class="citation"&gt;(&lt;a href="#ref-Wasserstein2019" role="doc-biblioref"&gt;Wasserstein, Schirm, and Lazar 2019&lt;/a&gt;)&lt;/span&gt;. Here I would just sum up some recommendations the authors provided. These recommendations would enable you to find &lt;strong&gt;fewer false alarms (false positives, type I error), and to overlook fewer discoveries (false negatives, type II error)&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id="what-to-do"&gt;What to do:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;report precise continuous p-values&lt;/strong&gt; without reference to arbitrary thresholds. For example, P = 0.023 rather than P &amp;lt; 0.05.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;lower the 0.05 “&lt;em&gt;statistical significance&lt;/em&gt;” threshold for claims of novel discoveries to a 0.005 threshold and refer to p-values between 0.05 and 0.005 as “&lt;em&gt;suggestive&lt;/em&gt;.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;remember, &lt;strong&gt;the p-value as only one aspect of a complete data analysis&lt;/strong&gt;. Thus, supplement the p-value with visualizations of confidence intervals on effect sizes or likelihood ratios, justify the adequacy of the sample size and &lt;strong&gt;the reasons various statistical methods were employed&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;clarify objectives, invest into careful planning &amp;amp; design, invest into producing solid data, use more descriptive stats, use more of the regularized, robust, nonlinear and nonparametric methods for exploratory research, check the robustness of your methods and use several different data analysis techniques for testing the same hypothesis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;look for both (1) a small p-value and (2) a large effect size before declaring a result “significant,” instead of dichotomized p-values alone.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;interpret the p-value in the context of sample size. A large study can detect a small, clinically unimportant finding. The larger the sample the more likely a difference to be detected. Lower the p-value threshold (e.g. to 0.001) for large studies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;reduce unplanned and uncontrolled modeling/testing (p-hacking). &lt;strong&gt;Examining 20 associations will produce one result that is “significant at P = 0.05” by chance alone.&lt;/strong&gt; Thus, testing multiple variables, using long questionnaires with hundreds of variables and measuring a wide range of potential outcomes, would &lt;strong&gt;guarantee several false positive findings&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Accept uncertainty and embrace variation in effects” &lt;span class="citation"&gt;(&lt;a href="#ref-McShane2019" role="doc-biblioref"&gt;McShane et al. 2019&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="what-not-to-do"&gt;What not to do:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Don’t make decisions based solely on some arbitrary threshold such as p &amp;lt; 0.05. Dichotomizing p-values into “significant” and “non significant” one loses information. A P-value is not black and white, but ≈ 50 shades of grey 😂&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t treat &lt;strong&gt;“p = 0.051” and “p = 0.049” as being categorically different. 🙂 As &lt;span class="citation"&gt;&lt;a href="#ref-Gelman2006" role="doc-biblioref"&gt;Gelman and Stern&lt;/a&gt; (&lt;a href="#ref-Gelman2006" role="doc-biblioref"&gt;2006&lt;/a&gt;)&lt;/span&gt; famously observed, the difference between “significant” and “not significant” is not itself statistically significant.&lt;/strong&gt; Don’t discard a well-designed, excellently conducted, thoughtfully analyzed, and scientifically important experiment only because it failed to cross the threshold of 0.05 (e.g. p-value = 0.09).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t believe that an association or effect exists just because it was “statistically significant.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t believe that an association or effect is absent just because it was not “statistically significant.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t conclude anything about practical importance based on “statistical significance” (or lack thereof).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In fact - don’t say “statistically significant” at all … whether expressed in words or by asterisks in a table.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t believe that your p-value gives the probability that your test hypothesis is TRUE. &lt;strong&gt;P-value is the probability of the data, not of (any) hypothesis!&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="what-about-bayesian-statistical-inference"&gt;What about Bayesian statistical inference?&lt;/h2&gt;
&lt;p&gt;The switch to the use of Bayesian statistical inference in medical research was proposed for several decades, but is barely possible. A major reason is that &lt;strong&gt;prior knowledge can be difficult to quantify&lt;/strong&gt;. The major reason for that is that science suppose to produce NEW knowledge. Thus, repeated experiments are necessary, but few scientist want to do them. Moreover, if the priors are flat (a wide range of values is considered to be equally likely), then the &lt;em&gt;Frequentist&lt;/em&gt; and &lt;em&gt;Bayesian&lt;/em&gt; results are similar, while computing power required for Bayesian analysis is “significantly” ;) higher. Two approaches will give different results, however, if our prior opinion is not vague. Besides, we can’t ignore the fact that &lt;em&gt;Bayesian statistics&lt;/em&gt; is even less intuitive to the majority of scientists then &lt;em&gt;Frequentists statistics&lt;/em&gt;. Otherwise everybody would have already used it. Thus, until an alternative, simple and widely accepted approach to the p-values exists, banning p-values is of no help to anyone.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Most of the scientist can’t tell you what the p-value is. Most of the statisticians can, but can’t explain it to intuitively. So, don’t give yourself a hard time if you still a bit confused. It’s normal. Instead, I’d suggest we stop caring about understanding the p-value, but start caring to use it properly. And remember, &lt;strong&gt;you don’t need to understand how the car works, you only need to know how to drive&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/impossible-parking.gif" /&gt;&lt;/p&gt;
&lt;h2 id="whats-next-to-learn"&gt;What’s next to learn?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;misinterpretations of p-values (only if you are interested)&lt;/li&gt;
&lt;li&gt;effect size and&lt;/li&gt;
&lt;li&gt;power analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="further-readings-and-watchings"&gt;Further readings and watchings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;that’s an amazing video: &lt;a href="https://www.youtube.com/watch?v=tLM7xS6t4FE&amp;amp;ab_channel=SciShow" class="uri"&gt;https://www.youtube.com/watch?v=tLM7xS6t4FE&amp;amp;ab_channel=SciShow&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="http://www.stat.ualberta.ca/~hooper/teaching/misc/Pvalue.pdf" class="uri"&gt;http://www.stat.ualberta.ca/~hooper/teaching/misc/Pvalue.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-Gelman2006" class="csl-entry"&gt;
Gelman, Andrew, and Hal Stern. 2006. &lt;span&gt;“&lt;span class="nocase"&gt;The difference between "significant" and "not significant" is not itself statistically significant&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;American Statistician&lt;/em&gt; 60 (4). &lt;a href="https://doi.org/10.1198/000313006X152649"&gt;https://doi.org/10.1198/000313006X152649&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-Goodman2019" class="csl-entry"&gt;
Goodman, Steven N. 2019. &lt;span&gt;“&lt;span class="nocase"&gt;Why is Getting Rid of P-Values So Hard? Musings on Science and Statistics&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;American Statistician&lt;/em&gt; 73 (sup1). &lt;a href="https://doi.org/10.1080/00031305.2018.1558111"&gt;https://doi.org/10.1080/00031305.2018.1558111&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-McShane2019" class="csl-entry"&gt;
McShane, Blakeley B., David Gal, Andrew Gelman, Christian Robert, and Jennifer L. Tackett. 2019. &lt;span&gt;“&lt;span&gt;Abandon Statistical Significance&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;American Statistician&lt;/em&gt; 73 (sup1). &lt;a href="https://doi.org/10.1080/00031305.2018.1527253"&gt;https://doi.org/10.1080/00031305.2018.1527253&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-Sterne2001" class="csl-entry"&gt;
Sterne, Jonathan A. C., George Davey Smith, and D. R. Cox. 2001. &lt;span&gt;“&lt;span class="nocase"&gt;Sifting the evidence—what’s wrong with significance tests?&lt;/span&gt;”&lt;/span&gt; &lt;em&gt;BMJ&lt;/em&gt; 322 (7280). &lt;a href="https://doi.org/10.1136/bmj.322.7280.226"&gt;https://doi.org/10.1136/bmj.322.7280.226&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-Wasserstein2019" class="csl-entry"&gt;
Wasserstein, Ronald L, Allen L Schirm, and Nicole A Lazar. 2019. &lt;span&gt;“&lt;span class="nocase"&gt;Moving to a World Beyond "p &lt;span&gt;&amp;lt;&lt;/span&gt; 0.05"&lt;/span&gt;.”&lt;/span&gt; &lt;a href="https://doi.org/10.1080/00031305.2019.1583913"&gt;https://doi.org/10.1080/00031305.2019.1583913&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5>fbe358502d16d4b3c983e5217d5bc6d2</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>R demo | Chi-Square Test | how to conduct, visualize &amp; interpret | + pairwise post-hoc tests</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-12-14-how-to-conduct-chi-square-test-in-r</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 5 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/8Tj0-yMPO64" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;Understanding &lt;a href="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/"&gt;hypothesis testing&lt;/a&gt; and &lt;a href="https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation/"&gt;p-values&lt;/a&gt; would be very helpful.&lt;/p&gt;
&lt;h2 id="how-to-conduct-chi-squared-test-in-r"&gt;How to conduct Chi-Squared test in R&lt;/h2&gt;
&lt;p&gt;If you have installed and loaded {ggstatsplot} package, you can use {ggbarstats} function to conduct and visualize Chi-Square test of independence between two categorical variables, where variables can have two or more categories. Within this function we need to specify only four arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;our data&lt;/strong&gt;, e.g. let’s take &lt;code&gt;mtcars&lt;/code&gt;, which you already have in R, so you don’t need to look for it&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt; - as one of your categorical variables, for example &lt;em&gt;Transmission&lt;/em&gt; of the car (with 0 being automatic, and 1 being manual transmission)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;y&lt;/strong&gt; - would be your second variable, let’s take the &lt;em&gt;Number of cylinders&lt;/em&gt; (4, 6 or 8) and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the label&lt;/strong&gt; argument - which displays &lt;strong&gt;both&lt;/strong&gt; numbers and percentages of observations in each category.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This simple command results in a statistically rich and publication ready plot! Now, let’s interpret the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;ggstatsplot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ggstatsplot)

ggbarstats(
  data  = mtcars, 
  x     = am, 
  y     = cyl, 
  label = &amp;quot;both&amp;quot;, 
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file17d3c4ec56d98_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2 id="interpretation"&gt;Interpretation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Chi-Square statistics&lt;/strong&gt; was previously used to manually calculate p-value, but nowadays, since p-values are always calculated by computers, we can safely ignore it&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;P-value&lt;/strong&gt; in our test can be seen as the probability of independence between two variables, low p-value (usually p &amp;lt; 0.05), like in our example, indicates that number of cylinders and transmission of cars &lt;em&gt;are dependent on each other&lt;/em&gt;. In other words - &lt;em&gt;there is a relationship between them&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Indeed, the plot shows that the number of cars using automatic transmission (am = 0) increases with increasing number of cylinders. The opposite is true for cars with manual transmission (am = 1), their frequency declines as number of cylinders increases.&lt;/p&gt;
&lt;p&gt;So, we can conclude, that &lt;em&gt;the relationship between transmission and number of cylinders exists&lt;/em&gt;. However, p-value doesn’t say &lt;em&gt;how strong this relationship is&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;That’s why we have &lt;strong&gt;V Cramer&lt;/strong&gt; value with its 95% confidence intervals as &lt;strong&gt;the effect size&lt;/strong&gt; next to p-value. Our effect size of 0.46 indicates a &lt;strong&gt;relatively strong relationship&lt;/strong&gt;, which supports the conclusion made by the p-value. The confidence intervals do not make much sense though, since &lt;em&gt;V Cramer&lt;/em&gt; goes from 0 to 1 anyway.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-14-how-to-conduct-chi-square-test-in-r/v_cramer.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;However, &lt;code&gt;ggbarstats&lt;/code&gt; also provides a second &lt;strong&gt;Bayesian V Cramer effect size&lt;/strong&gt;, which delivers much more useful 95% Highest Density Intervals. The interpretation of the Bayesian effect size is the same, so the relationship between our variables is &lt;strong&gt;relatively strong&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If that’s not enough, we can look at the &lt;strong&gt;Bayes Factor&lt;/strong&gt; (Jeffreys, 1961), which tests both null and alternative hypotheses at the same time. Bayes Factor of - 2.82 in our example indicates a &lt;strong&gt;strong evidence for the alternative hypothesis&lt;/strong&gt; - that the relationship exists, which IS in line with the frequentists statistics on the top of the plot.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-14-how-to-conduct-chi-square-test-in-r/bf_interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can also see &lt;strong&gt;Proportion Tests&lt;/strong&gt; for transmissions in each cylinder. They show whether proportions inside every cylinder differ. Our Null Hypothesis (&lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;) here is that there are equal proportions of different transmissions in a particular category of a cylinder. Which is the case for cylinders 4 and 6. While, our Alternative Hypothesis (&lt;span class="math inline"&gt;\(H_alt\)&lt;/span&gt;) is that the proportions differ, which is the case for the cylinder 8.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="pairwise-proportion-tests-or-post-hoc-tests"&gt;Pairwise Proportion Tests … or post-hoc tests&lt;/h2&gt;
&lt;p&gt;If you find a significant relationship between variables and you have more then two categories in any of your variables, like in our example, you might be interested to compare proportions of cylinders with each other, namely 4 with 6, 4 with 8 and 6 with 8. Such simple pairwise comparisons is often called with an unnecessary fancy name - &lt;strong&gt;post-hoc tests&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The easiest was to make pairwise proportions tests is to use {pairwise_prop_test} function from {rstatix} package. Thus, first, install and load {rstatix} package, then use {table} function for a contingency table of your variables. And finally, simply apply {pairwise_prop_test} function to your contingency table. The results show two kinds of p-values, normal and adjusted for multiple comparisons. Always use the adjusted ones. So, we see that there is a significant association between cylinders 4 and 8, where cylinder 4 has more cars with manual transmission, while cylinder 8 has more cars with automatic transmission.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;rstatix&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(rstatix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;contingency_table &amp;lt;- table(mtcars$cyl, mtcars$am)
contingency_table
pairwise_prop_test(contingency_table) &lt;/code&gt;&lt;/pre&gt;
&lt;template id="d8ed1aa3-e4c0-444a-8c13-3a8f03e1bfba"&gt;&lt;style&gt;
.tabwid table{
  border-spacing:0px !important;
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
&lt;/style&gt;&lt;div class="tabwid"&gt;&lt;style&gt;.cl-3a84551e{}.cl-3a7e29e6{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3a7e45de{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3a7e45df{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3a7e8a58{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a7e8a62{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a7e8a6c{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a7e8a6d{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a7e8a76{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a7e8a77{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a7e8a78{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a7e8a80{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a7e8a8a{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a7e8a8b{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a7e8a94{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a7e8a95{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}&lt;/style&gt;&lt;table class='cl-3a84551e'&gt;
&lt;thead&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-3a7e8a8b"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;group1&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a8b"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;group2&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a94"&gt;&lt;p class="cl-3a7e45df"&gt;&lt;span class="cl-3a7e29e6"&gt;p&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a94"&gt;&lt;p class="cl-3a7e45df"&gt;&lt;span class="cl-3a7e29e6"&gt;p.adj&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a95"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;p.adj.signif&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-3a7e8a58"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;4&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a58"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;6&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a62"&gt;&lt;p class="cl-3a7e45df"&gt;&lt;span class="cl-3a7e29e6"&gt;0.4400&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a62"&gt;&lt;p class="cl-3a7e45df"&gt;&lt;span class="cl-3a7e29e6"&gt;0.7300&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a6c"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;ns&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-3a7e8a6d"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;4&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a6d"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;8&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a76"&gt;&lt;p class="cl-3a7e45df"&gt;&lt;span class="cl-3a7e29e6"&gt;0.0108&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a76"&gt;&lt;p class="cl-3a7e45df"&gt;&lt;span class="cl-3a7e29e6"&gt;0.0324&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a77"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;*&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-3a7e8a78"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;6&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a78"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;8&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a80"&gt;&lt;p class="cl-3a7e45df"&gt;&lt;span class="cl-3a7e29e6"&gt;0.3650&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a80"&gt;&lt;p class="cl-3a7e45df"&gt;&lt;span class="cl-3a7e29e6"&gt;0.7300&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-3a7e8a8a"&gt;&lt;p class="cl-3a7e45de"&gt;&lt;span class="cl-3a7e29e6"&gt;ns&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;/template&gt;
&lt;div class="flextable-shadow-host" id="2512942a-cc57-4743-b0e1-43b482ec69b6"&gt;&lt;/div&gt;
&lt;script&gt;
var dest = document.getElementById("2512942a-cc57-4743-b0e1-43b482ec69b6");
var template = document.getElementById("d8ed1aa3-e4c0-444a-8c13-3a8f03e1bfba");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
&lt;/script&gt;

&lt;h2 id="whats-next"&gt;What’s next?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;first of all, never forget to cite this amazing package!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;citation(&amp;quot;ggstatsplot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  Patil, I. (2021). Visualizations with statistical details:
  The &amp;#39;ggstatsplot&amp;#39; approach. Journal of Open Source Software,
  6(61), 3167, doi:10.21105/joss.03167

A BibTeX entry for LaTeX users is

  @Article{,
    doi = {10.21105/joss.03167},
    url = {https://doi.org/10.21105/joss.03167},
    year = {2021},
    publisher = {{The Open Journal}},
    volume = {6},
    number = {61},
    pages = {3167},
    author = {Indrajeet Patil},
    title = {{Visualizations with statistical details: The {&amp;#39;ggstatsplot&amp;#39;} approach}},
    journal = {{Journal of Open Source Software}},
  }&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;learn more about &lt;a href="https://yury-zablotski.netlify.app/post/chi-square-2/"&gt;Chi-Squared Test&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;have a look at the &lt;a href="https://yury-zablotski.netlify.app/post/goodness-of-fit/"&gt;Chi-Squared Test Goodness of Fit&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="further-readings-and-watchings"&gt;Further readings and watchings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Jeffreys, H. 1961. Theory of Probability. 3rd ed. Oxford: Oxford University Press.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>9c36df85059d7ab5d0aba54cfa61bab2</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-12-14-how-to-conduct-chi-square-test-in-r</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-12-14-how-to-conduct-chi-square-test-in-r/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>One-sample Student’s t-test and One-sample Wilcoxon test: or how to compare your work to the work of others.</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 5 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/x5RcZlc-w4A" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;Understanding &lt;a href="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/"&gt;hypothesis testing&lt;/a&gt; and &lt;a href="https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation/"&gt;p-values&lt;/a&gt; would be very helpful.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/to-do-list.jpg" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 1. simulate your to-do results

# install.packages(&amp;quot;tidyverse&amp;quot;)
library(tidyverse)

set.seed(1)  # stabilizes random output, so you always get the same result
my_to_dos &amp;lt;- round(rnorm(n = 21, mean = 7, sd = 3)) 

my_to_dos&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1]  5  8  4 12  8  5  8  9  9  6 12  8  5  0 10  7  7 10  9  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you collect your to-do lists for the last 21 days, you find out that you finish 7 out of 10 tasks per day on average … ± 3. Now, since we have our data we can compare your average of 7 to the average of others, which is 6 our of 10, or 60%, according to &lt;a href="https://www.huffpost.com/entry/forty-one-percent-of-tasks-on-to-do-lists-are-never-done_b_9308978?guccounter=1&amp;amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;amp;guce_referrer_sig=AQAAANQ9AhoKTEu0BJFpJJbnXQLY_7SeRTIybJaZEHrK2N0bi1lPpFSByEH5kUdhBXNRo93VwzVF_ZLIkjBOCxhVzkK8I4CFTm0XPXsPQ1Rnl-Q77Q7RXpmcKQkrmr9QlKXKxEDFxoxsrkXwbe_wF2fNSsqotvBAE472NESpYzs7nY2v"&gt;this article&lt;/a&gt;. Is 7 significantly different from 6? &lt;strong&gt;One-sample t-test&lt;/strong&gt; or &lt;strong&gt;One-sample Wilcoxon test&lt;/strong&gt; can tell that, but how do we know which test we take?&lt;/p&gt;
&lt;h2 id="choose-the-test"&gt;Choose the test&lt;/h2&gt;
&lt;p&gt;Well, we’ll simply check whether our data is bell shaped, or &lt;strong&gt;normally distributed&lt;/strong&gt;. This can be done in two ways.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;first, with &lt;em&gt;Shapiro-Wilk normality test&lt;/em&gt;, where p-value &amp;gt; 0.05 would mean normally distributed data,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;or visually, with a Quantile-Quantile plot, using {ggqqplot} function from {ggpubr} package, where all points falling into the gray area would mean normally distributed data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If data is normally distributed, like in our example, &lt;strong&gt;the mean&lt;/strong&gt; is a good representative for our sample, in this case we’ll use &lt;strong&gt;one-sample t-test&lt;/strong&gt;. But, if data is skewed, we choose &lt;a href="https://yury-zablotski.netlify.com/post/wilcoxon-ака-mann-whitney-test/"&gt;&lt;strong&gt;one-sample Wilcoxon test&lt;/strong&gt;&lt;/a&gt; which uses &lt;strong&gt;median&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 2. check for normality

shapiro.test(my_to_dos) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Shapiro-Wilk normality test

data:  my_to_dos
W = 0.93695, p-value = 0.1896&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggpubr&amp;quot;)
library(ggpubr)

ggqqplot(my_to_dos)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file17d3c775964bd_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2 id="one-sample-t-test"&gt;One-sample t-test&lt;/h2&gt;
&lt;p&gt;Since &lt;em&gt;one-sample t-test&lt;/em&gt; checks the &lt;strong&gt;similarity&lt;/strong&gt; between our sample’s mean and the expected mean:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;our &lt;strong&gt;null hypothesis (H&lt;sub&gt;0&lt;/sub&gt;)&lt;/strong&gt; suggests that the means are similar, while&lt;/li&gt;
&lt;li&gt;our &lt;strong&gt;alternative hypothesis (H&lt;sub&gt;alt&lt;/sub&gt;)&lt;/strong&gt; suggests that means differ&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The presence of similarity or difference is not good or bad per se. If the mean of others is known to be correct, and your experiment shows no difference from that, be glad, your results are plausible. But if your results are different from the expected mean, you either did something wrong, or you might have discovered something new.&lt;/p&gt;
&lt;p&gt;{gghistostats} function from {ggstatsplot} package is probably the best way for our test. Within this function we need to specify only 5 arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;our data&lt;/strong&gt;, which we just created and called &lt;strong&gt;my_to_dos&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt; - is the numeric variable in our data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;test.value&lt;/strong&gt; is the productivity of others we want to compare to&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;type&lt;/strong&gt; of statistical approach. We choose &lt;strong&gt;parametric&lt;/strong&gt; for &lt;strong&gt;One-sample t-test&lt;/strong&gt;, and we’ll get back to other approaches later and finally…&lt;/li&gt;
&lt;li&gt;we set the &lt;strong&gt;normal.curve&lt;/strong&gt; argument to TRUE, only because it looks cool ;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This simple command results in a statistically rich and publication ready plot! Now, let’s interpret the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 3. compare your mean to the other mean (6 tasks = 60%)

# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(ggstatsplot)

gghistostats(
    data       = my_to_dos %&amp;gt;% as_tibble,
    x          = value,
    test.value = 6, ## default value is 0
    type       = &amp;quot;p&amp;quot;, 
    normal.curve = T
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file17d3c775964bd_files/figure-html/unnamed-chunk-4-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# save your plot
ggsave(&amp;quot;ostt.jpg&amp;quot;, plot = last_plot(), width = 5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="interpretation"&gt;Interpretation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;t-value&lt;/strong&gt; is the &lt;strong&gt;measure of similarity&lt;/strong&gt; between compared means measured in units of standard error. The further &lt;em&gt;t-value&lt;/em&gt; is from zero, the more different are the means:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[t = \frac{our.mean - expected.mean}{standart.error} = \frac{our.mean - expected.mean}{ \frac{standart.deviation} {\sqrt sample.size} }\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;DF&lt;/strong&gt; (not shown) stands for &lt;strong&gt;degrees of freedom&lt;/strong&gt;. &lt;em&gt;DF&lt;/em&gt; is always equal to the sample size - 1. For our 20 measurements, &lt;em&gt;DF&lt;/em&gt; = 10 - 1 = 19.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;t-values&lt;/strong&gt; and &lt;strong&gt;DFs&lt;/strong&gt; were previously used to calculate &lt;strong&gt;p-values&lt;/strong&gt;. But, modern statistical software always report &lt;em&gt;p-values&lt;/em&gt;, so, we can safely ignore &lt;em&gt;t&lt;/em&gt;. &lt;em&gt;P-values&lt;/em&gt; can also be seeing as the measure of similarity, the smaller p-value, the less similar the means are. Our &lt;em&gt;p-value&lt;/em&gt; of 0.013 indicates that our means are different. Actually, significantly different, if we follow the threshold of 0.05. Thus, we can conclude that we are significantly more productive than other people. So, we can feel proud about ourselves for a second… and then get back to work ;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Indeed, if you imagine doing 1.7 tasks more every day for one year, at the end of this year you’ll accomplish over 620 more tasks then the average person. &lt;strong&gt;But, more importantly&lt;/strong&gt;, this will make &lt;strong&gt;YOU&lt;/strong&gt; significantly better then yourself-one-year-ago &lt;strong&gt;in whatever you do.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Hedges’ g&lt;/em&gt; is the estimated &lt;em&gt;standardized&lt;/em&gt; difference between the means, and is the &lt;strong&gt;Effect Size&lt;/strong&gt;. It goes from zero to one and is interpreted in the same way as &lt;em&gt;Cohen’s d&lt;/em&gt; Effect size. In our example the effect is &lt;em&gt;medium strong&lt;/em&gt;, which supports the conclusion of the p-value that difference exists.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/effect_size_hedges_d.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There is one problem with &lt;em&gt;Hedges’ g&lt;/em&gt; though - it is a &lt;strong&gt;standardized difference&lt;/strong&gt; between the means, not the normal difference. And that is where &lt;strong&gt;Bayesian Difference&lt;/strong&gt; with it’s 95% Highest Density Intervals on the bottom of the plot seems more intuitive and is a second measure of the effect size here. It shows that the average person accomplishes 1.56 tasks less then you.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If that’s not enough, we can look at the &lt;strong&gt;Bayes Factor&lt;/strong&gt; (Jeffreys, 1961), which tests both null and alternative hypotheses at the same time. Bayes Factor of -1.37 in our example indicates a &lt;strong&gt;substantial evidence for the alternative hypothesis&lt;/strong&gt; - that our productivity is above average, which IS in line with the frequentists statistics on the top of the plot.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/bf_interpretation.png" /&gt;&lt;/p&gt;
&lt;h3 id="the-old-way-to-do-one-sample-t-test-in-r"&gt;The old way to do one-sample t-test in R&lt;/h3&gt;
&lt;pre class="r"&gt;&lt;code&gt;t.test(my_to_dos, mu = 6) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    One Sample t-test

data:  my_to_dos
t = 2.7116, df = 20, p-value = 0.01343
alternative hypothesis: true mean is not equal to 6
95 percent confidence interval:
 6.384558 8.948776
sample estimates:
mean of x 
 7.666667 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also conduct the t-test in a classical way in R, which gives us some additional metrics, for instance, &lt;strong&gt;95% confidence intervals (CI)&lt;/strong&gt; of the mean. The previous results of 6 [tasks/day] is outside of our &lt;strong&gt;95% CI&lt;/strong&gt; [6.38, 8.95], thus our result is indeed different. It also gives us the &lt;em&gt;Degrees of freedom&lt;/em&gt;, which were used to get p-values.&lt;/p&gt;
&lt;h3 id="one-sided-one-sample-t-test-do-them-only-if-you-really-know-what-you-are-doing"&gt;One-sided one-sample t-test (do them only if you really know what you are doing)&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;R&lt;/em&gt; performs a two-sided test by default. Performing a one-sided test allows us to say whether our result is &lt;strong&gt;significantly lower&lt;/strong&gt; or &lt;strong&gt;significantly greater&lt;/strong&gt;. We could also change the confidence level, to i.e. &lt;code&gt;conf.level = 0.99&lt;/code&gt; to be 99% sure of our conclusion:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t.test(my_to_dos, mu = 6, alternative = &amp;quot;less&amp;quot;,    conf.level = 0.99) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    One Sample t-test

data:  my_to_dos
t = 2.7116, df = 20, p-value = 0.9933
alternative hypothesis: true mean is less than 6
99 percent confidence interval:
     -Inf 9.220453
sample estimates:
mean of x 
 7.666667 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;t.test(my_to_dos, mu = 6, alternative = &amp;quot;greater&amp;quot;, conf.level = 0.99)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    One Sample t-test

data:  my_to_dos
t = 2.7116, df = 20, p-value = 0.006716
alternative hypothesis: true mean is greater than 6
99 percent confidence interval:
 6.11288     Inf
sample estimates:
mean of x 
 7.666667 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first &lt;em&gt;one-sided t-test&lt;/em&gt; shows that your mean performance is &lt;strong&gt;definitely not lower&lt;/strong&gt; (p&amp;gt;0.05 and expected average of 6 is included in 99% CI) then the average. The second &lt;em&gt;one-sided t-test&lt;/em&gt; indicates that your performance is &lt;strong&gt;significantly greater&lt;/strong&gt; then the average (p&amp;lt;0.05 and 6 is not included in 99% CI).&lt;/p&gt;
&lt;h2 id="one-sample-wilcoxon-test"&gt;One-sample Wilcoxon test&lt;/h2&gt;
&lt;p&gt;If our data is not normally distributed, the &lt;strong&gt;median&lt;/strong&gt; would describe our data much better than the average. And that’s what &lt;strong&gt;One-sample Wilcoxon test&lt;/strong&gt; does. The good news about {gghistostats} function is that, the only thing you need to change is the &lt;strong&gt;type&lt;/strong&gt; argument, from “parametric” to “nonparametric.” By the way, I encourage you to explore the function for yourself by simply asking R about it.&lt;/p&gt;
&lt;p&gt;If our data would have been not-normally distributed, your productivity would be even better, which you can see from&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the higher median to-do, namely 8, as compared with the mean of 7.7,&lt;/li&gt;
&lt;li&gt;lower p-value if we consider a p-value a measure of similarity (0.01 vs. 0.013) and&lt;/li&gt;
&lt;li&gt;stronger effect size (0.66 vs. 0.57).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;?gghistostats

gghistostats(
    data       = my_to_dos %&amp;gt;% as_tibble(),
    x          = value,
    test.value = 6, ## default value is 0
    type       = &amp;quot;np&amp;quot;, 
    normal.curve = T,
    xlab = &amp;quot;tasks done&amp;quot;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file17d3c775964bd_files/figure-html/unnamed-chunk-7-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggsave(&amp;quot;oswt.jpg&amp;quot;, plot = last_plot(), width = 5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the interpretation of the R-biserial correlation coefficient, as the effect size for the non-parametric One-sample Wilcoxon test. It shows a very large effect size&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/effect_size_rank_biserial.png" /&gt;&lt;/p&gt;
&lt;h2 id="whats-next"&gt;What’s next?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;first of all, never forget to cite this amazing package!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;citation(&amp;quot;ggstatsplot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  Patil, I. (2021). Visualizations with statistical details:
  The &amp;#39;ggstatsplot&amp;#39; approach. Journal of Open Source Software,
  6(61), 3167, doi:10.21105/joss.03167

A BibTeX entry for LaTeX users is

  @Article{,
    doi = {10.21105/joss.03167},
    url = {https://doi.org/10.21105/joss.03167},
    year = {2021},
    publisher = {{The Open Journal}},
    volume = {6},
    number = {61},
    pages = {3167},
    author = {Indrajeet Patil},
    title = {{Visualizations with statistical details: The {&amp;#39;ggstatsplot&amp;#39;} approach}},
    journal = {{Journal of Open Source Software}},
  }&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Need to do Chi-Square test? There is no better way than {ggbarstats} function from {ggstatsplot} package 📦. Here is [R demo on how to conduct, visualize and interpret Chi-Square test &amp;amp; pairwise post-hoc tests] (&lt;a href="https://youtu.be/8Tj0-yMPO64" class="uri"&gt;https://youtu.be/8Tj0-yMPO64&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="further-readings-and-watchings"&gt;Further readings and watchings&lt;/h2&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;a href="http://www.sthda.com/english/wiki/one-sample-t-test-in-r" class="uri"&gt;http://www.sthda.com/english/wiki/one-sample-t-test-in-r&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.instantr.com/2012/12/29/performing-a-one-sample-t-test-in-r/" class="uri"&gt;http://www.instantr.com/2012/12/29/performing-a-one-sample-t-test-in-r/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Funder, D. C., &amp;amp; Ozer, D. J. (2019). Evaluating effect size in psychological research: sense and nonsense. Advances in Methods and Practices in Psychological Science.&lt;/li&gt;
&lt;li&gt;Gignac, Gilles E, and Eva T Szodorai. 2016. “Effect Size Guidelines for Individual Differences Researchers.” Personality and Individual Differences 102: 74–78.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>ad2dcf8028cf2b8022b134c6631af623</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others</guid>
      <pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>R package reviews {DataExplorer} explore your data!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data</link>
      <description>What is the best way to explore the data quick? I think it's visualization. And what it the best way to visualize the data quick? I think it's - {DataExplorer} package, because it can visualize all your data in seconds using only one function! Check this out...</description>
      <category>R package reviews</category>
      <category>EDA</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data</guid>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data/2.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Survival analysis 2: parametric survival models</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models</link>
      <description>The non-parametric Kaplan-Meier method (KM) can not describe survival probability by a smooth function, which means it can not predict anything. The parametric models (e.g. Exponential, Weibull etc.) can! Besides, in case where parametric models are appropriate, they are more exact, more effective and more informative than KM or Cox. However, unfortunately, this step is often left out due to the rear use of parametric models. In this post we’ll try to close this gap.</description>
      <category>survival analysis</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models</guid>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models/thumbnail_survival_2.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {performance} check how good your model is! </title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is</link>
      <description>There are several indicators of model quality, e.g. $R^2$ or AIC, and several assumption for every model which supposed to be checked, e.g. normality of residuals, multicollinearity etc.. R provides solutions for every indicator or assumption you can imagine. However, they are usually spread around different packages and functions. {performance} package brings all of quality indicators and all of the assumption under one roof. Thus, for me it became the one-stop solution for modelling.</description>
      <category>R package reviews</category>
      <category>videos</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is</guid>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/14.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Survival analysis 1: a gentle introduction into Kaplan-Meier Curves</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves</link>
      <description>Survival time analysis is necessary in any study which investigates the time to a particular outcome of interest. Cancer studies in the medicine and the first failure of the car in the engineering field (failure time analysis) are good examples. The outcome of interest could be death, remission to relapse, progression, or failure. Point in time of reaching that outcome is generally called the event. Thank goodness, not every “event” is fatal 😃, but can sometimes even be a favorable outcome such as discharge from hospital. And thus, survival analysis is also a generic term, because it is not only about survival.</description>
      <category>survival analysis</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves</guid>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves/thumbnail_survival_1.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {janitor} clean your data!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data</link>
      <description>Data Scientists spend up to 80% of their time cleaning and preparing data for analysis. " Happy families are all alike; every unhappy family is unhappy in its own way" — Leo Tolstoy. "Like families, tidy datasets are all alike but every messy dataset is messy in its own way" - Hadley Wickham. Thats when "janitor" helps to clean the mess.</description>
      <category>R package reviews</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data</guid>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data/11.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>How to visualize models, their assumptions and post-hocs</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs</link>
      <description>A picture is worth a thousand words! This article shows how to visualize results of 16 different models in R: from a simple linear model to a multiple-additive-non-linear-mixed-effects model. Among them are logistic, multinomial, additive and survival models with and without interactions. **Goal: minimum R code &amp; maximum output!** We'll also go a bit beyond only model visualization. So, don't miss the bonuses 😉.</description>
      <category>visualization</category>
      <category>videos</category>
      <category>models</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs</guid>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/thumbnail_visualize_models.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>How to create a blog or a website in R with {Distill} package</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package</link>
      <description>If you're not online, you don't exist. A personal webpage or a blog became the business card of the digital century. It shows who you are and what you are capable of. Thus: show, don't tell.</description>
      <category>R &amp; the Web</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package</guid>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/images/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
  </channel>
</rss>
