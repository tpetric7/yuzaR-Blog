[
  {
    "path": "posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/",
    "title": "R package reviews {performance} check how good your model is! ",
    "description": "There are several indicators of model quality, e.g. $R^2$ or AIC, and several assumption for every model which supposed to be checked, e.g. normality of residuals, multicollinearity etc.. R provides solutions for every indicator or assumption you can imagine. However, they are usually spread around different packages and functions. {performance} package brings all of quality indicators and all of the assumption under one roof. Thus, for me it became the one-stop solution for modelling.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-03",
    "categories": [
      "R package reviews",
      "videos",
      "visualization"
    ],
    "contents": "\n\nContents\nR demo for how check model performance and model assumptions\nCheck model performance, or model qualityUsual linear models\nComplex mixed-effects models\nFancy Bayesian mixed-effects models\nCompare models\nUse individual quality indicators\n\nCheck assumptions: or modern (Jan 2020) model diagnosticsAll assumptions at once!\nIndividual assumptions with reports of statistical tests!\n\nUseful references\n\n\n\nlibrary(tidyverse)          # data manipulation\nlibrary(performance)        # model performance\n\n\n\nR demo for how check model performance and model assumptions\nA short (ca. 15 min) video below shows how performance package works and the code you’ll see in the video is provided below.\n\npreserve03620d5a50fb17f6\n\nCheck model performance, or model quality\nUsual linear models\nIn order to see how model performs, use the intuitive function - model_performance. It provide several quality indicators, which differ depending on the model. For instance the first model below provides the most “classical” quality indicators:\nAIC - Akaike’s Information Criterion, the lower the better. AIC is an estimator of out-of-sample prediction error and thereby relative quality of statistical models for a given set of data.\nBIC - Bayesian Information Criterion, the lower the better\n\\(R^2\\) - the proportion of the variance explained, the higher the better. It is sometimes referred to as a goodness of fit of the model\n\\(R^2 adjusted\\) - the proportion of the variance explained for multiple (several predictors) models, the higher the better\nRMSE - Root Mean Square Error is a measure of spread of the residuals around predictions (prediction errors), the lower the better\nSigma - standard deviation is a measure of spread of the data around the mean, the lower the better\n\n\nm <- lm(mpg ~ hp + cyl, data = mtcars)\n\nmodel_performance(m)\n\n\n# Indices of model performance\n\nAIC    |    BIC |   R2 | R2 (adj.) | RMSE | Sigma\n-------------------------------------------------\n169.56 | 175.42 | 0.74 |      0.72 | 3.02 |  3.17\n\nComplex mixed-effects models\nMixed effects model provide two different \\(R^2\\)s and ICC:\nconditional \\(R^2\\) shows model’s total explanatory power and\nmarginal \\(R^2\\) show the part related to the fixed effects (predictors) alone\nICC - intraclass correlation coefficient, is similar to \\(R^2\\) and also shows the goodness of fit or, in other words, quantifies the proportion of variance explained by a grouping (random) factor in mixed-effects (multilevel/hierarchical) models.\n\n\nlibrary(lme4)\n\nm1 <- lmer(mpg ~  hp * cyl + wt + (1 | am), data = mtcars)\n\nmodel_performance(m1)\n\n\n# Indices of model performance\n\nAIC    |    BIC | R2 (cond.) | R2 (marg.) |      ICC | RMSE | Sigma\n-------------------------------------------------------------------\n167.07 | 177.33 |       0.86 |       0.86 | 8.79e-03 | 2.06 |  2.24\n\nFancy Bayesian mixed-effects models\nELPD - expected log pointwise predictive density is a measure the prediction accuracy of Bayesian models, the closer to 0 the better. The out-of-sample predictive fit can either be estimated by Bayesian leave-one-out cross-validation (LOO) or by widely applicable information criterion (WAIC). Thus, the two next indicators are:\nLOOIC - leave-one-out cross-validation information criterion, the lower the better\nWAIC - widely applicable information criterion, the lower the better\n\n\nlibrary(rstanarm)\n\nm2 <- stan_glmer(mpg ~ hp * cyl + wt  + (cyl | am), data = mtcars, refresh=0)\n\nmodel_performance(m2)\n\n\n# Indices of model performance\n\nELPD   | ELPD_SE |  LOOIC | LOOIC_SE |   WAIC |   R2 | R2 (marg.) | R2 (adj.) | RMSE | Sigma\n--------------------------------------------------------------------------------------------\n-75.54 |    3.95 | 151.09 |     7.91 | 150.59 | 0.86 |       0.86 |      0.83 | 2.15 |  2.25\n\nCompare models\nUnfortunately we can’t compare not similar models, for instance different types of models (e.g. linear vs. mixed effects) or models done with different amounts of data, because one of them will be better for reasons other then model quality. Even more unfortunately, such comparisons are often conducted anyway. I was also guilty of it in the past. One of the common R functions allowing such comparisons is anova(model1, model2). It smartly does not allow the comparison of linear vs. mixed effects models (see the red warning below), but stupidly can be out-tricked by placing mixed effects model first.\n\n\nanova(m, m1)\n\n\n\nError: $ operator not defined for this S4 class\n\n\nanova(m1, m)\n\n\nData: mtcars\nModels:\nm: mpg ~ hp + cyl\nm1: mpg ~ hp * cyl + wt + (1 | am)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm     4 169.56 175.43 -80.781   161.56                         \nm1    7 151.05 161.31 -68.524   137.05 24.513  3  1.952e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIn contrast compare_performance() compares them immediately and provides a useful warning (see below). It also provides an somewhat superficial (according to the package author), but still useful ranking of models (see Performance-Score).\n\n\ncompare_performance(m, m1, rank = T)\n\n\n# Comparison of Model Performance Indices\n\nModel |    Type |    AIC |    BIC |    BF | RMSE | Sigma | Performance-Score\n----------------------------------------------------------------------------\nm1    | lmerMod | 167.07 | 177.33 | 0.385 | 2.06 |  2.24 |            75.00%\nm     |      lm | 169.56 | 175.42 |  1.00 | 3.02 |  3.17 |            25.00%\n\nWarning: Models are not of same type. Comparison of indices might be not meaningful.\nHowever, if we compare models which are comparable, no warning will be displayed.\n\n\nm1.1 <- lmer(mpg ~  hp + cyl + wt + (1 | am), data = mtcars)\ncompare_performance(m1, m1.1, rank = T)\n\n\n# Comparison of Model Performance Indices\n\nModel |    Type |    AIC |    BIC |   BF | R2 (cond.) | R2 (marg.) |      ICC | RMSE | Sigma | Performance-Score\n----------------------------------------------------------------------------------------------------------------\nm1    | lmerMod | 167.07 | 177.33 | 1.00 |       0.86 |       0.86 | 8.79e-03 | 2.06 |  2.24 |            71.43%\nm1.1  | lmerMod | 164.24 | 173.03 | 8.58 |       0.83 |       0.83 | 8.48e-03 | 2.35 |  2.51 |            28.57%\n\nMoreover, performance package can also easily plot this comparison, simply by wrapping up the compare_performance() function into a plot() function. It could not be easier or more elegant than that!\n\n\nplot( compare_performance(m1, m1.1) )\n\n\n\n\nUse individual quality indicators\n\n\nr2(m1)    # model fit - the proportion of the variance explained\n\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.864\n     Marginal R2: 0.862\n\nr2(m2)    # Bayesian R2 even with Credible Intervals\n\n\n# Bayesian R2 with Standard Error\n\n  Conditional R2: 0.863 (89% CI [0.805, 0.918])\n     Marginal R2: 0.861 (89% CI [0.769, 0.931])\n\nicc(m2)   # Intraclass Correlation Coefficient (ICC)\n\n\n# Intraclass Correlation Coefficient\n\n     Adjusted ICC: 0.971\n  Conditional ICC: 0.829\n\n# not part of the \"performance\" package, but fits well here\nAIC(m1)   # Akaike's Information Criterion\n\n\n[1] 167.0742\n\nBIC(m1)   # Bayesian Information Criterion\n\n\n[1] 177.3343\n\nCheck assumptions: or modern (Jan 2020) model diagnostics\nAll assumptions at once!\nThe first time I discovered check_model() function, I have got an intensive intellectual “nerdgasm”. I could not believe how simple and at the same time sophisticated this function is! I then checked out other work of the authors of {performance} package and was stunned, how many useful things they already produced. And I don’t think they’ll stop any time soon. Thus, if you just began to learn R and stats, check out their work to quickly step up your data science game!\n\n\ncitation(\"performance\")\n\n\n\nTo cite performance in publications use:\n\n  Lüdecke, Makowski, Waggoner & Patil (2020). Assessment of\n  Regression Models Performance. CRAN. Available from\n  https://easystats.github.io/performance/\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {performance: Assessment of Regression Models Performance},\n    author = {Daniel Lüdecke and Dominique Makowski and Philip Waggoner and Indrajeet Patil},\n    journal = {CRAN},\n    year = {2020},\n    note = {R package},\n    doi = {10.5281/zenodo.3952174},\n    url = {https://easystats.github.io/performance/},\n  }\n\nNow the function itself: it visually checks all the assumptions you need to check and gives you a big-picture overview of assumptions for almost any model you can have (at least for all the common ones). Two examples below display such a big-picture of a usual linear model and a mixed-effects model with random effects. The subplots of this picture even explain what you should look for! For instance: “Dots should be plotted along the line” in the residuals diagnostics plots. Have a look at the big-picture first and go to the next chapter for some more details on the particular assumption. Why? Because the package provides the opportunity to check individual assumptions too and even goes one step deeper into it!\n\n\ncheck_model(m)\n\n\n\n\n\n\ncheck_model(m1)\n\n\n\n\nIndividual assumptions with reports of statistical tests!\nChecking individual assumption is also very intuitive. For instance, for checking the normality of the residuals, use check_normality() function. It will conduct the Shapiro-Wilk Normality test and report the result of it. Put check_normality(m) inside of the plot() function to visualize the result. It is even preferable to visually inspect the residuals, because Shapiro-Wilk Test will often produce significant results for large sample sizes (and in the age of big data we always have large samples) even if data is perfectly normally distributed.\n\n\ncheck_normality(m)     # shapiro.test, however, visual inspection (e.g. Q-Q plots) are preferable\n\n\nOK: residuals appear as normally distributed (p = 0.212).\n\ncheck_normality(m1)\n\n\nWarning: Non-normality of residuals detected (p = 0.010).\n\nplot( check_normality(m1) )\n\n\nWarning: Non-normality of residuals detected (p = 0.010).\n\n\nThe collinearity is measured by the variance inflation factor (VIF). VIF<5 is acceptable for individual predictors, while VIF<10 is moderate, so, that it gives you an idea that some variables in the model might be redundant. If you model interactions, VIF would naturally increase, and the unwritten (I forgot where did I learned it from 🙈) rule is that interactions with VIF<20 are still acceptable.\n\n\ncheck_collinearity(m)  # by variance inflation factor (VIF)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n Parameter  VIF Increased SE\n        hp 3.26         1.80\n       cyl 3.26         1.80\n\ncheck_collinearity(m1)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n Parameter  VIF Increased SE\n        wt 2.50         1.58\n\nHigh Correlation\n\n Parameter    VIF Increased SE\n        hp  78.22         8.84\n       cyl  12.22         3.50\n    hp:cyl 122.72        11.08\n\nplot( check_collinearity(m1) )\n\n\n\n\nIf you wanna check the heteroscedasticity, use function check_heteroscedasticity(). The heteroscedasticity assumption itself is kind of self-explanatory. But what I can’t explain is that why nobody in the history of R programming language came up with such intuitive functions before {performance}?\n\n\ncheck_heteroscedasticity(m)\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.041).\n\ncheck_heteroscedasticity(m1)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.153).\n\nplot( check_heteroscedasticity(m) )\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.041).\n\n\nOne particular function somehow didn’t finish up in the “big-picture”. Namely check_outliers(). May be because there are too many methods for identifying outliers (“zscore”, “iqr”, “cook”, “pareto”, “mahalanobis”, “robust”, “mcd”, “ics”, “optics”, “lof”)? I don’t know. Z-Scores is the default method though, however, the method can be easily specified as an argument of the function:\n\n\ncheck_outliers(mtcars$mpg)\n\n\nWarning: 4 outliers detected (cases 18, 19, 20, 28).\n\nplot(check_outliers(mtcars$mpg))\n\n\n\nplot(check_outliers(mtcars$mpg, method = \"iqr\"))\n\n\n\n\nThere are many more useful functions in this package. And there is no need to describe them all here. If you liked what you have seen so far, just type “?performance” in the RStudio console, go to Help, scroll down to the bottom of the Help page, click Index and enjoy the package 😉.\nIf you think, I missed something, please comment on it, and I’ll improve this tutorial.\nCheers guys, have a good one!\nUseful references\nhttps://easystats.github.io/performance/\nhttps://cran.r-project.org/web/packages/performance/performance.pdf\nhttps://www.rdocumentation.org/packages/performance/versions/0.6.1\n\n\n\n",
    "preview": "posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/thumbnail_performance.jpg",
    "last_modified": "2021-01-03T14:59:21+01:00",
    "input_file": "r-package-reviews-performance-check-how-good-your-model-is.utf8.md"
  },
  {
    "path": "posts/2021-01-02-r-package-reviews-janitor-clean-your-data/",
    "title": "R package reviews {janitor} clean your data!",
    "description": "Data Scientists spend up to 80% of their time cleaning and preparing data for analysis. \" Happy families are all alike; every unhappy family is unhappy in its own way\" — Leo Tolstoy. \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way\" - Hadley Wickham. Thats when \"janitor\" helps to clean the mess.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-02",
    "categories": [
      "R package reviews",
      "videos"
    ],
    "contents": "\n\nContents\nGet dirty data and look at it\nR demo for how to clean your data\nMain functionsclean_names()\nremove_empty() & remove_constant()\nget_dupes()\nround_to_fraction()\nconvert_to_date()\nrow_to_names()\n\nGenerate “adorable” frequency table (1-, 2-, or 3-way).\n\n\n\nlibrary(tidyverse)          # data manipulation\nlibrary(janitor)            # data cleaning\nlibrary(readxl)             # data importing\nlibrary(kableExtra)         # beautifying tables\n\n\n\nGet dirty data and look at it\nIf you are reading this post, you are probably already familiar with the concept of tidy data. If not, have a look at it. And you have most likely already worked with the messy (dirty) data. I did, and that is why I found the janitor package sooo useful!\nThe messy data displayed below can be found here. Some of the indicators of messyness are: strange (difficult) names, empty columns and rows, constant columns, which do not provide much of a value, duplicates, strange dates which do not look like dates etc..\n\n\ndirty_data <- read_excel(\"dirty_data.xlsx\")\ndirty_data %>%\n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nFirst Name\n\n\nLast Name\n\n\nEmployee Status\n\n\nSubject\n\n\nHire Date\n\n\n% Allocated\n\n\nFull time?\n\n\ndo not edit! —>\n\n\nCertification…9\n\n\nCertification…10\n\n\nCertification…11\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nInstr. music\n\n\nVocal music\n\n\nNA\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nComputers\n\n\nNA\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nEnglish 6-12\n\n\nNA\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPhysical ed\n\n\nNA\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nPolitical sci.\n\n\nNA\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nNA\n\n\nVocal music\n\n\nEnglish\n\n\nNA\n\n\nR demo for how to clean your data\nA short (ca. 12 min) video below shows how to clean this data and the code you’ll see in the video is provided below.\n\npreserve84584ba96750c386\n\nMain functions\nclean_names()\nThis function removes all the non-letters and signs from the names and connects several words with underscores. You can ignore the kbl() and kable_classic_2() rows in the code below, they just make the HTML table look clean, but do not really clean anything in our dataset.\n\n\nd <- dirty_data %>% \n  clean_names()\n\nd %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ndo_not_edit\n\n\ncertification_9\n\n\ncertification_10\n\n\ncertification_11\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nInstr. music\n\n\nVocal music\n\n\nNA\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nComputers\n\n\nNA\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nEnglish 6-12\n\n\nNA\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPhysical ed\n\n\nNA\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nPolitical sci.\n\n\nNA\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nNA\n\n\nVocal music\n\n\nEnglish\n\n\nNA\n\n\nremove_empty() & remove_constant()\nremove_empty() removes both empty rows and empty columns. We have two empty columns and one empty row. They are just useless. Lets add two constant columns to the dataset and see how we can remove all this junk with janitor.\n\n\n# add two constant columns\nd <- d %>% \n  mutate(constant_column   = 42,\n         constant_column_2 = \"text\")\n\n# remove the junk\nd %>% \n  remove_constant() %>%  \n  remove_empty() %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ncertification_9\n\n\ncertification_10\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nPhysical ed\n\n\nTheater\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nPhysical ed\n\n\nTheater\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nInstr. music\n\n\nVocal music\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nPENDING\n\n\nComputers\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nPENDING\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nScience 6-12\n\n\nPhysics\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nScience 6-12\n\n\nPhysics\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nEnglish 6-12\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nPENDING\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nPhysical ed\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPolitical sci.\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nVocal music\n\n\nEnglish\n\n\nget_dupes()\nYou can hunt duplicates rows in several columns. The function also returns the counts for every duplicate.\n\n\nd %>% \n  get_dupes(first_name) %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\ndupe_count\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ndo_not_edit\n\n\ncertification_9\n\n\ncertification_10\n\n\ncertification_11\n\n\nconstant_column\n\n\nconstant_column_2\n\n\nChien-Shiung\n\n\n2\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\n42\n\n\ntext\n\n\nChien-Shiung\n\n\n2\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\n42\n\n\ntext\n\n\nJason\n\n\n2\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\n42\n\n\ntext\n\n\nJason\n\n\n2\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\n42\n\n\ntext\n\n\nround_to_fraction()\nThis can be very useful if you have lots of data. I do work with agricultural animals and they have all kinds of scores. One of them - Body Condition Score (BCS) is always recorded in quarters, e.g. 2.25, 2.5, 2.75 etc. So, if some colleagues try to be very smart and very exact, they stupidly record values like 0.8 or 3.149 instead of wanted 0.75 and 3, you need to correct this. round_to_fraction() simplifies this task enormously! Have a look at the last number below, before and after applying round_to_fraction() to a variable.\n\n\n# before\nd$percent_allocated\n\n\n [1] 0.75 0.25 1.00 1.00 1.00 0.50 0.50   NA 0.50 0.50   NA   NA 0.80\n\n# after\nround_to_fraction(d$percent_allocated, denominator = 4) # digits = 3\n\n\n [1] 0.75 0.25 1.00 1.00 1.00 0.50 0.50   NA 0.50 0.50   NA   NA 0.75\n\nconvert_to_date()\nA modern Excel always tries to automate things, and I hate it! 😂 For instance you write a number into a cell and it sometimes immediately converts it into date. Then you try to have a date in a cell, and it returns a number. Moreover, Excel also has some strange date encoding systems, which can be confused with a normal numeric columns. Luckily, our dirty dataset has a “date” word in the name of a column “hire_date”, otherwise we wouldn’t know that it is a date:\n\n\nd$hire_date\n\n\n [1] 39690 39690 37118 27515 41431 11037 11037    NA 32994 27919 42221\n[12] 34700 40071\n\nconvert_to_date(d$hire_date)\n\n\n [1] \"2008-08-30\" \"2008-08-30\" \"2001-08-15\" \"1975-05-01\" \"2013-06-06\"\n [6] \"1930-03-20\" \"1930-03-20\" NA           \"1990-05-01\" \"1976-06-08\"\n[11] \"2015-08-05\" \"1995-01-01\" \"2009-09-15\"\n\nrow_to_names()\nPeople often have several header columns, in order to beautifully explain everything, make things accurate and don’t miss any important information. I’ve been there too. But as soon as I started to work with software, I realized that this “beauty” hurts. I get a lot of Excel datasets like that, which is not a problem, I just delete not needed rows and continue working. But then I get the same table with a few corrected values, even if I tell colleagues to have only one header. That is why I was pleased to discover row_to_names() function. Have a look at the dataset “x” below and how easy can we handle it.\n\n\nx <- data.frame(\n  X_1 = c(\"some general description\", \"Real title\", 1:3),\n  X_2 = c(\"something `very!!! important` :) \", \"Which we wont!\", 4:6))\n\nx\n\n\n                       X_1                               X_2\n1 some general description something `very!!! important` :) \n2               Real title                    Which we wont!\n3                        1                                 4\n4                        2                                 5\n5                        3                                 6\n\nx %>%\n  row_to_names(row_number = 2)\n\n\n  Real title Which we wont!\n3          1              4\n4          2              5\n5          3              6\n\nGenerate “adorable” frequency table (1-, 2-, or 3-way).\nThe table() function is actually cool, but as I discovered tabyl() in the janitor packages, I couldn’t go back. First of all, I always need to explicitly write , useNA = “ifany” if I wanna see whether there NAs. Secondly the proportions have to be called with an extra function prop.table() on top of the table() function. Now, look at what tabyl() does:\n\n\n# old way\ntable(d$employee_status, useNA = \"ifany\")\n\n\n\nAdministration          Coach        Teacher           <NA> \n             1              2              9              1 \n\nprop.table(table(d$employee_status))\n\n\n\nAdministration          Coach        Teacher \n    0.08333333     0.16666667     0.75000000 \n\n# new way\ntabyl(d$employee_status) # \"show_na\" is TRUE by default\n\n\n d$employee_status n    percent valid_percent\n    Administration 1 0.07692308    0.08333333\n             Coach 2 0.15384615    0.16666667\n           Teacher 9 0.69230769    0.75000000\n              <NA> 1 0.07692308            NA\n\n# new way with two variables\nd %>% \n  tabyl(employee_status, full_time)\n\n\n employee_status No Yes NA_\n  Administration  0   1   0\n           Coach  2   0   0\n         Teacher  3   6   0\n            <NA>  0   0   1\n\nMoreover, along the counts janitor’s tabyl can also display totals, formatted percentages and even all of them together by using a family of “adorable” functions.\n\n\n#  \nd %>%\n  tabyl(employee_status, full_time) %>%\n  adorn_totals(c(\"col\", \"row\"))\n\n\n employee_status No Yes NA_ Total\n  Administration  0   1   0     1\n           Coach  2   0   0     2\n         Teacher  3   6   0     9\n            <NA>  0   0   1     1\n           Total  5   7   1    13\n\nd %>%\n  tabyl(employee_status, full_time) %>%\n  adorn_totals(\"row\") %>%\n  adorn_percentages(\"row\") %>%\n  adorn_pct_formatting() %>%\n  adorn_ns(position = \"front\") %>%               \n  adorn_title() %>% \n  adorn_title(\"combined\")\n\n\n employee_status/full_time  full_time                      \n           employee_status         No        Yes        NA_\n            Administration 0   (0.0%) 1 (100.0%) 0   (0.0%)\n                     Coach 2 (100.0%) 0   (0.0%) 0   (0.0%)\n                   Teacher 3  (33.3%) 6  (66.7%) 0   (0.0%)\n                      <NA> 0   (0.0%) 0   (0.0%) 1 (100.0%)\n                     Total 5  (38.5%) 7  (53.8%) 1   (7.7%)\n\n\n\nmtcars %>%\n  tabyl(am, cyl) %>%\n  adorn_totals(c(\"col\", \"row\")) %>% \n  adorn_percentages(\"all\") %>% \n  adorn_pct_formatting() %>%\n  adorn_ns() \n\n\n    am          4         6          8       Total\n     0  9.4%  (3) 12.5% (4) 37.5% (12)  59.4% (19)\n     1 25.0%  (8)  9.4% (3)  6.2%  (2)  40.6% (13)\n Total 34.4% (11) 21.9% (7) 43.8% (14) 100.0% (32)\n\nYou could also compare columns of two dataframes to see the difference. To see more function, type ?janitor in the console of RStudio, scroll down and press index.\n\n\nd1 <- d %>% \n  mutate(new_column = 42, \n         second_new = \"into it\")\n\ncompare_df_cols(d, d1) %>% View()\n\n\n\nEvery single function from janitor package makes your life easier and more productive for a moment, some of them a lot easier, e.g. clean_names() and remove_empty(). But the real power of it accumulates over time, because you free your mind and time for creative work, instead of solving problems. Thus, thousand thanks to package-developer Sam Firke!\n\n\ncitation(\"janitor\")\n\n\n\nTo cite package 'janitor' in publications use:\n\n  Sam Firke (2020). janitor: Simple Tools for Examining and\n  Cleaning Dirty Data. R package version 2.0.1.\n  https://CRAN.R-project.org/package=janitor\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {janitor: Simple Tools for Examining and Cleaning Dirty Data},\n    author = {Sam Firke},\n    year = {2020},\n    note = {R package version 2.0.1},\n    url = {https://CRAN.R-project.org/package=janitor},\n  }\n\nWhat is you favorite R package?\nIf you think, I missed something, please comment on it, and I’ll improve this tutorial.\n\n\n\n",
    "preview": "posts/2021-01-02-r-package-reviews-janitor-clean-your-data/thumbnail_janitor.jpg",
    "last_modified": "2021-01-03T09:51:03+01:00",
    "input_file": "r-package-reviews-janitor-clean-your-data.utf8.md"
  },
  {
    "path": "posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/",
    "title": "How to visualize models, their assumptions and post-hocs",
    "description": "A picture is worth a thousand words! This article shows how to visualize results of 16 different models in R: from a simple linear model to a multiple-additive-non-linear-mixed-effects model. Among them are logistic, multinomial, additive and survival models with and without interactions. **Goal: minimum R code & maximum output!** We'll also go a bit beyond only model visualization. So, don't miss the bonuses 😉.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-01",
    "categories": [
      "visualization",
      "videos",
      "models"
    ],
    "contents": "\n\nContents\nA demo for the first 7 linear models1. Simple linear model with one categorical predictor\n2. Simple linear model with one numeric predictor\n3. Multiple linear model with several categorical predictors\n4. Multiple linear model with several numeric predictors\n5. Multiple linear model with numeric and categorical predictors\n\nBonus 1: check all the assumption in one line of code6. Multiple linear model with interactions\n\nBonus 2: easy post-hocs7. Multiple linear model with interactions bwtween numeric predictors\n\nBonus 3: quick multiple models with ggplot\nA demo for the next 9 models: non-linear, logistic, multinomial, mixed-effects, survival…8. Multiple non-linear polynomial model with interactions\n9. Multiple non-linear Generalized Additive Models (GAM)\n10. Multiple logistic regression with interactions\n\nBonus 4: visualize the post-hoc analysis with the PairWise P-value Plot (pwpp)11. Multinomial logistic regression models via neural networks\n12. Multiple Linear Mixed-Effects-Model with interactions\n\nBonus 5: how to choose the best model13. GAMMs - Multiple Generalised Additive (non-linear) Mixed Effects Models\n14. Kaplan-Meier survival model\n15. Exponential Parametric Models\n16. Cox proportional hazard models\n\n\nA demo for the first 7 linear models\nTwo YouTube videos in this article demonstrate the code presented below and explain all you need to know. Thus, I reduced the text to a minimum in order to decrease redundancy. If you still have a question, drop me a comment on YouTube or on this article below. I’ll try to respond as quick as I can.\n\n\n\n\n\n\n\nLet’s get our first data. We’ll use several dataset through out this article.\n\n\n# install.packages(\"tidyverse\") \n# install.packages(\"ISLR\") \nlibrary(tidyverse) # data wrangling\nlibrary(ISLR)      # get \"Wage\" dataset\n\n# reproducibility: the same seed grows the same tree\nset.seed(1)        \n\nd <- Wage %>%\n  sample_n(1000) %>% \n  rename(salary = wage) \n\n# have a look at the data\nglimpse(d)\n\n\nRows: 1,000\nColumns: 11\n$ year       <int> 2004, 2003, 2007, 2006, 2004, 2006, 2004, 2003, …\n$ age        <int> 59, 45, 21, 49, 59, 41, 63, 42, 46, 42, 50, 26, …\n$ maritl     <fct> 2. Married, 2. Married, 1. Never Married, 2. Mar…\n$ race       <fct> 1. White, 1. White, 1. White, 1. White, 1. White…\n$ education  <fct> 5. Advanced Degree, 4. College Grad, 2. HS Grad,…\n$ region     <fct> 2. Middle Atlantic, 2. Middle Atlantic, 2. Middl…\n$ jobclass   <fct> 2. Information, 2. Information, 2. Information, …\n$ health     <fct> 2. >=Very Good, 2. >=Very Good, 1. <=Good, 2. >=…\n$ health_ins <fct> 1. Yes, 1. Yes, 2. No, 1. Yes, 1. Yes, 1. Yes, 1…\n$ logwage    <dbl> 5.204120, 4.602060, 4.565848, 5.107210, 4.490520…\n$ salary     <dbl> 182.02062, 99.68946, 96.14407, 165.20877, 89.167…\n\n1. Simple linear model with one categorical predictor\nThe effects() package is the main visualization package we’ll use. It can visualize results of almost any model. We’ll start with the visualization of predicted values.\n\n\nm <- lm(salary ~ jobclass, d)\n\n# install.packages(\"effects\")\nlibrary(effects)    # for model visualization & more\n\nplot(allEffects(m))\n\n\n\n\n2. Simple linear model with one numeric predictor\nHere we’ll add grid = TRUE for a better readability, so that we don’t need to stare on the y-axis and guess the result.\n\n\nm <- lm(salary ~ age, d)\n\nplot(allEffects(m), grid = TRUE)\n\n\n\n\n3. Multiple linear model with several categorical predictors\nWe can visualize all predictors at once, or any particular predictor from a multiple model individually.\n\n\nm <- lm(salary ~ jobclass + education, d)\n\nplot(allEffects(m))\n\n\n\nplot(predictorEffect(predictor = \"education\", mod = m))\n\n\n\n\n4. Multiple linear model with several numeric predictors\nHere we’ll see how to change the appearance of confidence intervals and introduce another amazing model-visualization package - sjPlot. In this chapter we’ll use the effects package for the visualization of the predicted values and sjPlot package for the visualization of the estimates with their confidence intervals.\n\n\nm <- lm(salary ~ age + year, d)\n\nplot(allEffects(m))\n\n\n\nplot(allEffects(m), confint=list(style=\"bars\"))\n\n\n\n# install.packages(\"sjPlot\")\nlibrary(sjPlot)    # for model visualization\nplot_model(m)\n\n\n\n\n5. Multiple linear model with numeric and categorical predictors\nWe can change design of the plot by determining the number of rows and columns in the plot(allEffects()). The plot_model() function can also display the numeric values of the estimates and the significance stars, which is often all we need. Besides, it looks much better than a table-looking output of the model results.\n\n\nm <- lm(salary ~ age + education, d)\n\nplot(allEffects(m))\n\n\n\n\n\n\nplot(allEffects(m), rows = 2, cols = 1)\n\n\n\n\n\n\nplot_model(m, show.values = TRUE)\n\n\n\n\nBonus 1: check all the assumption in one line of code\ncheck_model is just awesome! One of my favorite R functions! I get a “nerdgasm” every time I use it 😂. The video explains it very well.\n\n\n# install.packages(\"performance\")\nlibrary(performance)    # model assumptions & performance\ncheck_model(m)\n\n\n\n\n6. Multiple linear model with interactions\nFirst of all, the allEffects() functions visualizes interactions easily! Secondly, we can put several lines on the same plot with or without confidence intervals by using argument multiline = TRUE. “T” instead of “TRUE” also works. The sjPlot package is not only able to also easily visualize interactions, but can in addition be extended with the usual ggplot2 syntax, which can greatly improve the appearance of the plot.\n\n\nm <- lm(salary ~ education * jobclass, d)\n\n# not too neat representation!\nplot(allEffects(m))\n\n\n\n# better representation\nplot(allEffects(m), lines = list(multiline = TRUE))\n\n\n\n\n\n\n# perfect representation\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, type = \"int\")+theme_blank()+theme(legend.position = \"top\")\n\n\n\n\nBonus 2: easy post-hocs\nThe emmeans package is one of my favorite for conducting post-hocs. In this chapter we only display the results in the text/table form. Later we’ll also visualize post-hocs.\n\n\n# install.packages(\"emmeans\")\nlibrary(emmeans)      # for post-hocs\nemmeans(m, pairwise ~ jobclass | education, adjust = \"fdr\")$contrasts\n\n\neducation = 1. < HS Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information   0.7937 8.50 990  0.093  0.9257 \n\neducation = 2. HS Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information   0.0431 4.49 990  0.010  0.9923 \n\neducation = 3. Some College:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information  -8.7985 5.33 990 -1.651  0.0990 \n\neducation = 4. College Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information  -7.8854 4.94 990 -1.597  0.1105 \n\neducation = 5. Advanced Degree:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information -21.2809 8.24 990 -2.581  0.0100 \n\n7. Multiple linear model with interactions bwtween numeric predictors\n\n\nm <- lm(salary ~ age * health, d)\n\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, type = \"int\")+\n  theme_minimal()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\ncheck_model(m)\n\n\n\n\nBonus 3: quick multiple models with ggplot\nWe do not always need to explicitly model things in order to explore our data. A geom_smooth() function from ggplot2 package will automatically fit numeric data. Mostly with non-linear models (e.g. GAM), but the argument method=“lm” can force it to take the linear one. Moreover, we could also use any formula inside of the geom_smooth(), but if we need to write the formula anyway, we’d rather produce an explicit model. However, the code presented below is quick and easy, and therefore very practical! On this note we’ll enter the world of non-linear models and I recommend to watch the second video first, before checking out the code.\n\n\nggplot(d, aes(age, salary))+\n  geom_point()+\n  geom_smooth()+    # the quickest way to model numeric data\n  facet_grid(education~jobclass, scales = \"free\") # quick multiple model \n\n\n\n\nA demo for the next 9 models: non-linear, logistic, multinomial, mixed-effects, survival…\n\n\n\n\n\n\n\n8. Multiple non-linear polynomial model with interactions\nThe allEffects function can also easily handle polynomial non-linear models. In contrast, the plot_model function can’t. However, we still can plot the estimates.\n\n\nm <- lm(log(salary) ~ poly(age, 2) * health, d)\n\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, show.values = T)+\n  theme_bw()\n\n\n\n\n\n\ncheck_model(m)\n\n\n\n\n9. Multiple non-linear Generalized Additive Models (GAM)\nThere are two main GAM packages: gam and mgcv. They both have an identical function which we need - gam(), which produces a conflict and R might be confused about it. Thus, use gam::gam() to specify from which package exactly the gam() function should be used. The gam package has it’s own plotting function - plot.Gam and you can display all subplots on one plot by using par(mfrow = c(2, 2)). Please, don’t use “plot(allEffects(gam1))” for GAM models, since it will produce only linear results.\ns() function indicates that we would like to use smoothing splines.\n\n\n# install.packages(\"gam\")\nlibrary(gam)\n\ngam1 <- gam::gam(salary~s(year, df = 4)+s(age, df = 5)+education + jobclass, data=d)\n\npar(mfrow = c(2, 2) )\nplot.Gam(gam1 , se=TRUE, col= \"blue\")\n\n\n\n\n10. Multiple logistic regression with interactions\nInteractions in logistic regression are not a problem for both packages. Moreover, have a look at three different displays of post-hocs from the same model and find code differences. Hint: I love the “|” part of it!\n\n\nm <- glm(health ~ jobclass * health_ins, d, family = binomial)\n\nplot(allEffects(m))\n\n\n\nplot_model(m, type = \"int\")\n\n\n\nemmeans(m, pairwise ~ jobclass | health_ins, adjust = \"fdr\")$contrasts\n\n\nhealth_ins = 1. Yes:\n contrast                       estimate    SE  df z.ratio p.value\n 1. Industrial - 2. Information  -0.0553 0.177 Inf -0.312  0.7552 \n\nhealth_ins = 2. No:\n contrast                       estimate    SE  df z.ratio p.value\n 1. Industrial - 2. Information  -0.1830 0.241 Inf -0.760  0.4472 \n\nResults are given on the log odds ratio (not the response) scale. \n\nemmeans(m, pairwise ~ health_ins | jobclass, adjust = \"fdr\")$contrasts\n\n\njobclass = 1. Industrial:\n contrast       estimate    SE  df z.ratio p.value\n 1. Yes - 2. No    0.623 0.200 Inf 3.117   0.0018 \n\njobclass = 2. Information:\n contrast       estimate    SE  df z.ratio p.value\n 1. Yes - 2. No    0.495 0.222 Inf 2.228   0.0259 \n\nResults are given on the log odds ratio (not the response) scale. \n\n\n\nemmeans(m, pairwise ~ health_ins * jobclass, adjust = \"fdr\")$contrasts\n\n\n contrast                                     estimate    SE  df\n 1. Yes 1. Industrial - 2. No 1. Industrial     0.6232 0.200 Inf\n 1. Yes 1. Industrial - 1. Yes 2. Information  -0.0553 0.177 Inf\n 1. Yes 1. Industrial - 2. No 2. Information    0.4402 0.226 Inf\n 2. No 1. Industrial - 1. Yes 2. Information   -0.6785 0.196 Inf\n 2. No 1. Industrial - 2. No 2. Information    -0.1830 0.241 Inf\n 1. Yes 2. Information - 2. No 2. Information   0.4954 0.222 Inf\n z.ratio p.value\n  3.117  0.0055 \n -0.312  0.7552 \n  1.950  0.0768 \n -3.460  0.0032 \n -0.760  0.5367 \n  2.228  0.0518 \n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: fdr method for 6 tests \n\nBonus 4: visualize the post-hoc analysis with the PairWise P-value Plot (pwpp)\nAs promised, here is the visualization of the post-hocs. Notice the marginal means on the y axis and color-coded design of the plot, which connects the pairs the p-values for which were calculated and adjusted (x-axis).\n\n\npwpp(emmeans(m, ~ health_ins * jobclass), type = \"response\", adjust = \"fdr\"\n     )+theme_minimal()\n\n\n\n\n11. Multinomial logistic regression models via neural networks\nNo special treatment of the fancy neural networks needed. plot(allEffects()) just works!\n\n\n# get the data\nd <- foreign::read.dta(\"https://stats.idre.ucla.edu/stat/data/hsbdemo.dta\")\n\nm <- nnet::multinom(prog ~ ses + write, d)\n\n\n# weights:  15 (8 variable)\ninitial  value 219.722458 \niter  10 value 179.985215\nfinal  value 179.981726 \nconverged\n\nplot(allEffects(m), \n     lines = list(multiline = T), \n     confint = list(style = \"auto\"), rows = 2, cols = 1)\n\n\n\n\n12. Multiple Linear Mixed-Effects-Model with interactions\nThe mixed-effects models are very complex. But fortunately, all 4 main functions, namely plot(allEffects()), plot_model(), emmeans() and check_model() work flawlessly and simply deliver! The check_model() even checks the assumptions for the random effects! The random effects themselves can also be visualized, but they are rarely interpreted, so, why bother?\n\n\n# install.packages(\"lme4\")\n# install.packages(\"lmerTest\")\n\nlibrary(lme4)\nlibrary(lmerTest)\n\n# get the data\nset.seed(9)\nd <- InstEval %>% \n  group_by(service, studage) %>% \n  sample_n(100) %>% \n  mutate(dept = as.numeric(dept))\n\nm1 <- lmer(y ~ service * studage + (1|s) + (1|d), data=d)\n\nplot(\n  allEffects(m1), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m1, type = \"int\")+\n  theme_blank()+\n  theme(legend.position = \"top\")\n\n\n\n# post-hocs\nemmeans(m1, pairwise ~ service | studage, adjust = \"none\")$contrasts \n\n\nstudage = 2:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.447 0.186 787 2.404   0.0164 \n\nstudage = 4:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.183 0.186 783 0.983   0.3261 \n\nstudage = 6:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.120 0.184 785 0.650   0.5156 \n\nstudage = 8:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.344 0.185 782 1.858   0.0635 \n\nDegrees-of-freedom method: kenward-roger \n\npwpp(emmeans(m1, ~ service * studage), type = \"response\", adjust = \"none\")+\n  theme_minimal()\n\n\n\n\n\n\n# check model assumptions\ncheck_model(m1)\n\n\n\n\nBonus 5: how to choose the best model\nThe performance package, near the already insanely useful check_model() function, provides a compare_performance() function, which compares models with multiple quality indicators, e.g. \\(R^2\\) or AIC. It is not only more informative as compared to the anova() function, which is often used for model comparison, but also works much better, because it displays a warning when two models shouldn’t be compared, while anova(m, m1) simply fails, when models aren’t supposed to be compared, but can be tricked by placing the mixed effects model (m1) first.\n\n\nm  <- lm(y ~ service * studage, data=d)\nm1 <- lmer(y ~ service * studage + (1|s) + (1|d), data=d)\n\ncompare_performance(m, m1)\n\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n# Comparison of Model Performance Indices\n\nModel |            Type |     AIC |     BIC |      BF | RMSE | Sigma |   R2 | R2 (adj.) | R2 (cond.) | R2 (marg.)\n-----------------------------------------------------------------------------------------------------------------\nm     |              lm | 2702.04 | 2744.21 |    1.00 | 1.30 |  1.30 | 0.02 |  6.68e-03 |            |           \nm1    | lmerModLmerTest | 2709.21 | 2760.74 | < 0.001 | 1.09 |  1.18 |      |           |            |       0.02\n\nanova(m1, m)\n\n\nData: d\nModels:\nm: y ~ service * studage\nm1: y ~ service * studage + (1 | s) + (1 | d)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm     9 2702.0 2744.2 -1342.0   2684.0                         \nm1   11 2688.7 2740.2 -1333.3   2666.7 17.389  2  0.0001675 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n13. GAMMs - Multiple Generalised Additive (non-linear) Mixed Effects Models\nThese kind of models are just madness, can be easily visualized though.\n\n\n# install.packages(\"mgcViz\")\nlibrary(mgcViz)\n\nm <- gammV(y ~ s(as.numeric(d), k = 3) + lectage, random=list(s=~1), data= InstEval %>% \n             slice(1:5000))\n\nplot(m, allTerms = T)\n\n\nHit <Return> to see next plot:\n\nHit <Return> to see next plot:\n\n\n14. Kaplan-Meier survival model\nSurvival models can be visualized with survminer package. Even some statistical details can be displayed. I still did not figured out how to visualize the post-hocs for survival analysis. It you know how, please let me know. Thus, I here simply provide the p-values of the post-hocs.\n\n\n# install.packages(\"survival\")\n# install.packages(\"survminer\")\nlibrary(survival)\nlibrary(survminer)\n\nset.seed(1)\nd <- lung %>% \n  filter(ph.ecog != 3) %>% \n  sample_n(100)\n\nm <- survfit(Surv(time, status) ~ ph.ecog, data = d)\n\n# simple plot\nggsurvplot(m)\n\n\n\n\n\n\n# fancy plot\nggsurvplot(m, \n           pval = TRUE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\")\n\n\n\n\n\n\n# post-hocs for survival analysis\npairwise_survdiff(\n  formula = Surv(time, status) ~ ph.ecog, data = d, p.adjust.method = \"fdr\"\n)\n\n\n\n    Pairwise comparisons using Log-Rank test \n\ndata:  d and ph.ecog \n\n  0       1      \n1 0.14954 -      \n2 0.00042 0.01507\n\nP value adjustment method: fdr \n\n15. Exponential Parametric Models\nThese models are very rarely used.\n\n\n# install.packages(\"flexsurv\")\nlibrary(flexsurv)    # for Parametric Survival Modelling\n\nex <- flexsurvreg(Surv(time, status) ~ factor(ph.ecog), data = d, dist=\"exponential\")\n\nggsurvplot(ex)\n\n\n\n\n16. Cox proportional hazard models\nCox models are more common as compared to the exponential models and can be visualized with a beautiful ggforest() plot.\n\n\nm <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data =  d)\n\nggforest(m, d)\n\n\n\n\nI hope you found this article useful. The are of coarse more interesting models out there. Thus, please let me know what kind of models you make and how you visualize them.\nIf you think, I missed something, please comment on it, and I’ll improve this tutorial.\n\n\n\n",
    "preview": "posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/thumbnail_visualize_models.png",
    "last_modified": "2021-01-02T10:27:18+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/",
    "title": "How to create a blog or a website in R with {Distill} package (in progress!)",
    "description": "If you're not online, you don't exist. A personal webpage or a blog became the business card of the digital century. It shows who you are and what you are capable of. Thus: show, don't tell.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2020-12-26",
    "categories": [
      "R & the Web",
      "videos"
    ],
    "contents": "\n\nContents\nWhy should we use Distill?\nManual intermittent publishing (deploying)1. Install Distill package\n2. Create your blog either manually with RStudio or with one line of code2.1. with RStudio\n2.2. with one line of code\n\n3. Build your website\n4. Publish your blog (or a website) via Netlify\n5. Create a new blog-post\n\nAutomated continuous publishing (deploying)1. Install distill and usethis packages\n2. Create your blog either manually with RStudio or with one line of code2.1. with RStudio\n2.2. with one line of code\n\n3. Build your website\n4. Connect to your github4.1. Create new repository\n4.2. Stage and Commit\n4.3. Run “usethis::use_github()” to connect a local repository to Github\n\n5. Publish your blog (or a website) via Netlify\n6. Create a new blog-post\n\nBlog configuration with \"_site.yml\"\nTheming - change desing and appearence of your blog\nPimp your contentLinks\nPlots\nFigures\nTables\nAsides\nEquations\nFootnotes\nThumbnails\nCitations\n\nUseful ressources\n\nWhy should we use Distill?\nBecause it’s an easy, quick and free way to go online today. All you need to create a website or a blog is R, RStudio and Netlify account. Moreover, it requires very little programming. It might not look too fancy in the beginning (though totally enough for me), but with some effort you’ll be able to re-design your website as you wish. Besides, Distill was originally aimed to make scientific online-publishing easier, so, it can’t be too bad ;). By the way, The blog you reading right now was created with Distill. How? That’s what this article is all about.\nIf you don’t have or don’t want to have a Github account (and continuous deployment), just read on. But if you are comfortable with Github and wanna continuously deploy via Github, jump to the chapter “Automated continuous publishing (deploying)”.\nManual intermittent publishing (deploying)\nWhy should we even consider manual instead of automated? Well, for some non-programmers going with the continuous deployment via Github immediately might be very challenging and frustrating. I’ve been there. We all know what is “better” and “more time effective” in the long run, but the complexity might sometimes turn us off from doing something we would love to, resulting into “it’s just not my cup of tea” attitude. Other do not even need the continuous deployment, because they publish rarely, e.g. once per month, but would still like to be online. Going to the WordPress and co., which would most likely screw either code or code-output, is not always a solution. Fortunately, Distill and Netlify greatly reduce the complexity of going online. Thus, have a look at the quick video and then follow the step by step procedure described below and you’ll be online with your blog in minutes.\n1. Install Distill package\n\n\ninstall.packages(\"distill\")\n\n\n\n2. Create your blog either manually with RStudio or with one line of code\n2.1. with RStudio\nOpen RStudio\nGo to File => New Project… => New Directory => Distill Blog\n“Directory name” will be the name of the folder, where all the documents from your blog will be stored\nChoose the directory where this folder supposed to be stored\nGive your blog a name. You’ll be able to rename it later\nClick “Create project”\n2.2. with one line of code\n\n\ndistill::create_blog(dir = \"amazing blog\", title = \"A cool name of my blog\")\n# or\ndistill::create_website(dir = \"amazing website\", title = \"A cool name of my Website\")\n\n\n\n3. Build your website\nA new RStudio window with newly created files and folders (right bottom corner) will open itself. One of the folders \"_posts\" will contain all your posts. There are already some examples of it. But don’t bother about it now.\nFind the “Build” tab and press “Build Website”. You’ll see the process of building.\nClick “Open in Browser” and explore your website\nWe aren’t online yet. But very soon!\n4. Publish your blog (or a website) via Netlify\ngo the https://www.netlify.com/, sing up for Netlify (I used the Email way) and confirm your Email.\nIn Netlify you’ll see a window with: “Want to deploy a new site without connecting to Git? Drag and drop your site folder here”. If you somehow don’t see it, find and press “Team overview”.\ngo to the directory of your blog and find the \"_site\" folder\ndrag and drop the \"_site\" folder from your computed into this window\nwait a moment till the “Production” tab produces green colored “Published” and you’ll get a funny named website in the left top corner, also green and starting with “https://”. My was “https://condescending-darwin-bc567f.netlify.app” :)\nclick on it\nCongrats, you are online!\ngo back to Netlify, click “Site settings” => “Change site name”\nrename your site (e.g. better-name) and hit “Save”\nClick on the “better-name.netlify.app” to make sure the name has changed and the site is still working\ngo back to RStudio to populate your blog\n5. Create a new blog-post\nrun the line below with the name of your choice\n\n\ndistill::create_post(\"My second blog post...this one will be really good :-)\")\n\n\n\na new partly pre-filled Rmarkdown document will open itself in RStudio\nfill it with some text and code as you usually do with Rmarkdown documents\nhit “Knit”. NOTE: you’ll need to always “Knit” all changed or created blog-posts individually. It is the only way to update them. “Build Website” would not re-render them for you, because it’s computationally expensive and prone to problems. However, the Distill-Website (we are doing Distill-Blog now) would. You can learn more about the difference between them here.\nThis new post exists only on your local computer, still not online, thus…\ngo back to Netlify and click “Deploys” tab, where you’ll see another window with: “Need to update your site? Drag and drop your site folder here”\ndrag and drop the \"_site\" folder there and wait till “Production” tab produces green “Published”\nclick on the “better-name.netlify.app” to make sure the new blog-post appeared\nclick on your post and enjoy your creative work for a moment ;), then\ngo back to RStudio and repeat step 5\nAutomated continuous publishing (deploying)\nContinuous deployment is cool! But the path there can be a little prickly. This path may take a couple of hours or days (in my case 🙈). But, once there, you quickly forget all the troubles and using Github with continuous deployment becomes your second nature. So, I think there are many people which deploy either still manually or already continuously. But not many of them in the middle (I might be wrong though). Thus, I will assume you already have installed Git on your computer, created a Github account and connected your RStudio to your Github. If not, but you wanna be there, I could recommend a single short free online book which helped me go through it: Happy Git and GitHub for the useR. You’ll only need it once! If you ready to proceed, have a look at the quick video and then follow the step by step procedure described below and you’ll be online with your blog in minutes.\n1. Install distill and usethis packages\n\n\ninstall.packages(\"distill\")\ninstall.packages(\"usethis\")\n\n\n\n2. Create your blog either manually with RStudio or with one line of code\n2.1. with RStudio\nOpen RStudio\nGo to File => New Project… => New Directory => Distill Blog\n“Directory name” will be the name of the folder, where all the documents from your blog will be stored\nChoose the directory where this folder supposed to be stored\nGive your blog a name. You’ll be able to rename it later\nClick “Create project”\n2.2. with one line of code\n\n\ndistill::create_blog(dir = \"amazing blog\", title = \"A cool name of my blog\")\n# or\ndistill::create_website(dir = \"amazing website\", title = \"A cool name of my Website\")\n\n\n\n3. Build your website\nA new RStudio window with newly created files and folders (right bottom corner) will open itself. One of the folders \"_posts\" will contain all your posts. There are already some examples of it. But don’t bother about it now.\nFind the “Build” tab and press “Build Website”. Your blog will be created in a new window.\nClick “Open in Browser” and explore your website\nWe aren’t online yet. But very soon!\n4. Connect to your github\n4.1. Create new repository\nGo back to RStudio and run use_git() in order to create a new local Git repository\nthen answer two questions:\n“Is it ok to commit them?” Don’t commit by typing 3 for “No”, or “Nope” or similar.\n“A restart of RStudio is required to activate the Git pane Restart now?” Restart by typing 2 for “Yes” or “Yup”.\n\n\n\nusethis::use_git() \n\n\n\nAfter restart you’ll see a new “Git” tab appear between the “Build” and “Tutorial” tabs. That’s gut!\nclick on the “Git” tab and you’ll see empty boxed under “Staged”, lot’s of yellow question marks under “Status” and the file-names under “Path”.\n4.2. Stage and Commit\ncheck all the boxes and press “Commit” button, which is (vertically) between the “Status” and “History” tabs. A colourful window will pop up. This window describes all the changes you are about to make to your blog.\nFind the “Commit message” box and definitely describe what changes you have done (e.g. “First commit”), because then you’ll always be able to get back to the previous version, in case something stops working. That’s what they call - a version control.\npress “Commit”\nwait until you see the “Close” button and close “Git commit”\nforget the other pop up window and go back to RStudio.\n4.3. Run “usethis::use_github()” to connect a local repository to Github\n\n\nusethis::use_github() \n\n\n\nA new repository will be automatically created on your Github profile and the new browser window with your Github will pop up.\nIf you’ll be asked: “Which git protocol to use?”, choose the one with “https” and if you’ll then be asked: “Are title and description ok?”, agree to proceed.\nNOTE: if something (e.g. Github Personal Access Token) doesn’t work, get back to the Happy Git and GitHub for the useR book and work through it if you still didn’t. You’ll only need it once!\n5. Publish your blog (or a website) via Netlify\nsign into your Netlify account, if have one, if not…\ngo the https://www.netlify.com/, sing up for Netlify either with your Email or with your Github profile.\nclick a green box “New site from Git”\nconfigure Netlify on “Github”,\nchoose a newly created repository, you’ll recognize the name (“my new blog”?).\nNOTE: Make sure to set the “Publish Directory” to \"_site\" (could be “docs” if you checked some boxes while creating new project). \"_site\" (or “docs”) contains all the information about your blog.\nclick “Deploy”!\nwait a moment till the “Production” tab produces green colored “Published” and you’ll get a funny named website in the left top corner, also green and starting with “https://”. My was “https://condescending-darwin-bc567f.netlify.app” :)\nclick on it\nCongrats, you are online!\ngo back to Netlify, click “Site settings” => “Change site name”\nrename your site (e.g. better-name) and hit “Save”\nclick on the “better-name.netlify.app” to make sure the name has changed and the site is still working\ngo back to RStudio to populate your blog\n6. Create a new blog-post\nrun the line below with the name of your choice\n\n\ndistill::create_post(\"My second blog post...this one will be really good :-)\")\n\n\n\na new partly pre-filled RMarkdown document will open itself in RStudio\nfill it with some text and code as you usually do with RMarkdown documents\nhit “Knit”. NOTE: you’ll need to always “Knit” all changed or created blog-posts individually. It is the only way to update them. “Build Website” would not re-render them for you, because it’s computationally expensive and prone to problems. However, the Distill-Website (we are doing Distill-Blog now) would. You can learn more about the difference between them here.\nThis new post exists only on your local computer, still not online, thus…\ngo to the “Git” tab in RStudio and check all the boxes\npress “Commit”, a new window will pop up\nadd description of your commit\nagain press “Commit”\nwait until you see the “Close” button and close “Git commit” pop up window\npress “Push”. Pushing will transfer changes in your blog from your local computer to a remote place, namely your Github repository. And since your Github repository is connected to Netlify, this changes will be online after successful push.\nwait until you see the “Close” button and close “Git push” pop up window\nclose or ignore the other pop up window\nget back to your blog “better-name.netlify.app” and refresh (it may take a few seconds, so, don’t panic if the first refresh don’t work). You should see a new blog-post.\nCongrats! You now continuously deploy your online blog!\nclick on your post and enjoy your creative work for a moment ;), then\ngo back to RStudio and repeat step 6 with following routine:\nCreate or change posts\nKnit\nCommit\nPush\n\nYou don’t need to commit and push every change, only important ones. Think of this process as really saving the progress you made on your blog. Another useful thing I learned to appreciate after committing a couple of thousands of changes (which is annoying!) is to - check all the boxes (under “stage”) at once! For this:\ngo to “Terminal” tab in RStuio (it’s near the “Console”)\ntype “git add -A” and press enter\ncheck one of the stage-boxes, the rest of them suppose to be then check themselves automatically\nthen press “Commit”\nBy the way, if you don’t want to publish your post until you really satisfied with it, you can start out as a draft:\n\n\ndistill::create_post(\"Another nice post\", draft = TRUE)\n\n\n\nOr add draft: true to the post’s metadata. When you are ready, delete draft: true.\nBlog configuration with \"_site.yml\"\nOpen \"_site.yml\". You’ll see something like that:\n\nname: \"Your cool webpage\"\ntitle: \"Your cool webpage title\"\ndescription: |\n  Exploring something very important.\nbase_url: https://beta.rstudioconnect.com/content/your_thing/\nnavbar:\n  logo: images/fancy_logo.png\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\n    - text: \"Rest\"\n      href: rest.html\n    - icon: fa fa-rss\n      href: index.xml\noutput: distill::distill_article\n\n\"_site.yml\" is the most important document for your website. Configure it carefully and slowly. You can add a lot of useful things: categories, google-analytics, customize the navigation bar, add references, new theme with CSS code for an individual design of your site, icons of twitter & co. and much more, which is not to important for a new site and can be added at any time.\nIf you work through the Distill web page, you’ll see a lot of examples for how to design your \"_site.yml\" and “theme.css” files. But, the best way I found to do this, is just to find the \"_site.yml\" or “theme.css” of other Distill-blogs on Github and get inspiration from them by playing with your own code (I hope nobody is offended by this sentence due to the open source nature of R, but please let me know if it’s wrong and I’ll remove this recommendation!).\nWhile you already have \"_site.yml\" file in your blog-folder, you don’t have a “theme.css” file. To get one, read on…\nTheming - change desing and appearence of your blog\nYou can modify the CSS code in your theme after creating it by running the following line of code:\n\n\ndistill::create_theme(name = \"theme\") \n\n\n\nTo activate a custom theme site-wide, add a theme key to the top-level of your \"_site.yml\" configuration file:\n\nname: \"Your cool webpage\"\ntitle: \"Your cool webpage title\"\ntheme: theme.css \n(...the rest of your _site.yml)\n\nPimp your content\nMost elements enhancing your content, like links, tables, plots, equations etc., are similar to the usual R Markdown syntax. Thus for a deeper insights go to the R Markdown: The Definitive Guide book, it’s online and free. Below I just display some quick “how to” examples and provide links to a more thorough online resources.\nLinks\nThe links are displayed with the help of two different brackets. First, use the square brackets to produce a [clickable word or phrase], then, directly after the square brackets, use round brackets with the URL inside, e.g. (https://bookdown.org/yihui/rmarkdown/). The URL by itself would certainly also work, but it’s not as convenient as this.\nPlots\n\n\nlibrary(tidyverse)\nggplot(mtcars, aes(hp, mpg)) + \n  geom_point() + \n  geom_smooth() +\n  theme_bw()\n\n\n\n\nFigures\nYou can add external static and dynamic figures, plots, photos or diagrams by using knitr::include_graphics() function. More on figures here.\n\n\nknitr::include_graphics(\"images/your_figure.png\")\n\n\n\n\n\n\n\n\n\n\n\nTables\nMore on tables here.\n\n\n# install.packages(\"gtsummary\")\nlibrary(gtsummary)\niris %>% \n  tbl_summary()\n\n\npreserve00bdefe3a65759c1\n\nAsides\nYou can include notes or even plots “aside” (to the right) your article:\n\n\n\n\nHere the funny fact or some important info.\n\n\n\nEquations\nYou can use a Latex syntax for it:\n\n$$\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}$$\n\n\\[\n\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\n\\]\nFootnotes\nFootnotes are created in the usual RMarkdown way, namely ^[here is the content of your footnote] and here are two examples 1 and 2. Point your cursor on the footnote number or scroll to the bottom of the page to see what’s inside of the footnote.\nThumbnails\nThumbnails (or previews) are the images which are displayed along the post. They serve as an eye catcher. You can add a preview image into the post’s metadata by adding a “preview” field:\n\ntitle: \"Blog post on how to write blog posts\"\ndescription: |\n  Here we were out of ideas, and therefore we are proud to announce\nour new post about how to write a post. Creativity is a b**ch! ... And I love her!\npreview: images/photo-of-me-because-I-am-sooo-beautifuuulll-toniiiight.png\n\nIf you don’t provide a picture for preview, the first plot or picture from your blog-post will be used as a thumbnail by default. To override this behavior, you can add the preview = TRUE in to the code-chunk, e.g.: {r some_chunk, preview=TRUE}.\nIf you think, I missed something, please comment on it, and I’ll improve this tutorial.\nCitations\nYou either can include a BibTex-like citation of others or make your article citable (see the very bottom of this article). “How to cite” is best described in the citation section of the Distill website.\nUseful ressources\nThe best place to start is actually the Distill website itself: https://rstudio.github.io/distill.\nTom Mocks blog-post on how to build blogs with Distill helped me a lot! https://themockup.blog/posts/2020-08-01-building-a-blog-with-distill/\n\nHere is the first footnote, which does not suppose to interrupt the main text!↩︎\nHere is the second↩︎\n",
    "preview": "posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/images/thumbnail.png",
    "last_modified": "2021-01-02T17:58:56+01:00",
    "input_file": "how-to-create-a-blog-or-a-website-in-r-with-distill-package.utf8.md",
    "preview_width": 1920,
    "preview_height": 1080
  }
]
