<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>yuzaR-Blog</title>
    <link>https://yuzar-blog.netlify.app/</link>
    <atom:link href="https://yuzar-blog.netlify.app/index.xml" rel="self" type="application/rss+xml"/>
    <description>Data Science with R
</description>
    <generator>Distill</generator>
    <lastBuildDate>Mon, 21 Mar 2022 00:00:00 +0000</lastBuildDate>
    <item>
      <title>R demo | Two-Samples t-Test | Student's &amp; Welch's | How to conduct, visualise, interpret | What happens if we use a wrong test 😱</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-03-11-ttest</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 8 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/6qDRu_1kNXg" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/"&gt;One Sample tests&lt;/a&gt; would help.&lt;/p&gt;
&lt;h2 id="get-the-data"&gt;Get the data&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;tidyverse&amp;quot;)  # for everything ;)
library(tidyverse)

set.seed(1) # for reproducibility
d &amp;lt;- tibble(
  IT      = rnorm(n = 10, mean = 130, sd = 60),
  factory = rnorm(n = 10, mean = 100, sd = 10)
) %&amp;gt;% 
  gather(key = &amp;quot;job&amp;quot;, value = &amp;quot;wage&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll compare 10 random IT-workers with 10 random factory workers. First of all, we’ll see, that on average, IT-crowd earns more then folks in the factory. But is average actually a good choice? That question is important, because comparing averages only makes sense if the data is normally distributed. While if data is not-normally distributed, an average would not represent our data well!&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-03-11-ttest/not_normal.png" /&gt;&lt;/p&gt;
&lt;h2 id="check-normality"&gt;Check normality&lt;/h2&gt;
&lt;p&gt;So, it’s obvious that we NEED to check for normality. For that we’ll use the {normality} function from {dlookr} package, which conducts Shapiro-Wilk normality tests with every sample. High p-values in both samples indicate that our data IS normally distributed, so now we are sure that using a &lt;strong&gt;parametric t-test&lt;/strong&gt; is a right choice.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;dlookr&amp;quot;)
library(dlookr)
d %&amp;gt;% 
  group_by(job) %&amp;gt;% 
  normality(wage)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 × 5
  variable job     statistic p_value sample
  &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
1 wage     factory     0.895   0.193     10
2 wage     IT          0.938   0.534     10&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="check-homogeneity-of-variances"&gt;Check Homogeneity of Variances&lt;/h2&gt;
&lt;p&gt;However, the normality alone is not enough to make a right decision, because there are two different t-tests: Student’s t-test and Welch’s t-test. In order to choose the right test, we need to understanding variance of our data. And here is why: (1) even very different means with huge variance (samples a and b) may not be significantly different while (2) even very similar means with small variance (samples c and d) can be significantly different.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-03-11-ttest/huge_variance.jpg" style="width:45.0%" /&gt; &lt;img src="https://yuzar-blog.netlify.app//posts/2022-03-11-ttest/small_variance.jpg" style="width:45.0%" /&gt;&lt;/p&gt;
&lt;p&gt;So, the variances between two samples can be either different (sometimes called - heterogen) or similar (sometimes called - homogen). And we need to know which one is true, because a classic Student’s t-Test can be applied only when variances are similar! While two samples with different variances should be analyzed with Welch’s t-Test.&lt;/p&gt;
&lt;p&gt;Levene’s Test for Homogeneity of Variance helps to decide which t-test to use. The {leveneTest} function from {car} package shows a small p-value, which tells us that our variances differ and that we need to use Welch’s t-Test.&lt;/p&gt;
&lt;p&gt;Now, having checked both assumptions, we are ready to compute Welch’s t-test.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;#install.packages(&amp;quot;car&amp;quot;)
library(car)
leveneTest(wage ~ job, d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Levene&amp;#39;s Test for Homogeneity of Variance (center = median)
      Df F value  Pr(&amp;gt;F)   
group  1  10.579 0.00442 **
      18                   
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="compute-a-correct-two-samples-t-test"&gt;Compute A Correct Two-Samples t-Test&lt;/h2&gt;
&lt;p&gt;And the best way to compute our test (in my opinion) is the {ggbetweenstats} function from {ggstatsplot} package, which needs only 5 arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, &lt;strong&gt;our data&lt;/strong&gt; - d, which we just created, with&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt; - as the grouping variable - job, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;y&lt;/strong&gt; - being wages&lt;/li&gt;
&lt;li&gt;then, since our data is normally distributed, we’ll choose a &lt;strong&gt;parametric type&lt;/strong&gt; of statistical approach,&lt;/li&gt;
&lt;li&gt;and since our jobs have different variances, we set &lt;strong&gt;var.equal&lt;/strong&gt; argument to FALSE&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Such simple command results in this statistically rich and publication ready plot! Now, let’s interpret the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(ggstatsplot)

set.seed(1)   # for Bayesian reproducibility of 95% CIs
ggbetweenstats(
  data = d,
  x    = job, 
  y    = wage, 
  type = &amp;quot;parametric&amp;quot;, 
  var.equal = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf6b6ef8c1_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggsave(filename = &amp;quot;t_test.jpg&amp;quot;, plot = last_plot(), width = 5.1, height = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="interpret-the-result"&gt;Interpret the result&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Welch’s t-statistics&lt;/strong&gt; is the &lt;strong&gt;measure of similarity&lt;/strong&gt; between compared samples measured in units of standard error. The further &lt;em&gt;t-value&lt;/em&gt; is from zero, the more different are the samples. But &lt;strong&gt;t-value&lt;/strong&gt; by itself can not say how far from zero is far enough, to conclude that this difference is significant. That’s why t-value and the degrees of freedom were previously used to get a &lt;strong&gt;p-value&lt;/strong&gt;. But nowadays every software delivers p-values by default.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;our &lt;strong&gt;P-value&lt;/strong&gt; of 0.04 shows a moderate evidence against the null hypothesis (H&lt;sub&gt;0&lt;/sub&gt;), that mean salaries are similar, in favor of the alternative hypothesis (H&lt;sub&gt;Alt&lt;/sub&gt;), that average salaries differ. Particularly, IT-crowd gets 35.000 dollars on average more than factory-workers. But is a difference of 35.000 dollars large? A P-value can not tell that. A P-value only checks whether there is a difference, but not how large this difference is.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-03-11-ttest/p_value_interpretation.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-03-11-ttest/interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fortunately, {ggbetweenstats} provides &lt;strong&gt;Hedges’ g&lt;/strong&gt; as the measure of the &lt;strong&gt;Effect Size&lt;/strong&gt;, which shows how large the difference in salaries is. &lt;strong&gt;Hedges’ g&lt;/strong&gt; is interpreted in the same way as &lt;strong&gt;Cohen’s d&lt;/strong&gt; Effect size, but outperforms &lt;strong&gt;Cohen’s d&lt;/strong&gt; for small samples, like in our example. Our effect size of -0.96 &lt;strong&gt;is large&lt;/strong&gt;, and tells us that the difference of 35.000 dollars is literally - &lt;strong&gt;large&lt;/strong&gt;. Well, the effect size is cool, but not particularly intuitive, right?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-03-11-ttest/effect_size_hedges_d.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Luckely, {ggbetweenstats} provides the &lt;strong&gt;Bayesian Difference&lt;/strong&gt; between our samples with 95% Highest Density Intervals as the second and more intuitive measure of the effect size. A difference of 27.000 bucs per year seems large to me, because I could get a new car for that money ;) (show picture/video of a funny car). The difference is intuitive, but neither difference, not effect size, do not test hypotheses.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;And that what &lt;strong&gt;Bayes Factor&lt;/strong&gt; is for. Our &lt;strong&gt;Bayes Factor&lt;/strong&gt; (Jeffreys, 1961), which is conceptually similar to the &lt;strong&gt;p-value&lt;/strong&gt;, is king of … small. A &lt;strong&gt;Bayes Factor&lt;/strong&gt; of -0.84 indicates that the difference is &lt;strong&gt;Not worth more than a bare mention&lt;/strong&gt; … which IS similar to a moderate evidence against the Null Hypothesis found by frequentists statistics on the top of the plot.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-03-11-ttest/bf_interpretation.png" /&gt;&lt;/p&gt;
&lt;h3 id="final-conclusion"&gt;Final conclusion&lt;/h3&gt;
&lt;p&gt;But how can it be, that both the &lt;strong&gt;effect size&lt;/strong&gt; and the &lt;strong&gt;difference&lt;/strong&gt; are large, while this difference is hardly significant??? Well, this happens often, and here is why. First, some factory workers still earn more then some of the IT guys. For example an older worker on BMW factory, ears definitely more than a younger IT-guy in some start-up. Secondly, we only have 10 people in each sample, which is simply not enough to conclude that our huge difference appeared not by accident. Let me prove it to you. If we double the number of people per group, the frequentists effect size and the Bayesian difference remained almost the same, while both, p-value and Bayes factor became very significant and showed a strong evidence against the null hypothesis in favor of the alternative.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(1) # for reproducibility
d2 &amp;lt;- tibble(
  IT      = rnorm(n = 20, mean = 130, sd = 60),
  factory = rnorm(n = 20, mean = 100, sd = 10)
) %&amp;gt;% 
  gather(key = &amp;quot;job&amp;quot;, value = &amp;quot;wage&amp;quot;)

set.seed(1)   # for Bayesian reproducibility of 95% CIs
ggbetweenstats(
  data = d2,
  x    = job, 
  y    = wage, 
  type = &amp;quot;parametric&amp;quot;, 
  var.equal = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf6b6ef8c1_files/figure-html/unnamed-chunk-6-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Thus, a p-value and the effect size never contradict each other, simply because they show different things. And if our p-value is around 0.05, like 0.04 in our example, Ronald Fisher - the father of modern statistics - himself recommended to treat such result as suggestive or inconclusive and collect more data. And that’s all we can do. What we can not do, however, is to use a wrong test. And there are three ways things can get messed up.&lt;/p&gt;
&lt;h2 id="what-happens-if-we-choose-a-wrong-test"&gt;What happens if we choose a wrong test?&lt;/h2&gt;
&lt;h3 id="taking-non-parametric-test-out-of-pure-laziness"&gt;1. Taking non-parametric test out of pure laziness&lt;/h3&gt;
&lt;!-- Here I can show 4 parts of the screen with code first, then with picture + p.values then Error 1 &amp; 2 types instead of 3 of those pictures and living one left upper corner as the winner. I can also use a moving backgroupd as the frage and pictures separator - that's fucking genious ;) --&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggbetweenstats(
  data = d,
  x    = job, 
  y    = wage, 
  type = &amp;quot;nonparametric&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf6b6ef8c1_files/figure-html/unnamed-chunk-7-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;First, if we are lazy to check the normality of data, we’ll go straight to the &lt;strong&gt;non-parametric Mann–Whitney U Test&lt;/strong&gt;. Here, a higher then the real p-value failed to reject the Null Hypothesis, failing to find a difference, where difference actually exists. Such mistake is called &lt;strong&gt;type II Error&lt;/strong&gt;, or simply - &lt;strong&gt;missing a discovery&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-03-11-ttest/type_2_errors.png" /&gt;&lt;/p&gt;
&lt;h3 id="wrong-homogeneity-of-variances"&gt;2. Wrong homogeneity of variances&lt;/h3&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggbetweenstats(
  data = d,
  x    = job, 
  y    = wage, 
  type = &amp;quot;parametric&amp;quot;, 
  var.equal = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf6b6ef8c1_files/figure-html/unnamed-chunk-9-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Secondly, if we are lazy to check the similarity of variances, we’ll go straight to the classic Student’s t-test which assumes equal variance, and produces smaller then real p-value indicating more evidence for the difference then necessary. This mistake is called &lt;strong&gt;type I Error&lt;/strong&gt;, or simply - &lt;strong&gt;discovering nonsense&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For unequal variances and/or unequal sample sizes Welch’s t-Test is more reliable and more robust then Student’s t-Test, while has the same power.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-03-11-ttest/type_1_errors.png" /&gt;&lt;/p&gt;
&lt;h3 id="taking-paired-test-hopefully-only-by-mistake"&gt;3. Taking paired test (hopefully only) by mistake&lt;/h3&gt;
&lt;p&gt;But that’s not all!!! We could be even more wrong if we took a &lt;strong&gt;PAIRED t-Test&lt;/strong&gt;, because we would test a completely different Null Hypothesis. Namely, that exactly the same 10 factory workers changed their job to IT and compare their salaries before and after this change. This is not the case in our example, but can be your next experiment. Then, Paired t-Test would be absolutely correct, and if you want to understand it really well, check out this video.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggwithinstats(
  data = d,
  x    = job, 
  y    = wage, 
  type = &amp;quot;parametric&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf6b6ef8c1_files/figure-html/unnamed-chunk-11-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2 id="whats-next-or-dont-use-t-test-if"&gt;What’s next, or Don’t use t-Test if&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;samples are paired. In this case apply&lt;a href="https://yuzar-blog.netlify.app/posts/2022-01-22-pairedsamplesttestinr/"&gt;&lt;strong&gt;Paired t-test&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;samples are small (n&amp;lt;30) and not-normally distributed. In this case use &lt;strong&gt;two-samples Mann–Whitney U Test&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you need to compare more then two samples, first check whether they are normally distributed, and if they are, go to &lt;a href="https://yury-zablotski.netlify.com/post/one-way-anova/"&gt;ANOVA&lt;/a&gt;, but if they aren’t, go to &lt;em&gt;Kruskal Wallis&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>c1709b781d239b65d5e51d43d72730f2</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-03-11-ttest</guid>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-03-11-ttest/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Cochran’s Q Test + Pairwise McNemar Tests (post-hoc)</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-03-04-cochran</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 5 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/a-rEZUd8FzQ" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="get-the-data"&gt;Get the data&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;tidyverse&amp;quot;)
library(tidyverse)

# get the data
set.seed(9) # for reproducibility 
data_wide &amp;lt;- data.frame(
  before = sample(c(&amp;quot;+&amp;quot;,&amp;quot;-&amp;quot;,&amp;quot;+&amp;quot;), 30, replace = TRUE),
  month  = sample(c(&amp;quot;-&amp;quot;,&amp;quot;+&amp;quot;,&amp;quot;-&amp;quot;), 30, replace = TRUE),
  year   = sample(c(&amp;quot;-&amp;quot;,&amp;quot;-&amp;quot;,&amp;quot;+&amp;quot;), 30, replace = TRUE)) %&amp;gt;% 
  mutate(id = 1:nrow(.))

data_long &amp;lt;- data_wide %&amp;gt;% 
  gather(key = &amp;quot;vaccine_time&amp;quot;, value = &amp;quot;outcome&amp;quot;, before:year) %&amp;gt;% 
  mutate_all(factor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Imagine that scientists tried to understand the impact of a anti-zombie-vaccine over time. So, they randomly capture 30 “volunteers” and test their zombieness on three different timepoints: first - before giving the anti-zombie-vaccine, secondly - one month after vaccination and lastly - one year after vaccination, just to see how long the effect of the vaccine holds.&lt;/p&gt;
&lt;p&gt;Any of the two paired time-points could be compared with a &lt;a href="https://yuzar-blog.netlify.app/posts/2022-02-20-mcnemar/"&gt;McNemar test&lt;/a&gt;, but if you have 100s of time-points it’s a lot of work. So, the lazy scientists found a shortcut - namely Cochran’s Q Test, which checks whether there is any difference among time-points at all. For that, they gathered all time-points below each other to create only three variables. The vaccine time-points themselves, the test results and finally the id of each individual volunteer, which helps not to mess things up.&lt;/p&gt;
&lt;p&gt;Cochran’s Q Test &lt;strong&gt;IS&lt;/strong&gt; useful, because if there is no difference between 100s of time-points, it will give you a high p.value and you wouldn’t need to compare 100s of time-points among each other pairwisely. That saves time! Hovewer, if Cochran test is significant, like in our example, we’d need to compare samples among each other pairwisely with McNemar tests.&lt;/p&gt;
&lt;p&gt;But what does Cochran test actually do? Visualizing the data helps to understand it better.&lt;/p&gt;
&lt;h2 id="visualize-proportions"&gt;Visualize proportions&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(ggstatsplot)
ggbarstats(
  data = data_long, 
  x    = outcome, 
  y    = vaccine_time, 
  paired = T, 
  label = &amp;quot;both&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf6f950487_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Cochran’s Q Test looks for differences in proportions of three or more paired samples, in which the same individuals appear in each sample. So, the &lt;strong&gt;Null Hypothesis&lt;/strong&gt; for Cochran’s Q Test is that the proportion of “successes” is the same for all groups. While the &lt;strong&gt;Alternate Hypothesis&lt;/strong&gt; is that the proportions differ for at least one group. Visualized data already shows that there is a difference in proportions, &lt;strong&gt;we just don’t know whether this difference is significant&lt;/strong&gt;. That’s why we need to conduct a test!&lt;/p&gt;
&lt;!-- It is an extension of the McNemar test; the two tests are equal if Cochran’s Q is calculated for two groups.  --&gt;
&lt;h2 id="compute-cochrans-q-test"&gt;Compute Cochran’s Q Test&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(rstatix)
library(rstatix)

cochran_qtest(data_long, outcome ~ vaccine_time|id)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 × 6
  .y.         n statistic    df       p method          
* &amp;lt;chr&amp;gt;   &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;           
1 outcome    30      9.33     2 0.00940 Cochran&amp;#39;s Q test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For that we’ll use {cochran_qtest} function from {rstatix} package, which needs only 2 arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;our data in a lazy long format and&lt;/li&gt;
&lt;li&gt;the formula, which takes
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;outcome&lt;/strong&gt; on the left side, which needs to be binomial and mutually exclusive, for example + &amp;amp; -, yes &amp;amp; no&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;time-points&lt;/strong&gt; will get on the right side of the formula and&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;id&lt;/strong&gt; of each volunteer after a &lt;strong&gt;vertical dash&lt;/strong&gt; to make sure results don’t get messed up between individuals&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="pairwise-mcnemar-tests"&gt;Pairwise McNemar Tests&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;pairwise_mcnemar_test(data    = data_long, 
                      formula = outcome ~ vaccine_time|id)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3 × 6
  group1 group2       p  p.adj p.adj.signif method      
* &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;       
1 before month  0.00952 0.0286 *            McNemar test
2 before year   0.48    1      ns           McNemar test
3 month  year   0.0433  0.13   ns           McNemar test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the results are significant, we’d need to conduct Pairwise McNemar Tests, and luckely for us, {rstatix} package provides {pairwise_mcnemar_test} function which does just that and needs the same two arguments, data and formula. However, this simplicity is dangerous for two reasons.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pairwise_mcnemar_test(data    = data_long, 
                      formula = outcome ~ vaccine_time|id, 
                      correct = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3 × 6
  group1 group2       p  p.adj p.adj.signif method      
* &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;       
1 before month  0.00468 0.014  *            McNemar test
2 before year   0.346   1      ns           McNemar test
3 month  year   0.0209  0.0627 ns           McNemar test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, the function uses continuity correction by default, which was shown to be very conservative by several scientific papers. Thus, if we stop the function from using continuity correction with {correct = FALSE} argument and compare the results - with and without correction, we’ll see that continuity corrected p-values are higher, which might help to miss an important discovery, also known as the &lt;strong&gt;Type II Error&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pairwise_mcnemar_test(data    = data_long, 
                      formula = outcome ~ vaccine_time|id, 
                      correct = F, 
                      p.adjust.method = &amp;quot;holm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3 × 6
  group1 group2       p  p.adj p.adj.signif method      
* &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;       
1 before month  0.00468 0.014  *            McNemar test
2 before year   0.346   0.346  ns           McNemar test
3 month  year   0.0209  0.0418 *            McNemar test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second danger is that this function uses a &lt;strong&gt;Bonferroni correction for multiple comparisons&lt;/strong&gt; by default, which was also shown to be too conservative. A correction for multiple comparisons is important though, because otherwise we can discover nonsense, also known as the &lt;strong&gt;Type I Error&lt;/strong&gt;. Fortunately, {pairwise_mcnemar_test} function allows to change the method easily, and if we use Holm method we’d see that the difference between month and year becomes significant.&lt;/p&gt;
&lt;h2 id="final-interpretation"&gt;Final interpretation&lt;/h2&gt;
&lt;p&gt;So, let’s make final conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;a very small p-value of the general Cochran’s Q Test allows us to reject the Null Hypothesis about similar proportions in favor of the Alternative Hypothesis - that proportions between time-points differ.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the consecutive Pairwise McNemar Tests show that vaccination significantly reduces the proportion of zombies as compared to not-vaccinated people and that we need to refresh the vaccine, because the proportion of zombies one year after vaccination significantly increases again.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;but, if we used a Bonferroni or continuity correction, we would think, that one vaccine is enough and would increase our risk to become a zombie.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, as you can see, using right statistics is healthy ;) But you might have wondered - what to do - if the outcome is not only + and - but has more levels? Well, then you can use &lt;a href="https://yuzar-blog.netlify.app/posts/2022-02-08-friedman/"&gt;Friedman test&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="whats-the-next-best-thing-to-learn"&gt;What’s the next best thing to learn?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://yuzar-blog.netlify.app/posts/2022-02-08-friedman/"&gt;Friedman test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mixed-Effects logistic regression (not for the beginners in stats)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>142fbf8bdb397e325d7d394176557286</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-03-04-cochran</guid>
      <pubDate>Fri, 11 Mar 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-03-04-cochran/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Paired Samples t-Test | How to conduct, visualise and interpret</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-01-22-pairedsamplesttestinr</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 6 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/QRS2z4ZmGTQ" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/"&gt;One Sample tests&lt;/a&gt; would help.&lt;/p&gt;
&lt;h2 id="get-the-data"&gt;Get the data&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;tidyverse&amp;quot;)  # for everything ;)
library(tidyverse)

# install.packages(&amp;quot;BSDA&amp;quot;)       # for Fitness data
library(BSDA)

View(Fitness)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{BSDA} package provides a {Fitness} dataset, with the sit-up performance of 9 people &lt;strong&gt;before&lt;/strong&gt; and &lt;strong&gt;after&lt;/strong&gt; one week of training.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# make wide format
d &amp;lt;- Fitness %&amp;gt;% 
  pivot_wider(
    id_cols     = subject, 
    names_from  = test, 
    values_from = number) %&amp;gt;% 
  mutate(difference =  After - Before)

View(d)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see whether training makes any difference, we’ll &lt;strong&gt;calculate this difference&lt;/strong&gt; by subtracting performance &lt;strong&gt;Before&lt;/strong&gt; from performance &lt;strong&gt;After&lt;/strong&gt; and compare our &lt;strong&gt;mean difference&lt;/strong&gt; to zero. If training didn’t help, our &lt;strong&gt;mean difference&lt;/strong&gt; would be equal to zero, which becomes our Null Hypothesis (H&lt;sub&gt;0&lt;/sub&gt;). If training made a difference, our &lt;strong&gt;mean difference&lt;/strong&gt; would be lower or, hopefully, higher then zero, which becomes our alternative hypothesis (H&lt;sub&gt;Alt&lt;/sub&gt;). And since this difference is more important then paired samples themselves, we only need to check the normality of the difference, not of both samples. Checking normality is important for choosing a correct test, otherwise we could get wrong result. We’ll see what happens if we choose a wrong test in a moment. Until then…&lt;/p&gt;
&lt;h2 id="check-normality-of-the-difference"&gt;Check normality of the difference&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;shapiro.test(d$difference)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Shapiro-Wilk normality test

data:  d$difference
W = 0.95124, p-value = 0.7037&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… a big p-value of the Shapiro-Wilk normality test indicates that our difference is &lt;strong&gt;normally distributed&lt;/strong&gt;. That’s why we need a &lt;strong&gt;parametric paired t-test&lt;/strong&gt;, to compare two paired samples. If the difference would have been &lt;strong&gt;not-normally distributed&lt;/strong&gt;, we would have taken a &lt;strong&gt;non-parametric Wilcoxon test&lt;/strong&gt;, which you can learn about from &lt;a href="https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/"&gt;this article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;paired&lt;/strong&gt; tests, the data &lt;strong&gt;needs to be sorted&lt;/strong&gt;, so that the first observation of the &lt;strong&gt;before&lt;/strong&gt; group, pairs with the first observation of the &lt;strong&gt;after&lt;/strong&gt; group. If our data is sorter, we are ready to compute the test.&lt;/p&gt;
&lt;h2 id="compute-paired-samples-t-test"&gt;Compute Paired Samples t-Test&lt;/h2&gt;
&lt;p&gt;And the best way to compute our test (in my opinion) is the {ggwithinstats} function from {ggstatsplot} package, which needs only 4 arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, &lt;strong&gt;our data&lt;/strong&gt; - Fitness, then&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt; - as the grouping variable,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;y&lt;/strong&gt; - are the numbers of sit-ups and&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;type&lt;/strong&gt; of statistical approach. Since our data was normally distributed, we choose a &lt;strong&gt;parametric&lt;/strong&gt; test, and {ggwithinstats} automatically takes &lt;strong&gt;Paired t-Test&lt;/strong&gt; .&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Such simple command results in this statistically rich and publication ready plot! Now, let’s interpret the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(ggstatsplot)

set.seed(1)   # for Bayesian reproducibility
ggwithinstats(
  data = Fitness,
  x    = test, 
  y    = number, 
  type = &amp;quot;parametric&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf208f9686_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggsave(filename = &amp;quot;paired_t_test.jpg&amp;quot;, plot = last_plot(), width = 6, height = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="interpret-the-result"&gt;Interpret the result&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Students t-statistics&lt;/strong&gt; is the &lt;strong&gt;measure of similarity&lt;/strong&gt; between compared samples measured in units of standard error. The further &lt;em&gt;t-value&lt;/em&gt; is from zero, the more different are the samples. But &lt;strong&gt;t-value&lt;/strong&gt; by itself can not say how far from zero is far enough, to conclude that this difference is significant. That’s why we need a &lt;strong&gt;p-value&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[t = \frac{our.mean - expected.mean}{standart.error} = \frac{our.mean - expected.mean}{ \frac{standart.deviation} {\sqrt sample.size} }\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;our &lt;strong&gt;P-value&lt;/strong&gt; of 0.025 tells us that our difference IS significant. It shows a moderate evidence against the null hypothesis (H&lt;sub&gt;0&lt;/sub&gt;) in favor of the alternative hypothesis (H&lt;sub&gt;Alt&lt;/sub&gt;). Particularly, our group of 9 students will make 2 sit-ups more on average after one week of training. But is a difference of only 2 sit-ups large? P-value can not tell that. A P-value only tells you that there is an effect from training, but not how strong this effect is.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-01-22-pairedsamplesttestinr/p_value_interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fortunately, {ggwithinstats} provides &lt;strong&gt;Hedges’ g&lt;/strong&gt; as the measure of the &lt;strong&gt;Effect Size&lt;/strong&gt;, which shows how large the effect of training is. &lt;strong&gt;Hedges’ g&lt;/strong&gt; is interpreted in the same way as &lt;strong&gt;Cohen’s d&lt;/strong&gt; Effect size, but outperforms &lt;strong&gt;Cohen’s d&lt;/strong&gt; for small samples, like 9 in our example. Our effect size of 0.83 &lt;strong&gt;is large&lt;/strong&gt;, and tells us that the increase in performance of only 2 sit-ups is actually - a lot. But it doesn’t seem like a lot. So, we have to double check it!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-01-22-pairedsamplesttestinr/effect_size_hedges_d.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For that {ggwithinstats} provides the &lt;strong&gt;Bayesian Difference&lt;/strong&gt; between our samples with 95% Highest Density Intervals, which is conceptually similar to the difference of 2 sit-ups we see on the plot, and the &lt;strong&gt;Bayes Factor&lt;/strong&gt;, which is conceptually similar to the &lt;strong&gt;p-value&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Our &lt;strong&gt;Bayes Factor&lt;/strong&gt; (Jeffreys, 1961) of -1.13 indicates a &lt;strong&gt;substantial evidence for the alternative hypothesis&lt;/strong&gt; - that training actually did make a difference, and we now make substantially more sit-ups then &lt;strong&gt;Before&lt;/strong&gt; … which IS in line with the frequentists statistics on the top of the plot.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-01-22-pairedsamplesttestinr/bf_interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The {interpret_hedges_g} function from {effectsize} package helps to interpret this effect size and even provides the reference for interpretation if you ask R about it: &lt;code&gt;?interpret_hedges_g&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;effectsize&amp;quot;)
library(effectsize)

interpret_hedges_g(0.83)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;large&amp;quot;
(Rules: cohen1988)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;?interpret_hedges_g&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="what-happens-if-we-choose-a-wrong-test"&gt;What happens if we choose a wrong test?&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggwithinstats(
  data = Fitness,
  x    = test, 
  y    = number, 
  type = &amp;quot;nonparametric&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf208f9686_files/figure-html/unnamed-chunk-7-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggsave(filename = &amp;quot;paired_t_test.jpg&amp;quot;, plot = last_plot(), width = 6, height = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are two ways things can go wrong. First, if we are lazy to check the normality, we’ll go straight to the &lt;strong&gt;non-parametric Paired Wilcoxon-Test&lt;/strong&gt;. Here, a p-value almost failed to reject the Null Hypothesis when we use the threshold of 0.05 to determine significance. And the effect size is lower, which means our study would be underpowered. Let me show you what I mean.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;pwr&amp;quot;)
library(pwr)

pwr.t.test(n = 9, d = 0.83, sig.level = 0.05, type = &amp;quot;paired&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
     Paired t test power calculation 

              n = 9
              d = 0.83
      sig.level = 0.05
          power = 0.5897082
    alternative = two.sided

NOTE: n is number of *pairs*&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;pwr.t.test(n = 9, d = 0.81, sig.level = 0.05, type = &amp;quot;paired&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
     Paired t test power calculation 

              n = 9
              d = 0.81
      sig.level = 0.05
          power = 0.5693516
    alternative = two.sided

NOTE: n is number of *pairs*&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The effect size from Wilcoxon test has 2% lower power, which increases the chances to miss an important discovery. And in our example with a p-value close to the significance threshold we almost missed it.&lt;/p&gt;
&lt;p&gt;But that’s not all!!! We could be even more wrong if we took a &lt;strong&gt;NOT-PAIRED t-Test&lt;/strong&gt;, because we would get completely opposite result, namely, that the effect of training is small and not significant. This wrong result could even seem plausible, since the difference of only 2 sit-ups is sooo small.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggbetweenstats(
  data = Fitness,
  x    = test, 
  y    = number, 
  type = &amp;quot;parametric&amp;quot;, 
  var.equal = T
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf208f9686_files/figure-html/unnamed-chunk-9-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;interpret_hedges_g(0.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;small&amp;quot;
(Rules: cohen1988)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggsave(filename = &amp;quot;usual_t_test.jpg&amp;quot;, plot = last_plot(), width = 6, height = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, how can such a small difference be significant in a paired test then??? Well, it lies in the nature of the paired test - which does not compare averages of two samples, but compares a &lt;strong&gt;mean difference&lt;/strong&gt; between those samples to zero. And if we look at the difference, which shows individual performance of students, we’ll see that 7 out of 9 improved their performance, some of them by a lot, so that, the group in general was successful, and our significant p-value actually makes sense.&lt;/p&gt;
&lt;h2 id="proof-of-the-concept-about-the-difference-between-two-samples"&gt;Proof of the concept about the difference between two samples&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;gghistostats(data = d, x = difference, test.value = 0, binwidth = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf208f9686_files/figure-html/unnamed-chunk-10-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggsave(filename = &amp;quot;one_sample_t_test.jpg&amp;quot;, plot = last_plot(), width = 6, height = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly, if we test our mean difference against zero using &lt;strong&gt;one-sample t-Test&lt;/strong&gt; and compare the t-statistics, p-value and the effect size to our &lt;strong&gt;Paired t-Test&lt;/strong&gt;, we’ll see identical result!&lt;/p&gt;
&lt;p&gt;Thus, our fancy &lt;strong&gt;Paired Samples test&lt;/strong&gt; is actually &lt;strong&gt;One-Sample test&lt;/strong&gt; on the difference. And if you wanna know more about &lt;strong&gt;one-sample tests&lt;/strong&gt; and how to interpret all these numbers, check out &lt;a href="https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/"&gt;this article&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# old way to do the tests
# install.packages(&amp;quot;broom&amp;quot;)
library(broom)

bind_rows(
  t.test(d$difference) %&amp;gt;% tidy(),
  t.test(d$After, d$Before, paired = T) %&amp;gt;% tidy()
) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 × 8
  estimate statistic p.value parameter conf.low conf.high method      
     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       
1        2      2.75  0.0249         8    0.325      3.68 One Sample …
2        2      2.75  0.0249         8    0.325      3.68 Paired t-te…
# … with 1 more variable: alternative &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="one-tailed-or-one-sided-parametric-two-samples-paired-t-tests"&gt;One-Tailed (or One-Sided) Parametric Two-Samples Paired t-Tests&lt;/h2&gt;
&lt;p&gt;We can also ask the question, whether performance-change is positive or negative by performing &lt;strong&gt;One-Tailed (or One-Sided) Parametric Two-Samples Paired t-Tests&lt;/strong&gt;. (I hate that name! :)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t.test(d$After, d$Before, paired = TRUE, alternative = &amp;quot;less&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Paired t-test

data:  d$After and d$Before
t = 2.753, df = 8, p-value = 0.9875
alternative hypothesis: true difference in means is less than 0
95 percent confidence interval:
    -Inf 3.35093
sample estimates:
mean of the differences 
                      2 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;t.test(d$After, d$Before, paired = TRUE, alternative = &amp;quot;greater&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Paired t-test

data:  d$After and d$Before
t = 2.753, df = 8, p-value = 0.01247
alternative hypothesis: true difference in means is greater than 0
95 percent confidence interval:
 0.6490697       Inf
sample estimates:
mean of the differences 
                      2 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Low &lt;em&gt;p-value&lt;/em&gt; (p = 0.012) of the &lt;strong&gt;greater-sided&lt;/strong&gt; test confirms that the &lt;strong&gt;performance increases&lt;/strong&gt; after one week of training. The &lt;em&gt;p-value&lt;/em&gt; of the &lt;strong&gt;less-sided&lt;/strong&gt; test screams that your performance will not decrease with the probability of 99% (p = 0.99). In fact only one student (N = 6), which ironically was the best Before training week, decreased his/her performance by 2 sit-ups and was overtaken by two students After the week. That’s the price of arrogance :)&lt;/p&gt;
&lt;h2 id="whats-next-or-dont-use-paired-t-test-if"&gt;What’s next, or Don’t use Paired t-Test if&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;samples are independent. In this case apply&lt;a href="https://yury-zablotski.netlify.com/post/two-sample-t-test-compare-your-work-to-others/"&gt;Student’s or Welsh t-test&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;samples are small (n&amp;lt;30) and not-normally distributed. In this case use &lt;a href="https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/"&gt;paired two-samples Wilcoxon-test&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you need to compare more then two samples, first check whether they are normally distributed, and if they are, go to &lt;a href="https://yury-zablotski.netlify.com/post/one-way-anova/"&gt;ANOVA&lt;/a&gt;, but if they aren’t, go to &lt;em&gt;Kruskal Wallis&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>8248488a59521b38f56da6d1677ad6cc</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-01-22-pairedsamplesttestinr</guid>
      <pubDate>Fri, 11 Mar 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-01-22-pairedsamplesttestinr/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | McNemar Test | How to Conduct, Visualise and Interpret</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-02-20-mcnemar</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 5 minutes long. The “A bit of theory about McNemar” chapter below is also very useful and did not become part of the video for trying-not-to-confuse reasons.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/MxvrbohjxPM" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="get-the-data"&gt;Get the data&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(9) # for reproducibility 
data &amp;lt;- data.frame(
before = sample(c(&amp;quot;+&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;+&amp;quot;), 20, replace = TRUE),
after  = sample(c(&amp;quot;-&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;-&amp;quot;), 20, replace = TRUE))

View(data)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="compute-mcnemar-test"&gt;Compute McNemar Test&lt;/h2&gt;
&lt;p&gt;This one simple command is the {ggbarstats} function from {ggstatsplot} package, which needs only 5 arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, &lt;strong&gt;our data&lt;/strong&gt;, which shows 20 people who were tested for having alien parasite in their blood &lt;strong&gt;before&lt;/strong&gt; and &lt;strong&gt;after&lt;/strong&gt; taking a treatment drug. So,&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt; argument will be our results &lt;strong&gt;before&lt;/strong&gt; the treatment and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;y&lt;/strong&gt; will be the results &lt;strong&gt;after&lt;/strong&gt; the treatment. Then&lt;/li&gt;
&lt;li&gt;we have to tell {ggbarstats} that our samples are &lt;strong&gt;TRUE-ly paired&lt;/strong&gt; and&lt;/li&gt;
&lt;li&gt;lastly, I always use the &lt;strong&gt;label&lt;/strong&gt; argument to displays &lt;strong&gt;both - numbers and percentages&lt;/strong&gt; of observations in order to see what changed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Such simple command results in this statistically rich and publication ready plot! Now, let’s interpret the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(ggstatsplot)

ggbarstats(
  data = data,
  x    = before, 
  y    = after,
  paired = TRUE, 
  label = &amp;quot;both&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf6da0b450_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;If you want to save the picture, use the code below.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(tidyverse)

ggsave(filename = &amp;quot;mcnemar2.jpg&amp;quot;, plot = last_plot(), width = 5, height = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="interpret-the-result"&gt;Interpret the result&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;let’s first look at the &lt;strong&gt;numbers&lt;/strong&gt;: the 4 which were and remained negative and the 7 which were and remained positive, do not provide any useful information, because nothing has changed there. But since 8 people, which had an alien parasite before and became negative after taking a drug, while only 1 new person got infected, we can conclude that our treatment … kind of … worked. “Kind-off” because only statistical test can tell for sure. That’s why {ggbarstats} displays test results on the top of the plot. For instance…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;McNemars’s Chi-Squared-Statistics&lt;/strong&gt; was previously used to get p-values. But, since modern statistical software always deliver &lt;strong&gt;p-values&lt;/strong&gt; nobody cares about it anymore.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the &lt;strong&gt;p-value&lt;/strong&gt; helps to test the &lt;strong&gt;Null Hypothesis&lt;/strong&gt;, which assumes that the drug has no impact on disease, while the &lt;strong&gt;Alternative Hypothesis&lt;/strong&gt; assumes that the drug has an effect on disease.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-02-20-mcnemar/p_value_interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;and our &lt;strong&gt;P-value&lt;/strong&gt; of p = 0.02 shows a ❌ &lt;strong&gt;a moderate evidence against the null hypothesis (H&lt;sub&gt;0&lt;/sub&gt;)&lt;/strong&gt;, ✅ &lt;strong&gt;in favor of the alternative hypothesis (H&lt;sub&gt;Alt&lt;/sub&gt;)&lt;/strong&gt;, that our drug worked. However, a P-value only tells you that our drug has an effect, but not how strong this effect is.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fortunately, {ggbarstats} provides &lt;strong&gt;Cohen’s g&lt;/strong&gt; with 95% Confidence Intervals as the measure of the &lt;strong&gt;Effect Size&lt;/strong&gt; for McNemar test, which shows how big the effect of the drug is. The {interpret_cohens_g} function from the {effectsize} package reveals that our effect size of 0.39 is actually large.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;effectsize&amp;quot;)
library(effectsize)

interpret_cohens_g(0.39)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;large&amp;quot;
(Rules: cohen1988)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;?interpret_cohens_g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;So, the final interpretation of our McNemar test is: the ability of our drug to kill the alien parasite is significant and large.&lt;/strong&gt; Which is amazing, since we can stop aliens. However, if we took a wrong test the things could go really wrong. Let me show you what I mean.&lt;/p&gt;
&lt;h2 id="what-happens-if-we-choose-a-wrong-test"&gt;What happens if we choose a wrong test&lt;/h2&gt;
&lt;p&gt;First, {ggbarstats} delivers even more than you can see on this plot. Namely, it automatically &lt;strong&gt;removes the continuity correction&lt;/strong&gt; which was shown to be very conservative by several scientific papers. We can clearly see it by using {mcnemar.test} function with {correct = FALSE} argument and getting identical results. In contrast, if we don’t use this argument, {mcnemar.test} will &lt;strong&gt;apply the continuity correction&lt;/strong&gt; and produce unnecessary large p-value, which almost failed to reject the Null Hypothesis, so that we almost missed a discovery of a drug, which saves humanity from alien parasites.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mcnemar.test(data$before, data$after, correct = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    McNemar&amp;#39;s Chi-squared test

data:  data$before and data$after
McNemar&amp;#39;s chi-squared = 5.4444, df = 1, p-value = 0.01963&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;mcnemar.test(data$before, data$after)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    McNemar&amp;#39;s Chi-squared test with continuity correction

data:  data$before and data$after
McNemar&amp;#39;s chi-squared = 4, df = 1, p-value = 0.0455&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Secondly, the things can go even more wrong, if we take Chi-Square test by accidentally forgetting the {paired = TRUE} argument, or, &lt;strong&gt;God forbid&lt;/strong&gt;, would take Chi-Squared test for paired data on purpose. Because, Chi-Square test has a completely different Null Hypothesis and may produce opposite results. And that’s exactly what happens in our example, a huge p-value and a small effect size tell us that our drug is useless, which is dramatically wrong (Type II Error). So, using Chi-Squared test for paired data is like using a wrong drug against the alien parasite! And now, I hope you can not unknow it :)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggbarstats(
  data = data,
  x    = before, 
  y    = after,
  label = &amp;quot;both&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf6da0b450_files/figure-html/unnamed-chunk-7-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggsave(filename = &amp;quot;chisquared.jpg&amp;quot;, plot = last_plot(), width = 5, height = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But IF your samples are &lt;strong&gt;not-paired&lt;/strong&gt;, and you actually want to conduct either &lt;strong&gt;frequentists or bayesian Chi-Squared&lt;/strong&gt; test and want to know how to interpret all these results, check out &lt;a href="https://youtu.be/8Tj0-yMPO64"&gt;this video&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="a-bit-of-theory-about-mcnemar"&gt;A bit of theory about McNemar&lt;/h2&gt;
&lt;p&gt;McNemar’s is a non-parametric (distribution-free) test, which checks whether there is some change in proportions on a binomial outcome at &lt;strong&gt;two time points on the same population&lt;/strong&gt;. It is applied &lt;strong&gt;only to 2×2 contingency table&lt;/strong&gt;. In medicine, if a researcher wants to determine whether or not a particular drug has an effect on a disease, then a count of the individuals is recorded (as + and – sign, or 0 and 1) in a table before and after being given the drug. Then, &lt;strong&gt;McNemar’s test is applied to make statistical decisions as to whether or not a drug has an effect on the disease&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In a 2×2 table there would be &lt;strong&gt;two cells with concordant results&lt;/strong&gt; (the frequency of individuals which responded positively or negatively to both stimuli or on both time points) and &lt;strong&gt;two cells with discordant results&lt;/strong&gt; (the frequency of individuals who responded differently).&lt;/p&gt;
&lt;p&gt;The key question McNemar’s test asks if &lt;strong&gt;whether the difference between the values of the two discordant cells is significantly high&lt;/strong&gt; based on the &lt;strong&gt;Null Hypothesis that both types of discord are equally likely&lt;/strong&gt;. And interestingly enough, the paired McNemar test is exactly the same as &lt;strong&gt;One-Sample&lt;/strong&gt; Chi-Squared Test goodness-of-fit test on two (discordant) numbers. Let me prove it to you:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;chisq.test(c(1,8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Chi-squared test for given probabilities

data:  c(1, 8)
X-squared = 5.4444, df = 1, p-value = 0.01963&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See, the Test-Statistics and the p-value are completely identical! That’s fascinating, because it’s similar to the paired t- or wilcoxon tests, which are also one-sample tests in their nature. The fact is very important, but it would probably confuse the viewers, since the pace of the video is quick and would mix-up two ideas: 1) the McNemar test = &lt;strong&gt;One-Sample&lt;/strong&gt; Chi-Square test and 2) the &lt;strong&gt;Two-Samples&lt;/strong&gt; Chi-Squared test is wrong. That’s why it did not became a part of the video.&lt;/p&gt;
&lt;p&gt;Finally, there are four versions of the McNemar test (classical, continuity corrected, exact, and mid-P). The traditional advice is to use the classical test in large samples and the exact test in small samples. The argument for using the exact test is that the classical test may violate the nominal significance level for small sample sizes because the required asymptotic do not hold. One disadvantage with the exact and continuity corrected tests is conservatism: they produce unnecessary large p-values and have poor power.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;MESS::power_mcnemar_test(n=30, paid=.05, psi=2, method = &amp;quot;exact&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
     McNemar paired comparison of proportions exact unconditional power calculation 

              n = 30
           paid = 0.05
            psi = 2
      sig.level = 0.05
          power = 0.05462106
    alternative = two.sided

NOTE: n is number of pairs&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;MESS::power_mcnemar_test(n=30, paid=.05, psi=2, method = &amp;quot;normal&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
     McNemar paired comparison of proportions approximate power calculation 

              n = 30
           paid = 0.05
            psi = 2
      sig.level = 0.05
          power = 0.1032173
    alternative = two.sided

NOTE: n is number of pairs&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="the-best-mcnemar-test-is-mid-p"&gt;The best McNemar test is “mid-P” ;)&lt;/h2&gt;
&lt;p&gt;The mid-P version is not directly available in any R-package but can be obtained relatively simply using the recipe below, where &lt;em&gt;b&lt;/em&gt; and &lt;em&gt;c&lt;/em&gt; are the values in the two discordant cells of the contingency table, and where &lt;em&gt;b &amp;lt; c&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[2* pbinom(b, (b+c), lower.tail = T) - dbinom(b, (b+c), 0.5)\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# cross_table
2* pbinom(1, 8, 0.5) - dbinom(1, 8, 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.0390625&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2 * pbinom(3, 18, 0.5) - dbinom(3, 18, 0.5)&lt;/p&gt;
&lt;p&gt;We can get a very similar p-value when we simulate it in a One-Sample Chi-Square test, which would be more realistic (see below) as compared to a not-simulated one, but most of the scientists who want to compare two paired categorical samples would not bother about it (no time). So, if scientist use the classical McNemar test for paired data instead of Two-Samples Chi-Square, I would already be fulfilled. Because the classical (asymptotic) McNemar test (without CC!) performs surprisingly well, even for quite small sample sizes. It often violates the nominal significance level, but not by much. Thus, my final recommendation here is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use ggbarstat with argument {paired = TRUE} or {mcnemar.test(data&lt;span class="math inline"&gt;\(before, data\)&lt;/span&gt;after, correct = F)} with no bad feeling, but if you are a perfectionist and have time,&lt;/li&gt;
&lt;li&gt;calculate mid-P or simulate p-value in a one-sample Chi-Square test on two disconcordant numbers from cross table&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(2); chisq.test(c(1,8), simulate.p.value = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Chi-squared test for given probabilities with simulated
    p-value (based on 2000 replicates)

data:  c(1, 8)
X-squared = 5.4444, df = NA, p-value = 0.03948&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="whats-the-next-best-thing-to-learn"&gt;What’s the next best thing to learn?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cochran’s Q Test + Pairwise McNemar Tests (post-hoc)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="reference"&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pembury Smith, M.Q.R., Ruxton, G.D. Effective use of the McNemar test. Behav Ecol Sociobiol 74, 133 (2020). &lt;a href="https://doi.org/10.1007/s00265-020-02916-y" class="uri"&gt;https://doi.org/10.1007/s00265-020-02916-y&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fagerland, Morten &amp;amp; Lydersen, Stian &amp;amp; Laake, Petter. (2013). The McNemar test for binary matched-pairs data: Mid-p and asymptotic are better than exact conditional. BMC medical research methodology. 13. 91. 10.1186/1471-2288-13-91.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>548ea163354fc595f3c6773b8ff0f6c5</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-02-20-mcnemar</guid>
      <pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-02-20-mcnemar/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Friedman Test | How to Conduct, Visualise and Interpret</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-02-08-friedman</link>
      <description>


&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-02-08-friedman/featured.jpg" /&gt;&lt;/p&gt;
&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 5 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/XSUKSE0cAJs" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://yuzar-blog.netlify.app/posts/2022-01-30-rmanova/"&gt;Repeated Measures ANOVA&lt;/a&gt; and &lt;a href="https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/"&gt;Paired Wilcoxon Signed-Rank test&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="why-do-we-need-friedman-test"&gt;Why do we need Friedman test&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Friedman Test&lt;/strong&gt; is a non-parametric brother of &lt;strong&gt;Repeated Measures ANOVA&lt;/strong&gt;, which does much better job when data is not-normally distributed (which happens pretty often ;). Friedman test is also superior to &lt;strong&gt;Repeated Measures ANOVA&lt;/strong&gt; when our data is ordinal (e.g., scales from 1 to 10). &lt;strong&gt;Friedman Test&lt;/strong&gt; can also be a non-parametric father of the Paired Wilcoxon test, because it can compare more then two groups. Having a lot of groups creates tons of work though, because all the groups need to be compared to each other pairwisely if Friedman test is significant.&lt;/p&gt;
&lt;p&gt;However, one simple command conducts and visualizes &lt;strong&gt;Friedman Test&lt;/strong&gt; you see on the screen, automatically conducts and displays pairwise tests and even corrects the p-values for multiple comparisons. So, let’s get the data and learn how to easily get all these results.&lt;/p&gt;
&lt;h2 id="how-friedman-test-works"&gt;How Friedman Test works&lt;/h2&gt;
&lt;p&gt;A good description about how Friedman test works and the picture below come from this &lt;a href="https://www.spss-tutorials.com/spss-friedman-test-simple-example/"&gt;blog-article&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-02-08-friedman/friedman-test-how-it-works.png" /&gt;&lt;/p&gt;
&lt;h2 id="get-the-data"&gt;Get the data&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;tidyverse&amp;quot;)  # for everything ;)
library(tidyverse)

# install.packages(&amp;quot;datarium&amp;quot;)   # for marketing data
library(datarium)

View(marketing)

d &amp;lt;- marketing %&amp;gt;%
  select(youtube, facebook, newspaper) %&amp;gt;% 
  rowid_to_column() %&amp;gt;% 
  gather(key = &amp;quot;channel&amp;quot;, value = &amp;quot;money&amp;quot;, youtube:newspaper) %&amp;gt;% 
  group_by(channel) %&amp;gt;% 
  slice(20:35) # looks better 

View(d)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we’ll load {datarium} package and&lt;/li&gt;
&lt;li&gt;take {marketing} data from it, which shows money spend on three advertising channels: youtube, facebook and newspaper&lt;/li&gt;
&lt;li&gt;we then gather all three channels into one column, so that our channels become a variable for the x-axis of the plot, and our spending in thousands of dollars becomes a variable for the y-axis of the plot,&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;finally, we’ll reduce the dataset a bit to make the plot look better&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For &lt;strong&gt;repeated measures&lt;/strong&gt;, the data &lt;strong&gt;needs to be sorted&lt;/strong&gt; so that, the first observation of the &lt;strong&gt;first channel&lt;/strong&gt;, pairs with the first observation of other &lt;strong&gt;channels&lt;/strong&gt;. If our data is sorter, we are ready to compute the test.&lt;/p&gt;
&lt;h2 id="compute-friedman-test"&gt;Compute Friedman Test&lt;/h2&gt;
&lt;p&gt;And the best way to compute Friedman Test (in my opinion) is the {ggwithinstats} function from {ggstatsplot} package, which needs only 4 arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, &lt;strong&gt;our data&lt;/strong&gt;, which we just prepared, then&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt; - as the grouping variable - &lt;strong&gt;channel&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;y&lt;/strong&gt; - is the numeric variable &lt;strong&gt;money&lt;/strong&gt; and&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;nonparametric type&lt;/strong&gt; of statistical approach, which tells {ggwithinstats} to conduct &lt;strong&gt;Friedman Test&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Such simple command results in this statistically rich and publication ready plot! Now, let’s interpret the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(ggstatsplot)

ggwithinstats(
  data = d,
  x    = channel, 
  y    = money, 
  type = &amp;quot;nonparametric&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf150c556d_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggsave(filename = &amp;quot;friedman.jpg&amp;quot;, plot = last_plot(), width = 5.5, height = 5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="interpret-the-result"&gt;Interpret the result&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Friedman’s Chi-Squared-Statistics&lt;/strong&gt; was previously used to get p-values. But, since modern statistical software always report &lt;strong&gt;p-values&lt;/strong&gt;, we can safely ignore it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the &lt;strong&gt;p-value&lt;/strong&gt; helps to test the &lt;strong&gt;Null Hypothesis&lt;/strong&gt;, which says that channels get similar amount of money, while the &lt;strong&gt;Alternative Hypothesis&lt;/strong&gt; says that channels get different amount of money.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-02-08-friedman/p_value_interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;our &lt;strong&gt;very low P-value&lt;/strong&gt; (p = 0.0000376) shows a ❌ &lt;strong&gt;very strong evidence against the null hypothesis (H&lt;sub&gt;0&lt;/sub&gt;)&lt;/strong&gt;, ✅ &lt;strong&gt;in favor of the alternative hypothesis (H&lt;sub&gt;Alt&lt;/sub&gt;)&lt;/strong&gt;, that difference exists. However, a P-value only tells you that there is a difference, but not how large this difference is.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fortunately, {ggwithinstats} provides &lt;strong&gt;Kendall’s coefficient of concordance&lt;/strong&gt; with 95% Confidence Intervals as the measure of the &lt;strong&gt;Effect Size&lt;/strong&gt; for &lt;strong&gt;Friedman test&lt;/strong&gt;. The {interpret_kendalls_w} function from {effectsize} package helps to interpret this effect size and even provides the reference for interpretation. Our effect size of 0.64 is substantial. (Please, ignore the word “agreement,” it’s not important there)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;effectsize&amp;quot;)
library(effectsize)

interpret_kendalls_w(0.64)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;substantial agreement&amp;quot;
(Rules: landis1977)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;So, the final interpretation of our Friedman test is: &lt;strong&gt;the amount of money spend on channels differs significantly and this difference is substantial&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nice, right? However, our &lt;strong&gt;Global Friedman Test&lt;/strong&gt; doesn’t say &lt;strong&gt;which channels exactly do differ???&lt;/strong&gt;. And that’s where pairwise comparisons come into play. But wait, which tests should we take?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Well, fortunately, {ggwithinstats}:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;knows that we need Durbin-Conover pairwise tests after significant Friedman test&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;moreover, it &lt;strong&gt;automatically conducts those tests and displays p-values&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;and finally, it &lt;strong&gt;even corrects p-values for multiple comparisons without any additional code&lt;/strong&gt;. I think it’s just amazing!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is a quick proof that {ggwithinstats} indeed uses paired Durbin-Conover tests.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;PMCMRplus&amp;quot;)
library(PMCMRplus)
durbinAllPairsTest(
  y      = d$money, 
  groups = d$channel, 
  blocks = d$rowid,
  p.adjust.method = &amp;quot;holm&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          facebook newspaper
newspaper 0.03     -        
youtube   2.0e-07  7.6e-05  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;by the way, our global Friedman test is often called with a strange name - &lt;strong&gt;omnibus test&lt;/strong&gt;, while the pairwise tests between channels, are sometimes described in a dead &lt;em&gt;latin&lt;/em&gt; language as - &lt;strong&gt;post-hoc&lt;/strong&gt; - which simply means - &lt;strong&gt;after the event&lt;/strong&gt;. I really think those unnecessary names make statistics more complicated then it is.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="customise-the-result"&gt;Customise the result&lt;/h2&gt;
&lt;p&gt;However, if we want to, we can easily customize our plot by using either additional arguments within the function, or arguments from {ggplot2} package outside of it. For example,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;we can easily change the p-values adjustment method from the default &lt;strong&gt;Holm-correction&lt;/strong&gt;, to a more famous &lt;strong&gt;Bonferroni correction for multiple comparisons&lt;/strong&gt; … but I wouldn’t recommend it, because &lt;strong&gt;Bonferroni correction is too conservative&lt;/strong&gt; and we could miss a discovery. And that’s exactly what happens in our example, namely significant difference between money spend on facebook and newspaper, discovered by the Holm method, disappears when we use Bonferroni. So, no discovery - no Nobel Price for me 😭.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;then, if you want to display &lt;strong&gt;not only significant&lt;/strong&gt;, but &lt;strong&gt;all comparisons&lt;/strong&gt;, you can use {pairwise.display = “all”} argument and see a p-value of 0.09 between facebook and newspaper, which was over-corrected by Bonferroni.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if you want to hide the &lt;strong&gt;pairwise comparisons&lt;/strong&gt; or &lt;strong&gt;statistical results&lt;/strong&gt;, or both…&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;you can easily do that and much more. Just ask R about {?ggwithinstats} function and try some things out, you’ll enjoy it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;But the coolest thing about {ggwithinstats} is that if you want to compare only two groups, you just need to change the variable channel, which has three groups in our data, to a variable which has only two groups, and {ggwithinstats} will automatically conduct Paired Wilcoxon test which you can learn all about from &lt;a href="https://youtu.be/YRlIkNKazF8"&gt;this video&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# customise the result
ggwithinstats(
  data = d,
  x    = channel, 
  y    = money, 
  type = &amp;quot;nonparametric&amp;quot;,
  p.adjust.method = &amp;quot;bonferroni&amp;quot;, 
  # pairwise.display = &amp;quot;all&amp;quot;,
  # pairwise.comparisons = FALSE,   
  # results.subtitle = F
) + 
  ylab(&amp;quot;money spend [thousands of US dollars]&amp;quot;)+
  theme_classic()+
  theme(legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf150c556d_files/figure-html/unnamed-chunk-6-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;?ggwithinstats&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="how-to-report-the-results"&gt;How to report the results&lt;/h2&gt;
&lt;p&gt;Report both, the mean ranks and the medians for each group. Besides, report the test statistic &lt;span class="math inline"&gt;\(\chi^2\)&lt;/span&gt; value (“Chi-square”), degrees of freedom (“df”) and the &lt;strong&gt;p-value&lt;/strong&gt; which show whether the difference between the mean ranks is significant. &lt;strong&gt;How to write&lt;/strong&gt;: There was a statistically significant difference in the amount of money spend on channels &lt;span class="math inline"&gt;\({\chi^2}_{Friedman}\)&lt;/span&gt;(df=2) = 20.38, p &amp;lt; 0.001. The effect size &lt;span class="math inline"&gt;\(W_{Kendall}\)&lt;/span&gt; = 0.64 with 95% CI [0.39-1] turned out to be substantial. &lt;strong&gt;Post-hoc&lt;/strong&gt; analysis with Durbin-Conover tests was conducted with a Bonferroni correction for multiple comparisons. Median (IQR) money spend were 20.16 (12.9 to 29.6), 27.8 (22.7 to 47.0) and 291 (116 to 291) for facebook, newspaper and youtube respectively. There was a suggestive differences between facebook and newspaper (p = 0.09). However, there was a statistically significant increase in money spending in youtube vs. newpaper (p &amp;lt; 0.001) and in youtube vs. facebook (p &amp;lt; 0.001).&lt;/p&gt;
&lt;h2 id="how-to-check-the-normality-assumption-to-make-sure-you-need-friedman-test"&gt;How to check the normality assumption to make sure you need Friedman test&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;# hard way
hard &amp;lt;- afex::aov_ez(
  data   = d,
  id     = &amp;quot;rowid&amp;quot;, 
  dv     = &amp;quot;money&amp;quot;,  
  within = &amp;quot;channel&amp;quot;)

residuals(hard) %&amp;gt;% shapiro.test()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Shapiro-Wilk normality test

data:  .
W = 0.91729, p-value = 0.002389&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="whats-next"&gt;What’s next&lt;/h2&gt;
&lt;p&gt;Friedman Test is actually not-very flexible, but very capricious and cranky. It does not like missing values or not exactly the same number of repeated measures for all individuals - &lt;strong&gt;imbalanced design&lt;/strong&gt;, it often overfits and it is almost impossible to add more then one predictor. Thus, the solution for almost all of these problems and the most logical next step in your learning journey are - &lt;a href="https://yury-zablotski.netlify.app/post/mixed-effects-models-1/"&gt;Mixed Effects Models&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>7be7324a6539e00a166d9c5e4a4f0e9e</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-02-08-friedman</guid>
      <pubDate>Sun, 20 Feb 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-02-08-friedman/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Repeated Measures ANOVA (One-Way) | How to Conduct, Visualise and Interpret</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-01-30-rmanova</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 7 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/CL7WlwKz5aw" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://yuzar-blog.netlify.app/posts/2022-01-22-pairedsamplesttestinr/"&gt;Paired t-Test&lt;/a&gt; and &lt;a href="https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/"&gt;Paired Wilcoxon test&lt;/a&gt; would help.&lt;/p&gt;
&lt;h2 id="get-the-data"&gt;Get the data&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;tidyverse&amp;quot;)  # for everything ;)
library(tidyverse)

# install.packages(&amp;quot;datarium&amp;quot;)   # for selfesteem data
library(datarium)

View(selfesteem)&lt;/code&gt;&lt;/pre&gt;
&lt;template id="e4716cc1-b0f9-44b3-b66f-c9f9657c0052"&gt;&lt;style&gt;
.tabwid table{
  border-spacing:0px !important;
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
&lt;/style&gt;&lt;div class="tabwid"&gt;&lt;style&gt;.cl-500a0ddc{}.cl-500027ea{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-50004202{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-50007f92{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-50007fa6{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-50007fa7{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-50007fa8{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-50007fb0{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-50007fb1{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-50007fba{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-50007fbb{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-50007fc4{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-50007fc5{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-50007fc6{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-50007fce{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}&lt;/style&gt;&lt;table class='cl-500a0ddc'&gt;
&lt;thead&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-50007fc6"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;id&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fce"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;t1&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fce"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;t2&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fce"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;t3&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-50007fa6"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;1&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;4.01&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;5.18&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;7.11&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-50007fb1"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;2&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fb0"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;2.56&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fb0"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;6.91&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fb0"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;6.31&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-50007fa6"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;3&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;3.24&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;4.44&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;9.78&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-50007fa6"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;4&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;3.42&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;4.71&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;8.35&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-50007fb1"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;5&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fb0"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;2.87&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fb0"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;3.91&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fb0"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;6.46&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-50007fbb"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;6&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fba"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;2.05&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fba"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;5.34&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fba"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;6.65&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-50007fc5"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;7&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fc4"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;3.53&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fc4"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;5.58&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fc4"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;6.84&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-50007fb1"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;8&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fb0"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;3.18&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fb0"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;4.37&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fb0"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;7.82&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-50007fa6"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;9&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;3.51&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;4.40&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007f92"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;8.47&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-50007fa8"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;10&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fa7"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;3.04&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fa7"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;4.49&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-50007fa7"&gt;&lt;p class="cl-50004202"&gt;&lt;span class="cl-500027ea"&gt;8.58&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;/template&gt;
&lt;div class="flextable-shadow-host" id="b00a0d5c-f8d0-440c-a6c3-997e882782bc"&gt;&lt;/div&gt;
&lt;script&gt;
var dest = document.getElementById("b00a0d5c-f8d0-440c-a6c3-997e882782bc");
var template = document.getElementById("e4716cc1-b0f9-44b3-b66f-c9f9657c0052");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
&lt;/script&gt;

&lt;p&gt;For that let’s take {selfesteem} data from {datarium} package, and gather all three time-points into one column, so that our timepoints become a variable for the x-axis of the plot, and our selfesteem scores become a variable for the y-axis of the plot. For &lt;strong&gt;repeated measures&lt;/strong&gt;, the data &lt;strong&gt;needs to be sorted&lt;/strong&gt; so that, the first observation of the &lt;strong&gt;first time point&lt;/strong&gt;, pairs with the first observation of other &lt;strong&gt;time points&lt;/strong&gt;. If our data is sorter, we are ready to compute the test.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# make long format
d &amp;lt;- selfesteem %&amp;gt;%
  gather(key = &amp;quot;time&amp;quot;, value = &amp;quot;score&amp;quot;, t1, t2, t3) 

View(d)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="compute-one-way-repeated-measures-anova"&gt;Compute One-Way Repeated Measures ANOVA&lt;/h2&gt;
&lt;p&gt;And the best way to compute Repeated Measures ANOVA (in my opinion) is the {ggwithinstats} function from {ggstatsplot} package, which needs only 4 arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, &lt;strong&gt;our data&lt;/strong&gt;, which is &lt;strong&gt;d&lt;/strong&gt;, then&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt; - as grouping variable - &lt;strong&gt;time&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;y&lt;/strong&gt; - will be the &lt;strong&gt;scores of selfesteem&lt;/strong&gt; and we choose&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;parametric type&lt;/strong&gt; of statistical approach, which tells {ggwithinstats} to conduct &lt;strong&gt;Repeated Measures ANOVA&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Such simple command results in this statistically rich and publication ready plot! Now, let’s interpret the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(ggstatsplot)

set.seed(1)   # for Bayesian reproducibility
ggwithinstats(
  data = d,
  x    = time, 
  y    = score, 
  type = &amp;quot;parametric&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf1cd6e65d_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggsave(filename = &amp;quot;rm_anova.jpg&amp;quot;, plot = last_plot(), width = 5.5, height = 5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="interpret-the-result"&gt;Interpret the result&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;F-statistics&lt;/strong&gt; and &lt;strong&gt;degrees of freedom (DFs)&lt;/strong&gt; were previously used to get p-values. But, since modern statistical software always report &lt;strong&gt;p-values&lt;/strong&gt;, we can safely ignore them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the &lt;strong&gt;p-value&lt;/strong&gt; helps to test the hypothesis, where &lt;strong&gt;Null Hypothesis&lt;/strong&gt; says that sample-means are similar, or, to be more exact, that the differences between pairwise samples are equal to Zero, while the &lt;strong&gt;Alternative Hypothesis&lt;/strong&gt; says that sample-means differ, or, in other words, that differences between pairwise samples are not-equal to Zero.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;diffs &amp;lt;- selfesteem %&amp;gt;%
  mutate(
    diff_t3_t1 = t3 - t1,
    diff_t3_t2 = t3 - t2,
    diff_t2_t1 = t2 - t1 )

View(diffs)&lt;/code&gt;&lt;/pre&gt;
&lt;template id="cabe04d9-8adb-4adb-a519-fabb1d05285b"&gt;&lt;style&gt;
.tabwid table{
  border-spacing:0px !important;
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
&lt;/style&gt;&lt;div class="tabwid"&gt;&lt;style&gt;.cl-574017d6{}.cl-5738d994{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5738e8d0{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-57392700{width:65.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5739270a{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-57392714{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5739271e{width:65.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5739271f{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-57392728{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-57392729{width:65.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5739272a{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-57392732{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-57392733{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5739273c{width:65.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5739273d{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5739273e{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-57392746{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-57392747{width:65.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}&lt;/style&gt;&lt;table class='cl-574017d6'&gt;
&lt;thead&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5739273e"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;id&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392746"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;t1&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392746"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;t2&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392746"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;t3&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392747"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;diff_t3_t1&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392747"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;diff_t3_t2&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392747"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;diff_t2_t1&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5739270a"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;1&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.01&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;5.18&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;7.11&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.10&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;1.93&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;1.18&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5739272a"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;2&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;2.56&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;6.91&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;6.31&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.75&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;-0.60&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.35&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5739270a"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.24&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.44&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;9.78&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;6.53&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;5.33&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;1.20&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5739270a"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.42&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.71&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;8.35&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.93&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.64&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;1.29&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5739272a"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;5&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;2.87&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.91&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;6.46&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.59&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;2.55&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;1.04&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5739270a"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;6&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;2.05&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;5.34&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392714"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;6.65&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.61&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;1.31&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392700"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.29&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5739273d"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;7&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392733"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.53&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392733"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;5.58&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392733"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;6.84&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5739273c"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.31&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5739273c"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;1.26&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5739273c"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;2.05&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5739272a"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;8&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.18&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.37&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;7.82&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.64&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.45&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;1.19&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5739272a"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;9&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.51&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.40&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392732"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;8.47&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.96&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.07&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-57392729"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;0.89&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-57392728"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;10&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5739271f"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;3.04&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5739271f"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.49&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5739271f"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;8.58&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5739271e"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;5.54&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5739271e"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;4.09&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5739271e"&gt;&lt;p class="cl-5738e8d0"&gt;&lt;span class="cl-5738d994"&gt;1.45&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;/template&gt;
&lt;div class="flextable-shadow-host" id="47ce5733-2746-45a4-b3fd-9116fb8c591c"&gt;&lt;/div&gt;
&lt;script&gt;
var dest = document.getElementById("47ce5733-2746-45a4-b3fd-9116fb8c591c");
var template = document.getElementById("cabe04d9-8adb-4adb-a519-fabb1d05285b");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
&lt;/script&gt;

&lt;ul&gt;
&lt;li&gt;our &lt;strong&gt;very low P-value&lt;/strong&gt; (p = 0.00000216) shows a &lt;strong&gt;very strong evidence against the null hypothesis (H&lt;sub&gt;0&lt;/sub&gt;)&lt;/strong&gt;, &lt;strong&gt;in favor of the alternative hypothesis (H&lt;sub&gt;Alt&lt;/sub&gt;)&lt;/strong&gt;, that sample-means are different. That tells us that exercise significantly increases selfesteem over time. Which is good to know! But how strong is the effect of sports on selfesteem? P-value can not tell that. A P-value only tells you that there is an effect from training, but not how strong this effect is.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-01-30-rmanova/p_value_interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fortunately, {ggwithinstats} provides &lt;strong&gt;partian omega squared&lt;/strong&gt; with 95% Confidence Intervals as the measure of the &lt;strong&gt;Effect Size&lt;/strong&gt; for Repeated Measures ANOVA, which shows that training effect of 0.81 is large.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-01-30-rmanova/interpret_omega_squared.png" style="width:50.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Moreover, {ggwithinstats} also provides a &lt;strong&gt;Bayesian Effect Size&lt;/strong&gt;, namely the &lt;strong&gt;coefficient of determination&lt;/strong&gt; - &lt;span class="math inline"&gt;\(R_2\)&lt;/span&gt; with 95% Highest Density Intervals. &lt;span class="math inline"&gt;\(R_2\)&lt;/span&gt; shows the explanatory power of our model and since &lt;span class="math inline"&gt;\(R_2\)&lt;/span&gt; goes from 0 to 100%, the explanatory power of 82% in our model is huge, or, if we interpret &lt;span class="math inline"&gt;\(R_2\)&lt;/span&gt; as the effect size, our effects is substantial.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-01-30-rmanova/interpret_r_squared.png" style="width:50.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If that’s not enough, we can have a look at the &lt;strong&gt;Bayes Factor&lt;/strong&gt;, which is conceptually similar to the &lt;strong&gt;p-value&lt;/strong&gt;. Our &lt;strong&gt;Bayes Factor&lt;/strong&gt; of -20 indicates a &lt;strong&gt;Decisive evidence for the alternative hypothesis&lt;/strong&gt; - that training does increase our selfesteem … which IS in line with the frequentists statistics on the top of the plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-01-30-rmanova/bf_interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;now, both, &lt;strong&gt;Bayes Factor&lt;/strong&gt; and &lt;strong&gt;p-value&lt;/strong&gt; tell us that difference among time-points exists, however, they don’t show between which time-points exactly. That’s why we need to compare every timepoint to every other timepoint pairwisely.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;by the way, our two global tests are often called with a strange name - &lt;strong&gt;omnibus test&lt;/strong&gt;, while the pairwise tests between time-points, are sometimes described in a dead latin language as - &lt;strong&gt;post-hoc&lt;/strong&gt; - which means - &lt;strong&gt;after the event&lt;/strong&gt; - in plain English. I really think those unnecessary names make statistics more complicated then it is.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Anyway, {ggwithinstats} &lt;strong&gt;automatically knows that we need Paired t-Tests for Repeated Measures ANOVA, automatically conducts those tests and displays p-values and even corrects p-values for multiple comparisons without any additional code&lt;/strong&gt;. How cool is that!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is a quick proof that {ggwithinstats} indeed uses paired t-test.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pairwise.t.test(d$score, d$time,
                paired=T, 
                p.adjust.method = &amp;quot;holm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Pairwise comparisons using paired t tests 

data:  d$score and d$time 

   t1     t2    
t2 0.0015 -     
t3 1e-06  0.0015

P value adjustment method: holm &lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="customise-the-result"&gt;Customise the result&lt;/h2&gt;
&lt;p&gt;However, if we want to, we can easily customize our plot by using either additional arguments within the function, or arguments from {ggplot2} package outside of it. For example,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if you found outliers in your data, you can display them on the plot and&lt;/li&gt;
&lt;li&gt;use a &lt;strong&gt;robust ANOVA&lt;/strong&gt; to minimize the effect of outliers,&lt;/li&gt;
&lt;li&gt;here again, the function automatically uses correct pairwise tests for every omnibus test and corrects p-values for multiple comparisons with a Holm method,&lt;/li&gt;
&lt;li&gt;which you can easily change to a more famous &lt;strong&gt;Bonferroni correction for multiple comparisons&lt;/strong&gt; … but I wouldn’t recommend it, because Bonferroni correction is too conservative and we could miss some interesting result&lt;/li&gt;
&lt;li&gt;then, if you want to display &lt;strong&gt;not only significant&lt;/strong&gt;, but &lt;strong&gt;all comparisons&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;if you want to hide either Frequentists or Bayesian statistics, or both…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;you can easily do that and much more. Just ask R about {?ggwithinstats} function and try some things out, I am sure you’ll enjoy it.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggwithinstats(
  data = d,
  x    = time, 
  y    = score, 
  outlier.tagging = T,
  type = &amp;quot;robust&amp;quot;, 
  p.adjust.method = &amp;quot;bonferroni&amp;quot;, 
  pairwise.display = &amp;quot;all&amp;quot;,
  # pairwise.comparisons = FALSE,   
  results.subtitle = F,
  bf.message = F
) + 
  ylab(&amp;quot;selfesteem score&amp;quot;)+
  theme_classic()+
  theme(legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf1cd6e65d_files/figure-html/unnamed-chunk-9-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;?ggwithinstats&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="check-sphericity-normality-assumptions"&gt;Check Sphericity &amp;amp; Normality assumptions&lt;/h2&gt;
&lt;p&gt;Now, the last thing is important - please check both the Sphericity &amp;amp; Normality assumptions, otherwise you’ll choose a wrong test and either miss a discovery having a wrong big p-value (also called - Type II Error) or you’ll find some nonsense having a wrong small p-value (also called - Type I Error).&lt;/p&gt;
&lt;p&gt;Repeated Measures ANOVA needs Sphericity, where Sphericity simply means that data-spread inside of the groups is similar. To say it a more correct but more boring way, - Sphericity is a condition where variances of the differences between all combinations of related groups don’t differ. You know that &lt;strong&gt;Variance&lt;/strong&gt; IS IMPORTANT, because the name of our test - ANOVA - is actually the abbreviation of the &lt;strong&gt;AN&lt;/strong&gt;alises &lt;strong&gt;O&lt;/strong&gt;f &lt;strong&gt;VA&lt;/strong&gt;riances. However, since most of the real world data have different variances, {ggwithinstats} already accounts for Sphericity by default. You can still check Sphericity assumption by yourself and I’ll show you how in a moment, but for now let’s talk about normality…&lt;/p&gt;
&lt;p&gt;ANOVA also needs the data to be normally distributed, or bell shaped, but often compares &lt;strong&gt;a lot of groups&lt;/strong&gt;, so that checking normality of separate groups for usual ANOVA or differences between those groups for Repeated Measures ANOVA, might become cumbersome.&lt;/p&gt;
&lt;p&gt;The {aov_ez} function from {avex} package allows to easily check both assumptions. First, it conducts &lt;strong&gt;Mauchly Test for Sphericity&lt;/strong&gt; and &lt;strong&gt;automatically corrects p-values&lt;/strong&gt; of omnibus test. So that, if Sphericity was violated, you’ll take the corrected p-value. Secondly, instead of checking the normality of thousands of groups, we can &lt;strong&gt;check the normality of the residuals&lt;/strong&gt; of our ANOVA model, where all groups are already included. We then decide whether we stay with the &lt;strong&gt;Parametric Repeated Measures ANOVA&lt;/strong&gt; if residuals are normally distributed, like in our example, or go to the &lt;strong&gt;Nonparametric Friedman test&lt;/strong&gt; if residuals are not-normally distributed, which is a completely different story.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# old hard way
# install.packages(afex)

library(afex)
hard &amp;lt;- aov_ez(
  data   = d,
  id     = &amp;quot;id&amp;quot;, 
  dv     = &amp;quot;score&amp;quot;,  
  within = &amp;quot;time&amp;quot;)

summary(hard)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Univariate Type III Repeated-Measures ANOVA Assuming Sphericity

            Sum Sq num Df Error SS den Df  F value    Pr(&amp;gt;F)    
(Intercept) 822.72      1   4.5704      9 1620.085 1.795e-11 ***
time        102.46      2  16.6237     18   55.469 2.014e-08 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1


Mauchly Tests for Sphericity

     Test statistic  p-value
time        0.55085 0.092076


Greenhouse-Geisser and Huynh-Feldt Corrections
 for Departure from Sphericity

      GG eps Pr(&amp;gt;F[GG])    
time 0.69006  2.161e-06 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

        HF eps   Pr(&amp;gt;F[HF])
time 0.7743711 6.032582e-07&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;residuals(hard) %&amp;gt;% shapiro.test()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Shapiro-Wilk normality test

data:  .
W = 0.95183, p-value = 0.1892&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sphericity is conceptually similar to homogeneity of variances in a between-subjects ANOVA. The violation of Sphericity makes repeated measures ANOVA to become too liberal (increases the Type I Error, or, as I like to say - increases the probability to find nonsense). Luckely, there are corrections for sphericity already build into {ggwithingstats}.&lt;/p&gt;
&lt;h2 id="whats-next-or-when-not-to-use-repeated-measures-anova"&gt;What’s next, or when not to use Repeated Measures ANOVA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;if samples are independent and normally distributed apply Student’s (variances are similar) or Welsh ANOVA (homogeneity of variances is violated)&lt;/li&gt;
&lt;li&gt;if samples are small (n&amp;lt;30) and not-normally distributed, use &lt;a href="https://yury-zablotski.netlify.app/post/fr_test/"&gt;Friedman test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Repeated Measures ANOVA is actually capricious and cranky. It does not like missing values or not exactly the same number of repeated measures for all individuals (imbalanced design), it often overfits, checking assumptions gets to cumbersome if we want to add more then one predictors. Thus, the solution for almost all of these problems and the most logical next step in your learning journey are - &lt;a href="https://yury-zablotski.netlify.app/post/mixed-effects-models-1/"&gt;Mixed Effects Models&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>b4b4fbc26586815beb0bcceb197bd5eb</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-01-30-rmanova</guid>
      <pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-01-30-rmanova/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Paired Samples Wilcoxon Signed Rank Test</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 6 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/YRlIkNKazF8" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://yury-zablotski.netlify.app/post/mann-whitney-wilcoxon-test/"&gt;Not-Paired (independent) Two Samples Mann-Whitney test&lt;/a&gt; and &lt;a href="https://yury-zablotski.netlify.app/post/two-samples-t-test-compare-groups/#paired-dependent-groups"&gt;Paired Two Samples t-Test&lt;/a&gt; would help.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;tidyverse&amp;quot;)  # for everything ;)
library(tidyverse)

# install.packages(&amp;quot;BSDA&amp;quot;)       # for Speed data
library(BSDA)

View(Speed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{BSDA} package provides a {Speed} dataset, with four columns on &lt;strong&gt;Speed reading&lt;/strong&gt;, namely, reading scores &lt;strong&gt;before&lt;/strong&gt; and &lt;strong&gt;after&lt;/strong&gt; the speed-reading course, &lt;strong&gt;difference&lt;/strong&gt; between them, which is actually much more important then the scores themselves, and finally, the &lt;strong&gt;signranks&lt;/strong&gt; column, which is the reason our test is called &lt;strong&gt;signed ranks&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A speed reading course should make me a faster reader. But how can I measure the progress? Well, I expect the &lt;strong&gt;median difference&lt;/strong&gt; in reading speed &lt;strong&gt;after&lt;/strong&gt; the course to be higher then zero, which is my alternative hypothesis (H&lt;sub&gt;Alt&lt;/sub&gt;), while my null hypothesis (H&lt;sub&gt;0&lt;/sub&gt;) is that the difference will be equal to zero. And since the difference here is more important then paired samples themselves, we only need to check the normality of the difference, not of both samples. And checking normality is important for choosing a correct test, otherwise we could get completely wrong result. We’ll see what happens if we choose a wrong test in a moment. Until then…&lt;/p&gt;
&lt;h2 id="check-normality-of-the-difference"&gt;Check normality of the difference&lt;/h2&gt;
&lt;p&gt;… a small p-value of the Shapiro-Wilk normality test indicates that our difference is &lt;strong&gt;not normally distributed&lt;/strong&gt;. That’s why we need a &lt;strong&gt;non-parametric Wilcoxon test&lt;/strong&gt;, to compare two paired samples. If the difference would have been &lt;strong&gt;normally distributed&lt;/strong&gt;, we would have taken a &lt;strong&gt;parametric paired t-test&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;shapiro.test(Speed$differ)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Shapiro-Wilk normality test

data:  Speed$differ
W = 0.75787, p-value = 0.001112&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, let’s bring our data in a tidy format by gathering columns, before and after, beneath each other. For &lt;strong&gt;paired&lt;/strong&gt; tests, the data &lt;strong&gt;needs to be sorted&lt;/strong&gt;, so that the first observation of the &lt;strong&gt;before&lt;/strong&gt; group, pairs with the first observation of the &lt;strong&gt;after&lt;/strong&gt; group. If our data is sorter, we are ready to compute the test.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# make long format - tidy data
d &amp;lt;- Speed %&amp;gt;% 
  gather(key = &amp;quot;speed&amp;quot;, value = &amp;quot;score&amp;quot;, before, after)

View(d)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="compute-paired-samples-wilcoxon-signed-rank-test"&gt;Compute Paired Samples Wilcoxon Signed Rank Test&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(ggstatsplot)

ggwithinstats(
  data = d,
  x    = speed, 
  y    = score, 
  type = &amp;quot;nonparametric&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf6fb798d6_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggsave(filename = &amp;quot;wilcoxon.jpg&amp;quot;, plot = last_plot(), width = 6, height = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the best way to compute our test is the {ggwithinstats} function from {ggstatsplot} package, which needs only 4 arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;our data&lt;/strong&gt; - d,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt; - as the grouping variable,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;y&lt;/strong&gt; - are the reading scores and&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;type&lt;/strong&gt; of statistical approach. Since our data was not normally distributed, we choose a &lt;strong&gt;nonparametric&lt;/strong&gt; test, and {ggwithinstats} automatically knows that we need a &lt;strong&gt;Paired Samples non-paramertic Wilcoxon Signed Rank Test&lt;/strong&gt; … (Good Lord, the name is killing me :)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Such simple command results in this statistically rich and publication ready plot! Now, let’s interpret the results.&lt;/p&gt;
&lt;h2 id="interpret-the-result"&gt;Interpret the result&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;V-statistics&lt;/strong&gt; explains why our test is called &lt;strong&gt;signed rank&lt;/strong&gt;. Namely,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;we first ignore the negative sign and &lt;strong&gt;rank&lt;/strong&gt; our absolute differences from smallest to largest, or from largest to smallest, does not matter. That is where the &lt;strong&gt;rank&lt;/strong&gt; part of the name comes from&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;we then put positive and negative signs of the difference back to the ranks, producing positive and negative ranks. That is where the &lt;strong&gt;signed&lt;/strong&gt; part of the name comes from and where &lt;strong&gt;signed rank&lt;/strong&gt; name comes together.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Summing up only positive ranks will give you the &lt;strong&gt;V - statistics&lt;/strong&gt; you see on the plot. This Wilcoxon statistics were previously used to get a &lt;strong&gt;p-value&lt;/strong&gt;, but nowadays, since p-values are calculated by computers, we can safely ignore it.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;Speed %&amp;gt;% 
  filter(signranks &amp;gt; 0) %&amp;gt;% 
  summarise(sum(signranks)) %&amp;gt;% 
  as.data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  sum(signranks)
1          100.5&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;our &lt;strong&gt;P-value&lt;/strong&gt; of 0.023 shows a moderate evidence against the null hypothesis (H&lt;sub&gt;0&lt;/sub&gt;), that median difference is equal to zero, in favor of the alternative hypothesis (H&lt;sub&gt;Alt&lt;/sub&gt;), that median difference is not equal to zero (Raiola, 2012). Particularly, we’ll read 7 score points faster after the course. But is a difference of 7 scores large? P-value can not tell that. A P-value only tells you that there is a difference, but not how strong this difference is.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/p_value_interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;fortunately, {ggwithinstats} provides a &lt;strong&gt;Rank biserial correlation coefficient&lt;/strong&gt; with 95% confidence intervals as the measure of the &lt;strong&gt;effect size&lt;/strong&gt;, which shows how large the difference is. The {interpret_rank_biserial} function from {effectsize} package helps to interpret this effect size and even provides the reference for interpretation. Our effect size of 0.68 means, that speed reading exercise had a &lt;strong&gt;very large, positive and significant effect on our speed reading&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;effectsize&amp;quot;)
library(effectsize)

interpret_rank_biserial(0.68)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;very large&amp;quot;
(Rules: funder2019)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;?interpret_rank_biserial&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="what-would-happen-if-we-choose-the-wrong-test"&gt;What would happen if we choose the wrong test&lt;/h2&gt;
&lt;p&gt;Now, what happens if I ignore the assumption of normality and conduct a &lt;strong&gt;Parametric Paired T-Test&lt;/strong&gt;? Well, I would compare means instead of medians and would get completely opposite result, namely, - speed reading course doesn’t help me to read faster, which is just wrong. Here I would have made a &lt;strong&gt;Type II Error&lt;/strong&gt;, or, in other words, I would have missed an important discovery. So, no Nobel Price for me.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggwithinstats(
  data = d,
  x    = speed, 
  y    = score, 
  type = &amp;quot;parametric&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf6fb798d6_files/figure-html/unnamed-chunk-8-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2 id="one-sided-two-samples-wilcoxon-test"&gt;One-sided two-samples Wilcoxon-test&lt;/h2&gt;
&lt;p&gt;Without visualization, the default &lt;code&gt;two.sided&lt;/code&gt; alternative of &lt;strong&gt;Wilcoxon test&lt;/strong&gt; only says that a difference is present, but does not say whether reading velocity decreases or increases. To find out exactly this, we need to test two new alternative hypotheses (&lt;strong&gt;H&lt;sub&gt;alt&lt;/sub&gt;&lt;/strong&gt;):&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;The median reading speed after the course &lt;strong&gt;decreases&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The median reading speed after the course &lt;strong&gt;increases&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Doing this will add another useful tool to your statistical toolbox, namely &lt;strong&gt;one-tailed (or one-sided) non-parametric two-samples paired Wilcoxon signed rank test&lt;/strong&gt; (the name is slowly killing me :)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(broom)
rbind(
  wilcox.test(data = d, score ~ speed, paired = T, alternative = &amp;quot;less&amp;quot;, conf.int = T, exact = F) %&amp;gt;% tidy(), 
  wilcox.test(data = d, score ~ speed, paired = T, alternative = &amp;quot;greater&amp;quot;, conf.int = T, exact = F) %&amp;gt;% tidy()
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 × 7
  estimate statistic p.value conf.low conf.high method     alternative
     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;      
1     3.50      100.  0.990   -Inf         6.00 Wilcoxon … less       
2     3.50      100.  0.0114     1.50    Inf    Wilcoxon … greater    &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Low &lt;em&gt;p-value&lt;/em&gt; (p = 0.01) of the &lt;strong&gt;greater-sided&lt;/strong&gt; test confirms that the &lt;strong&gt;reading velocity increases&lt;/strong&gt; after speed-reading course. The &lt;em&gt;p-value&lt;/em&gt; of the &lt;strong&gt;less-sided&lt;/strong&gt; test screams that your reading velocity will not decrease with the probability of 99% (p = 0.99).&lt;/p&gt;
&lt;h2 id="proof-of-the-concept-about-the-difference-between-two-samples"&gt;Proof of the concept about the difference between two samples&lt;/h2&gt;
&lt;p&gt;Interestingly, since we just tested our median difference against zero, we actually conducted the &lt;strong&gt;one-sample Wilcoxon test&lt;/strong&gt; on that difference. Let me prove it to you! If we compare the results of (1) &lt;strong&gt;one-sample Wilcoxon test&lt;/strong&gt; on the difference with (2) the &lt;strong&gt;two-samples paired Wilcoxon test&lt;/strong&gt;, we’ll get identical V-statistics and p-values. How cool is that?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;bind_rows(
  wilcox.test(Speed$differ) %&amp;gt;% tidy(),
  wilcox.test(Speed$after, Speed$before, paired = T) %&amp;gt;% tidy()
) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 × 4
  statistic p.value method                                 alternative
      &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                                  &amp;lt;chr&amp;gt;      
1      100.  0.0229 Wilcoxon signed rank test with contin… two.sided  
2      100.  0.0229 Wilcoxon signed rank test with contin… two.sided  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, our fancy &lt;strong&gt;Paired Samples test&lt;/strong&gt; is actually &lt;strong&gt;One-Sample test&lt;/strong&gt; on the difference, where difference is checked against zero. We can of course check our one-sample against a different value, let’s say 6, which you can learn from &lt;a href="https://youtu.be/x5RcZlc-w4A"&gt;this video&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="dont-use-two-samples-wilcoxon-test-if"&gt;Don’t use &lt;em&gt;two-samples Wilcoxon-test&lt;/em&gt; if:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;samples are independent. In this case apply [&lt;em&gt;Mann-Whitney-Wilcoxon-test&lt;/em&gt;] (&lt;a href="https://yury-zablotski.netlify.com/post/mann-whitney-wilcoxon-test/" class="uri"&gt;https://yury-zablotski.netlify.com/post/mann-whitney-wilcoxon-test/&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;samples are small (n&amp;lt;30) and normally distributed (or big and near normal). In this case use the more powerful &lt;a href="https://yury-zablotski.netlify.com/post/two-sample-t-test-compare-your-work-to-others/"&gt;two-samples t-test&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Two-samples Wilcoxon-test&lt;/em&gt; can be more powerful then &lt;em&gt;two-samples t-test&lt;/em&gt; when difference between two samples at low numbers (&amp;lt;30) is not-normally distributed. For big samples and not to unnormal distribution, &lt;em&gt;t-test&lt;/em&gt; will do fine. Another advantage of the &lt;em&gt;median-based non-parametric Wilcoxon test&lt;/em&gt; is that it more robust to the outliers.&lt;/p&gt;
&lt;h2 id="whats-next"&gt;What’s next&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Check out the &lt;a href="https://yury-zablotski.netlify.com/post/mann-whitney-wilcoxon-test/"&gt;&lt;em&gt;Mann-Whitney-Wilcoxon-test&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you need to compare more then two samples, first check whether they are normally distributed, and if they are, go to &lt;a href="https://yury-zablotski.netlify.com/post/one-way-anova/"&gt;ANOVA&lt;/a&gt;, but if they aren’t, go to the non-parametric analogue of &lt;em&gt;ANOVA&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Kruskal-Wallis Rank Sum Test&lt;/em&gt; would be another idea.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="http://faculty.washington.edu/heagerty/Books/Biostatistics/TABLES/Wilcoxon/" class="uri"&gt;http://faculty.washington.edu/heagerty/Books/Biostatistics/TABLES/Wilcoxon/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>6f8a4677d968d016331925790817637a</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr</guid>
      <pubDate>Wed, 26 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R demo | Correlation Matrix | Danger or opportunity?</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2022-01-05-correlationmatrixinr</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s less then 5 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/ffhQil7KhSo" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://yuzar-blog.netlify.app/posts/2021-12-29-correlationinr/"&gt;Correlation Blogpost&lt;/a&gt; would help.&lt;/p&gt;
&lt;h2 id="visualize-distribution"&gt;Visualize distribution!&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;dlookr&amp;quot;)
library(dlookr)
plot_correlate(iris)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf4570fb51_files/figure-html/unnamed-chunk-2-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;fastStat&amp;quot;)
library(fastStat)
cor_sig_star(iris[, 1:3])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                    Sepal.Width Petal.Length
1 Sepal.Length -0.118(0.152)     0.872(0)***
2                   Sepal.Width -0.428(0)***
3                               Petal.Length&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A {chart.Correlation} function from {PerformanceAnalytics} package does both (1) provides correlation coefficients with significance and (2) plots the data. This plot immediately uncovers &lt;strong&gt;4 reasons why we can’t trust a plain correlation matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;PerformanceAnalytics&amp;quot;)
library(&amp;quot;PerformanceAnalytics&amp;quot;)
chart.Correlation(iris[, 1:3])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf4570fb51_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h3 id="linearity-assumption"&gt;1. Linearity assumption&lt;/h3&gt;
&lt;p&gt;First, the data is not-linearly distributed, while correlation only makes sense for linear relationships. If fact, we could draw three straight lines in this plot, while our negative correlation coefficient assumes only one of them.&lt;/p&gt;
&lt;h3 id="grouping-variables-simpsons-paradox"&gt;2. Grouping variables &amp;amp; Simpson’s paradox&lt;/h3&gt;
&lt;p&gt;Secondly, some data seemed to be clustered into groups. Indeed, the “Iris” dataset, which you already have in R, has a “Species” column with 3 different Species of iris plant. So, putting all three species in one basket feels wrong.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2022-01-05-correlationmatrixinr/iris.jpg" /&gt; A {ggpairs} command from {GGally} package accounts for a grouping variable and conducts correlation analysis for all variables and groups at once, while also plotting a correlation line on top of every group.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;tidyverse&amp;quot;)
library(tidyverse) # for &amp;quot;aes()&amp;quot; &amp;amp; later for &amp;quot;%&amp;gt;%&amp;quot; 

# install.packages(&amp;quot;GGally&amp;quot;)
library(GGally)
ggpairs(iris, 
        columns = 1:3, 
        aes(colour=Species),
        lower = list(continuous = &amp;quot;smooth&amp;quot;),
        upper = list(continuous = wrap(&amp;quot;cor&amp;quot;, 
                         method = &amp;quot;pearson&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf4570fb51_files/figure-html/unnamed-chunk-4-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;These plots reveal that two of the negative coefficients before splitting into groups show the opposite of what is actually true, namely, the correlation within every group &lt;strong&gt;IS&lt;/strong&gt; positive.&lt;/p&gt;
&lt;p&gt;Moreover, NOT-significant-negative-correlation between Sepal.Length and Sepal.Width turned out to be a significant positive correlation for all three species! If I would have accepted the not-significant negative correlation, I would have made a Type II Error, in other words, I would have missed an important discovery. So, no Nobel Price for me.&lt;/p&gt;
&lt;p&gt;Vice Versa, a highly significant correlation between Petal.Length and Sepal.Length for all three Iris species, is not significant at all for “Setosa.” Here I would have made a Type I Error, meaning, I would have discovered nonsense.&lt;/p&gt;
&lt;p&gt;The phenomenon, where grouped data shows the opposite correlation scenario as compared to ungrouped data is known as &lt;strong&gt;Simpson’s Paradox, also called Yule-Simpson effect&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;But, that’s not all! The third problem of not visualizing the data is that we have no idea about the normality of distribution, so we actually don’t know which Correlation method to use, Pearson, Spearman or something else.&lt;/p&gt;
&lt;h3 id="normality-assumption"&gt;3. Normality assumption&lt;/h3&gt;
&lt;p&gt;But if we look at histograms from {chart.Correlation} function, we immediately see that two of variables, Sepal.Length and Petal.Length, are not-normally distributed. Shapiro-Wilk Normality Tests confirm this intuition with low p-values.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;flextable&amp;quot;)
library(flextable)

iris %&amp;gt;% 
  normality() %&amp;gt;% 
  mutate(across(is.numeric, ~round(., 3))) %&amp;gt;%
  regulartable()&lt;/code&gt;&lt;/pre&gt;
&lt;template id="ffab0c80-4cd5-4240-b238-91316652b1a2"&gt;&lt;style&gt;
.tabwid table{
  border-spacing:0px !important;
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
&lt;/style&gt;&lt;div class="tabwid"&gt;&lt;style&gt;.cl-5fbc5690{}.cl-5fb48cee{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5fb49a40{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5fb49a4a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5fb4c664{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5fb4c66e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5fb4c678{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5fb4c679{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5fb4c67a{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5fb4c682{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}&lt;/style&gt;&lt;table class='cl-5fbc5690'&gt;
&lt;thead&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fb4c682"&gt;&lt;p class="cl-5fb49a40"&gt;&lt;span class="cl-5fb48cee"&gt;vars&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c67a"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;statistic&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c67a"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;p_value&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c67a"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;sample&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fb4c664"&gt;&lt;p class="cl-5fb49a40"&gt;&lt;span class="cl-5fb48cee"&gt;Sepal.Length&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c66e"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;0.976&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c66e"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;0.010&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c66e"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;150&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fb4c664"&gt;&lt;p class="cl-5fb49a40"&gt;&lt;span class="cl-5fb48cee"&gt;Sepal.Width&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c66e"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;0.985&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c66e"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;0.101&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c66e"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;150&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fb4c664"&gt;&lt;p class="cl-5fb49a40"&gt;&lt;span class="cl-5fb48cee"&gt;Petal.Length&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c66e"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;0.876&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c66e"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;0.000&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c66e"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;150&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fb4c679"&gt;&lt;p class="cl-5fb49a40"&gt;&lt;span class="cl-5fb48cee"&gt;Petal.Width&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c678"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;0.902&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c678"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;0.000&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fb4c678"&gt;&lt;p class="cl-5fb49a4a"&gt;&lt;span class="cl-5fb48cee"&gt;150&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;/template&gt;
&lt;div class="flextable-shadow-host" id="afa7f2c6-9ebb-47f5-ad5b-efd7cbddd67a"&gt;&lt;/div&gt;
&lt;script&gt;
var dest = document.getElementById("afa7f2c6-9ebb-47f5-ad5b-efd7cbddd67a");
var template = document.getElementById("ffab0c80-4cd5-4240-b238-91316652b1a2");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
&lt;/script&gt;

&lt;p&gt;So, we could have applied a non-parametric Spearman or Kendall correlation methods, but if we check the normality for every Species, which are clustered in groups, we’ll see that ALL OF THEM are perfectly normally distributed, so, we should use a parametric Pearson correlation analysis to ensure the highest statistical power! Otherwise, we’ll again either miss a discovery, or discover complete nonsense.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;iris %&amp;gt;% 
  group_by(Species) %&amp;gt;% 
  select(1,3) %&amp;gt;% 
  normality() %&amp;gt;%  
  mutate(across(is.numeric, ~round(., 3))) %&amp;gt;%
  regulartable()&lt;/code&gt;&lt;/pre&gt;
&lt;template id="24459b1d-facd-41f7-b8db-729fb8947e0a"&gt;&lt;style&gt;
.tabwid table{
  border-spacing:0px !important;
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
&lt;/style&gt;&lt;div class="tabwid"&gt;&lt;style&gt;.cl-5fd5f500{}.cl-5fce90e4{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5fcea07a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5fcea084{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5fced874{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5fced87e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5fced888{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5fced892{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5fced893{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5fced894{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}&lt;/style&gt;&lt;table class='cl-5fd5f500'&gt;
&lt;thead&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fced894"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;variable&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced894"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;Species&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced893"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;statistic&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced893"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;p_value&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced893"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;sample&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fced87e"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;Sepal.Length&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced87e"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;setosa&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.978&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.460&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;50&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fced87e"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;Sepal.Length&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced87e"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;versicolor&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.978&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.465&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;50&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fced87e"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;Sepal.Length&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced87e"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;virginica&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.971&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.258&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;50&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fced87e"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;Petal.Length&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced87e"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;setosa&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.955&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.055&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;50&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fced87e"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;Petal.Length&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced87e"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;versicolor&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.966&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.158&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced874"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;50&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-5fced892"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;Petal.Length&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced892"&gt;&lt;p class="cl-5fcea07a"&gt;&lt;span class="cl-5fce90e4"&gt;virginica&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced888"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.962&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced888"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;0.110&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-5fced888"&gt;&lt;p class="cl-5fcea084"&gt;&lt;span class="cl-5fce90e4"&gt;50&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;/template&gt;
&lt;div class="flextable-shadow-host" id="5a18ff62-077e-4ee4-a1f2-c10a0ca7a893"&gt;&lt;/div&gt;
&lt;script&gt;
var dest = document.getElementById("5a18ff62-077e-4ee4-a1f2-c10a0ca7a893");
var template = document.getElementById("24459b1d-facd-41f7-b8db-729fb8947e0a");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
&lt;/script&gt;

&lt;h3 id="correct-for-multiple-comparisons"&gt;4. Correct for multiple comparisons&lt;/h3&gt;
&lt;p&gt;The last thing which makes using plain correlation matrix statistically dangerous is that you have multiple comparisons without adjusting p-values for them, which again, increases the probability to find nonsense. A {grouped_ggcorrmat} function from {ggstatsplot} package helps with that. Within this function your can specify only 3 argument to get all the results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, the data - “Iris,”&lt;/li&gt;
&lt;li&gt;then “type” of correlation, we’ll go with a “parametric” Pearsons method since we know that our data is linearly and normally distributed and&lt;/li&gt;
&lt;li&gt;the grouping variable, which is “Species” in our case&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here you’ll see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how strong the correlation is,&lt;/li&gt;
&lt;li&gt;in which direction it goes and&lt;/li&gt;
&lt;li&gt;whether correlation is significant or not after p-values were adjusted for multiple comparisons.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(ggstatsplot)

grouped_ggcorrmat(
  iris,
  type = &amp;quot;parametric&amp;quot;,
  grouping.var = Species, 
  plotgrid.args = list(nrow = 2)) # looks better&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf4570fb51_files/figure-html/unnamed-chunk-7-1.png" width="960" /&gt;&lt;/p&gt;
&lt;h2 id="whats-next"&gt;What’s next?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;As you can see, while correlation matrix is an useful Exploratory Tool, we shouldn’t trust the numbers too quickly. We need to (1) visualize the data, (2) check the assumptions of normality and (3) linearity, (4) check for grouping variables and (5) correct for multiple comparisons. And since some data might be suitable for parametric analysis, while some not, it’s usually better to make a deeper dive into every single correlation between only two numeric variables, which you can learn &lt;a href="https://youtu.be/eXczd0ewVgE"&gt;from this short video&lt;/a&gt;, where I showed how to produce this statistically rich plot using only one command and how to interpret all these results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Start to learn about linear regression, you won’t regret it ;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>adba62613ce5470853f6e91aceeefcd4</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2022-01-05-correlationmatrixinr</guid>
      <pubDate>Fri, 21 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2022-01-05-correlationmatrixinr/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Correlation Analysis in R | Pearson, Spearman, Robust, Bayesian | How to conduct, visualise and interpret</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-12-29-correlationinr</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 4 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/eXczd0ewVgE" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;Understanding &lt;a href="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/"&gt;hypothesis testing&lt;/a&gt; and &lt;a href="https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation/"&gt;p-values&lt;/a&gt; would be very helpful.&lt;/p&gt;
&lt;h2 id="the-most-effective-way-to-compute-correlation-in-r"&gt;The most effective way to compute correlation in R&lt;/h2&gt;
&lt;p&gt;{ggscatterstats} function from {ggstatsplot} package is probably the best way to conduct correlation analysis. Within this function we need to specify only 4 arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;our data&lt;/strong&gt;, e.g. let’s take &lt;code&gt;mtcars&lt;/code&gt;, which you already have in R, so you don’t need to look for it&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt; - as one of your numeric variables, for example &lt;em&gt;horsepower&lt;/em&gt; of cars&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;y&lt;/strong&gt; - would be your second numeric variable, let’s take the &lt;em&gt;Acceleration to a quarter mile&lt;/em&gt; and&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;type&lt;/strong&gt; of statistical approach. We choose &lt;strong&gt;parametric&lt;/strong&gt; for the classic &lt;strong&gt;Pearson’s correlation&lt;/strong&gt;, and we’ll get back to other approaches later.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;{?ggscatterstats} function has of coarse many more useful arguments, but using &lt;strong&gt;only 4 of them already results in this statistically rich and publication ready plot!&lt;/strong&gt; Now, let’s interpret the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(ggstatsplot)

ggscatterstats(
    data = mtcars, 
    x    = hp,
    y    = qsec,
    type = &amp;quot;parametric&amp;quot;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf4e4d281_files/figure-html/unnamed-chunk-2-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;?ggscatterstats

# install.packages(&amp;quot;ggplot2&amp;quot;)
library(ggplot2)

# save your plot
ggsave(&amp;quot;corr_p.jpg&amp;quot;, plot = last_plot(), width = 5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="interpretation"&gt;Interpretation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;t statistics&lt;/strong&gt; was previously used to manually calculate p-value, but nowadays, since p-values are calculated by computers, we can safely ignore it&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;our very low &lt;strong&gt;P-value&lt;/strong&gt; shows a very strong evidence against the null hypothesis, that there is no correlation between our variables, in favor of the alternative hypothesis, that horsepower of cars and their acceleration &lt;em&gt;are correlated&lt;/em&gt; (Raiola, 2012)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-29-correlationinr/p_value_interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;Bayes Factor&lt;/strong&gt; (Jeffreys, 1961) of - 8.25 in our example agrees with a &lt;strong&gt;p-value&lt;/strong&gt; by showing a &lt;strong&gt;decisive evidence for the alternative hypothesis&lt;/strong&gt; - that correlation exists&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-29-correlationinr/bf_interpretation.png" /&gt;&lt;/p&gt;
&lt;p&gt;However, neither &lt;strong&gt;p-value&lt;/strong&gt; nor &lt;strong&gt;Bayes Factor&lt;/strong&gt; can say &lt;strong&gt;how strong this correlation is&lt;/strong&gt; and &lt;strong&gt;in which direction, positive or negative, it goes&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pearson’s correlation coefficient&lt;/strong&gt;, however, can say both.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First: &lt;strong&gt;the minus in front of our correlation coefficient&lt;/strong&gt; points to a negative correlation, namely, the increase in horsepower of cars decreases their quarter mile time. In other words, the stronger the car, the faster it accelerates.&lt;/p&gt;
&lt;p&gt;Secondly: &lt;strong&gt;Correlation coefficient r&lt;/strong&gt; itself is the effect size and ranges from -1, for the perfect negative correlation, to 1 for the perfect positive correlation. The further &lt;strong&gt;r&lt;/strong&gt; is from zero and the closer &lt;strong&gt;r&lt;/strong&gt; is to 1 or -1, the stronger the correlation between two variables is:&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-29-correlationinr/correlation_examples.png" /&gt;&lt;/p&gt;
&lt;p&gt;Our coefficient of -0.71 shows &lt;strong&gt;high negative correlation&lt;/strong&gt;. But since no statistical method is perfect we should use several methods for the same question in order to be more certain. And that’s exactly what {ggscatterstats} does! (interpretation of the correlation coefficient in the picture below after Hinkle 2003).&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-29-correlationinr/correlation_interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it provides a second measure of the effect size, namely &lt;strong&gt;Bayesian Pearson’s correlation coefficient&lt;/strong&gt; with 95% Highest Density Intervals. Interestingly, Bayesian correlation is not high, like in the frequentist example, but &lt;strong&gt;moderately negative&lt;/strong&gt;. So, if two methods don’t agree, which of them should we trust?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="spearman-nonparametric-correlation-for-not-linearly-and-not-normally-distributed-data"&gt;Spearman nonparametric correlation for not-linearly and not-normally distributed data&lt;/h2&gt;
&lt;p&gt;Well, I would take the Bayesian one, because it is less susceptible to not-linearly or not-normally distributed data. However, if the data is non-linear or not normally distributed, it is much better to use a non-parametric Spearman rank-based correlation analysis instead of Pearson.&lt;/p&gt;
&lt;p&gt;Fortunately, the only thing you need to change in {ggscatterstats} function - is the &lt;strong&gt;type&lt;/strong&gt; argument, from “parametric” to “nonparametric.”&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggscatterstats(
    data = mtcars, 
    x    = hp,
    y    = qsec,
    type = &amp;quot;nonparametric&amp;quot;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf4e4d281_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# save your plot
ggsave(&amp;quot;corr_np.jpg&amp;quot;, plot = last_plot(), width = 5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="robust-method-if-you-have-outliers"&gt;Robust method if you have outliers&lt;/h2&gt;
&lt;p&gt;Moreover, if we find some outliers in our data, we could apply a “robust” correlation to decrease the influence of outliers.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggscatterstats(data = mtcars, x = hp, y = qsec, type = &amp;quot;robust&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf4e4d281_files/figure-html/unnamed-chunk-4-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2 id="checking-for-normality-and-linearity"&gt;Checking for normality and linearity&lt;/h2&gt;
&lt;p&gt;Plotting the relationships before making conclusions, will &lt;strong&gt;allow your to make sure they are linear&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Here is some code to check whether our data is bell shaped, or &lt;strong&gt;normally distributed&lt;/strong&gt;. This can be done in two ways.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;first, with &lt;em&gt;Shapiro-Wilk normality test&lt;/em&gt;, where p-value &amp;gt; 0.05 would mean normally distributed data,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;or visually, with a Quantile-Quantile plot, using {ggqqplot} function from {ggpubr} package, where all points falling into the gray area would mean normally distributed data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggpubr&amp;quot;)
library(ggpubr)

shapiro.test(mtcars$hp) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Shapiro-Wilk normality test

data:  mtcars$hp
W = 0.93342, p-value = 0.04881&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggqqplot(mtcars$hp) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf4e4d281_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;shapiro.test(mtcars$qsec) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Shapiro-Wilk normality test

data:  mtcars$qsec
W = 0.97325, p-value = 0.5935&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggqqplot(mtcars$qsec) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf4e4d281_files/figure-html/unnamed-chunk-5-2.png" width="672" /&gt; &lt;strong&gt;Conclusion:&lt;/strong&gt; Our data is linear and normally distributed, so we are fine with the classic Pearson’s correlation. However, if it is not the case, take a non-parametric Spearman correlation and if you find lots (&amp;gt;3) of outliers, use the Robust correlation method.&lt;/p&gt;
&lt;h2 id="whats-next"&gt;What’s next?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;first of all, never forget to cite this amazing package!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;citation(&amp;quot;ggstatsplot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  Patil, I. (2021). Visualizations with statistical details:
  The &amp;#39;ggstatsplot&amp;#39; approach. Journal of Open Source Software,
  6(61), 3167, doi:10.21105/joss.03167

A BibTeX entry for LaTeX users is

  @Article{,
    doi = {10.21105/joss.03167},
    url = {https://doi.org/10.21105/joss.03167},
    year = {2021},
    publisher = {{The Open Journal}},
    volume = {6},
    number = {61},
    pages = {3167},
    author = {Indrajeet Patil},
    title = {{Visualizations with statistical details: The {&amp;#39;ggstatsplot&amp;#39;} approach}},
    journal = {{Journal of Open Source Software}},
  }&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Ok, after learning about correlation between two numeric variables, you might wonder how to check the association between two categorical variables? Well, &lt;a href="https://youtu.be/8Tj0-yMPO64"&gt;this video will shown you a quick and effective way&lt;/a&gt;. It shows you how to conduct, visualize and interpret Chi-Square test &amp;amp; pairwise post-hoc tests via {ggbarstats} function from {ggstatsplot} package 📦.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="further-readings-and-watchings"&gt;Further readings and watchings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For more info about correlation and older, but maybe more familiar R code, check my older article on correlation: &lt;a href="https://yury-zablotski.netlify.app/post/correlation/#how-to-compute-correlation" class="uri"&gt;https://yury-zablotski.netlify.app/post/correlation/#how-to-compute-correlation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hinkle DE, Wiersma W, Jurs SG. Applied Statistics for the Behavioral Sciences. 5th ed. Boston: Houghton Mifflin; 2003.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Jeffreys, H. 1961. Theory of Probability. 3rd ed. Oxford: Oxford University Press.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Raiola, Gaetano &amp;amp; Di tore, Pio. (2012). Statistical study on bodily communication skills in volleyball to improve teaching methods. Journal of Human Sport and Exercise. 7. 10.4100/jhse.2012.72.12.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Correlation_and_dependence" class="uri"&gt;https://en.wikipedia.org/wiki/Correlation_and_dependence&lt;/a&gt;&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>08a8c2e0187452b8d18b5e153478dcb9</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-12-29-correlationinr</guid>
      <pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-12-29-correlationinr/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>One-sample Student’s t-test and One-sample Wilcoxon test: or how to compare your work to the work of others.</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 5 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/x5RcZlc-w4A" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;Understanding &lt;a href="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/"&gt;hypothesis testing&lt;/a&gt; and &lt;a href="https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation/"&gt;p-values&lt;/a&gt; would be very helpful.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/to-do-list.jpg" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 1. simulate your to-do results

# install.packages(&amp;quot;tidyverse&amp;quot;)
library(tidyverse)

set.seed(1)  # stabilizes random output, so you always get the same result
my_to_dos &amp;lt;- round(rnorm(n = 21, mean = 7, sd = 3)) 

my_to_dos&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1]  5  8  4 12  8  5  8  9  9  6 12  8  5  0 10  7  7 10  9  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you collect your to-do lists for the last 21 days, you find out that you finish 7 out of 10 tasks per day on average … ± 3. Now, since we have our data we can compare your average of 7 to the average of others, which is 6 our of 10, or 60%, according to &lt;a href="https://www.huffpost.com/entry/forty-one-percent-of-tasks-on-to-do-lists-are-never-done_b_9308978?guccounter=1&amp;amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;amp;guce_referrer_sig=AQAAANQ9AhoKTEu0BJFpJJbnXQLY_7SeRTIybJaZEHrK2N0bi1lPpFSByEH5kUdhBXNRo93VwzVF_ZLIkjBOCxhVzkK8I4CFTm0XPXsPQ1Rnl-Q77Q7RXpmcKQkrmr9QlKXKxEDFxoxsrkXwbe_wF2fNSsqotvBAE472NESpYzs7nY2v"&gt;this article&lt;/a&gt;. Is 7 significantly different from 6? &lt;strong&gt;One-sample t-test&lt;/strong&gt; or &lt;strong&gt;One-sample Wilcoxon test&lt;/strong&gt; can tell that, but how do we know which test we take?&lt;/p&gt;
&lt;h2 id="choose-the-test"&gt;Choose the test&lt;/h2&gt;
&lt;p&gt;Well, we’ll simply check whether our data is bell shaped, or &lt;strong&gt;normally distributed&lt;/strong&gt;. This can be done in two ways.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;first, with &lt;em&gt;Shapiro-Wilk normality test&lt;/em&gt;, where p-value &amp;gt; 0.05 would mean normally distributed data,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;or visually, with a Quantile-Quantile plot, using {ggqqplot} function from {ggpubr} package, where all points falling into the gray area would mean normally distributed data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If data is normally distributed, like in our example, &lt;strong&gt;the mean&lt;/strong&gt; is a good representative for our sample, in this case we’ll use &lt;strong&gt;one-sample t-test&lt;/strong&gt;. But, if data is skewed, we choose &lt;a href="https://yury-zablotski.netlify.com/post/wilcoxon-ака-mann-whitney-test/"&gt;&lt;strong&gt;one-sample Wilcoxon test&lt;/strong&gt;&lt;/a&gt; which uses &lt;strong&gt;median&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 2. check for normality

shapiro.test(my_to_dos) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Shapiro-Wilk normality test

data:  my_to_dos
W = 0.93695, p-value = 0.1896&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;ggpubr&amp;quot;)
library(ggpubr)

ggqqplot(my_to_dos)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf2ade3dd5_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2 id="one-sample-t-test"&gt;One-sample t-test&lt;/h2&gt;
&lt;p&gt;Since &lt;em&gt;one-sample t-test&lt;/em&gt; checks the &lt;strong&gt;similarity&lt;/strong&gt; between our sample’s mean and the expected mean:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;our &lt;strong&gt;null hypothesis (H&lt;sub&gt;0&lt;/sub&gt;)&lt;/strong&gt; suggests that the means are similar, while&lt;/li&gt;
&lt;li&gt;our &lt;strong&gt;alternative hypothesis (H&lt;sub&gt;alt&lt;/sub&gt;)&lt;/strong&gt; suggests that means differ&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The presence of similarity or difference is not good or bad per se. If the mean of others is known to be correct, and your experiment shows no difference from that, be glad, your results are plausible. But if your results are different from the expected mean, you either did something wrong, or you might have discovered something new.&lt;/p&gt;
&lt;p&gt;{gghistostats} function from {ggstatsplot} package is probably the best way for our test. Within this function we need to specify only 5 arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;our data&lt;/strong&gt;, which we just created and called &lt;strong&gt;my_to_dos&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt; - is the numeric variable in our data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;test.value&lt;/strong&gt; is the productivity of others we want to compare to&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;type&lt;/strong&gt; of statistical approach. We choose &lt;strong&gt;parametric&lt;/strong&gt; for &lt;strong&gt;One-sample t-test&lt;/strong&gt;, and we’ll get back to other approaches later and finally…&lt;/li&gt;
&lt;li&gt;we set the &lt;strong&gt;normal.curve&lt;/strong&gt; argument to TRUE, only because it looks cool ;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This simple command results in a statistically rich and publication ready plot! Now, let’s interpret the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 3. compare your mean to the other mean (6 tasks = 60%)

# install.packages(&amp;quot;ggstatsplot&amp;quot;)
library(ggstatsplot)

gghistostats(
    data       = my_to_dos %&amp;gt;% as_tibble,
    x          = value,
    test.value = 6, ## default value is 0
    type       = &amp;quot;p&amp;quot;, 
    normal.curve = T
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf2ade3dd5_files/figure-html/unnamed-chunk-4-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# save your plot
ggsave(&amp;quot;ostt.jpg&amp;quot;, plot = last_plot(), width = 5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="interpretation"&gt;Interpretation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;t-value&lt;/strong&gt; is the &lt;strong&gt;measure of similarity&lt;/strong&gt; between compared means measured in units of standard error. The further &lt;em&gt;t-value&lt;/em&gt; is from zero, the more different are the means:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[t = \frac{our.mean - expected.mean}{standart.error} = \frac{our.mean - expected.mean}{ \frac{standart.deviation} {\sqrt sample.size} }\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;DF&lt;/strong&gt; (not shown) stands for &lt;strong&gt;degrees of freedom&lt;/strong&gt;. &lt;em&gt;DF&lt;/em&gt; is always equal to the sample size - 1. For our 21 measurements, &lt;em&gt;DF&lt;/em&gt; = 21 - 1 = 20.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;t-values&lt;/strong&gt; and &lt;strong&gt;DFs&lt;/strong&gt; were previously used to calculate &lt;strong&gt;p-values&lt;/strong&gt;. But, modern statistical software always report &lt;em&gt;p-values&lt;/em&gt;, so, we can safely ignore &lt;em&gt;t&lt;/em&gt;. &lt;em&gt;P-values&lt;/em&gt; can also be seeing as the measure of similarity, the smaller p-value, the less similar the means are. Our &lt;em&gt;p-value&lt;/em&gt; of 0.013 indicates that our means are different. Actually, significantly different, if we follow the threshold of 0.05. Thus, we can conclude that we are significantly more productive than other people. So, we can feel proud about ourselves for a second… and then get back to work ;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Indeed, if you imagine doing 1.7 tasks more every day for one year, at the end of this year you’ll accomplish over 620 more tasks then the average person. &lt;strong&gt;But, more importantly&lt;/strong&gt;, this will make &lt;strong&gt;YOU&lt;/strong&gt; significantly better then yourself-one-year-ago &lt;strong&gt;in whatever you do.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Hedges’ g&lt;/em&gt; is the estimated &lt;em&gt;standardized&lt;/em&gt; difference between the means, and is the &lt;strong&gt;Effect Size&lt;/strong&gt;. It goes from zero to one and is interpreted in the same way as &lt;em&gt;Cohen’s d&lt;/em&gt; Effect size. In our example the effect is &lt;em&gt;medium strong&lt;/em&gt;, which supports the conclusion of the p-value that difference exists.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/effect_size_hedges_d.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There is one problem with &lt;em&gt;Hedges’ g&lt;/em&gt; though - it is a &lt;strong&gt;standardized difference&lt;/strong&gt; between the means, not the normal difference. And that is where &lt;strong&gt;Bayesian Difference&lt;/strong&gt; with it’s 95% Highest Density Intervals on the bottom of the plot seems more intuitive and is a second measure of the effect size here. It shows that the average person accomplishes 1.56 tasks less then you.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If that’s not enough, we can look at the &lt;strong&gt;Bayes Factor&lt;/strong&gt; (Jeffreys, 1961), which tests both null and alternative hypotheses at the same time. Bayes Factor of -1.37 in our example indicates a &lt;strong&gt;substantial evidence for the alternative hypothesis&lt;/strong&gt; - that our productivity is above average, which IS in line with the frequentists statistics on the top of the plot.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/bf_interpretation.png" /&gt;&lt;/p&gt;
&lt;h3 id="the-old-way-to-do-one-sample-t-test-in-r"&gt;The old way to do one-sample t-test in R&lt;/h3&gt;
&lt;pre class="r"&gt;&lt;code&gt;t.test(my_to_dos, mu = 6) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    One Sample t-test

data:  my_to_dos
t = 2.7116, df = 20, p-value = 0.01343
alternative hypothesis: true mean is not equal to 6
95 percent confidence interval:
 6.384558 8.948776
sample estimates:
mean of x 
 7.666667 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also conduct the t-test in a classical way in R, which gives us some additional metrics, for instance, &lt;strong&gt;95% confidence intervals (CI)&lt;/strong&gt; of the mean. The previous results of 6 [tasks/day] is outside of our &lt;strong&gt;95% CI&lt;/strong&gt; [6.38, 8.95], thus our result is indeed different. It also gives us the &lt;em&gt;Degrees of freedom&lt;/em&gt;, which were used to get p-values.&lt;/p&gt;
&lt;h3 id="one-sided-one-sample-t-test-do-them-only-if-you-really-know-what-you-are-doing"&gt;One-sided one-sample t-test (do them only if you really know what you are doing)&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;R&lt;/em&gt; performs a two-sided test by default. Performing a one-sided test allows us to say whether our result is &lt;strong&gt;significantly lower&lt;/strong&gt; or &lt;strong&gt;significantly greater&lt;/strong&gt;. We could also change the confidence level, to i.e. &lt;code&gt;conf.level = 0.99&lt;/code&gt; to be 99% sure of our conclusion:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t.test(my_to_dos, mu = 6, alternative = &amp;quot;less&amp;quot;,    conf.level = 0.99) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    One Sample t-test

data:  my_to_dos
t = 2.7116, df = 20, p-value = 0.9933
alternative hypothesis: true mean is less than 6
99 percent confidence interval:
     -Inf 9.220453
sample estimates:
mean of x 
 7.666667 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;t.test(my_to_dos, mu = 6, alternative = &amp;quot;greater&amp;quot;, conf.level = 0.99)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    One Sample t-test

data:  my_to_dos
t = 2.7116, df = 20, p-value = 0.006716
alternative hypothesis: true mean is greater than 6
99 percent confidence interval:
 6.11288     Inf
sample estimates:
mean of x 
 7.666667 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first &lt;em&gt;one-sided t-test&lt;/em&gt; shows that your mean performance is &lt;strong&gt;definitely not lower&lt;/strong&gt; (p&amp;gt;0.05 and expected average of 6 is included in 99% CI) then the average. The second &lt;em&gt;one-sided t-test&lt;/em&gt; indicates that your performance is &lt;strong&gt;significantly greater&lt;/strong&gt; then the average (p&amp;lt;0.05 and 6 is not included in 99% CI).&lt;/p&gt;
&lt;h2 id="one-sample-wilcoxon-test"&gt;One-sample Wilcoxon test&lt;/h2&gt;
&lt;p&gt;If our data is not normally distributed, the &lt;strong&gt;median&lt;/strong&gt; would describe our data much better than the average. And that’s what &lt;strong&gt;One-sample Wilcoxon test&lt;/strong&gt; does. The good news about {gghistostats} function is that, the only thing you need to change is the &lt;strong&gt;type&lt;/strong&gt; argument, from “parametric” to “nonparametric.” By the way, I encourage you to explore the function for yourself by simply asking R about it.&lt;/p&gt;
&lt;p&gt;If our data would have been not-normally distributed, your productivity would be even better, which you can see from&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the higher median to-do, namely 8, as compared with the mean of 7.7,&lt;/li&gt;
&lt;li&gt;lower p-value if we consider a p-value a measure of similarity (0.01 vs. 0.013) and&lt;/li&gt;
&lt;li&gt;stronger effect size (0.66 vs. 0.57).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;?gghistostats

gghistostats(
    data       = my_to_dos %&amp;gt;% as_tibble(),
    x          = value,
    test.value = 6, ## default value is 0
    type       = &amp;quot;np&amp;quot;, 
    normal.curve = T,
    xlab = &amp;quot;tasks done&amp;quot;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf2ade3dd5_files/figure-html/unnamed-chunk-7-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggsave(&amp;quot;oswt.jpg&amp;quot;, plot = last_plot(), width = 5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the interpretation of the R-biserial correlation coefficient, as the effect size for the non-parametric One-sample Wilcoxon test. It shows a very large effect size&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/effect_size_rank_biserial.png" /&gt;&lt;/p&gt;
&lt;h2 id="whats-next"&gt;What’s next?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;first of all, never forget to cite this amazing package!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;citation(&amp;quot;ggstatsplot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  Patil, I. (2021). Visualizations with statistical details:
  The &amp;#39;ggstatsplot&amp;#39; approach. Journal of Open Source Software,
  6(61), 3167, doi:10.21105/joss.03167

A BibTeX entry for LaTeX users is

  @Article{,
    doi = {10.21105/joss.03167},
    url = {https://doi.org/10.21105/joss.03167},
    year = {2021},
    publisher = {{The Open Journal}},
    volume = {6},
    number = {61},
    pages = {3167},
    author = {Indrajeet Patil},
    title = {{Visualizations with statistical details: The {&amp;#39;ggstatsplot&amp;#39;} approach}},
    journal = {{Journal of Open Source Software}},
  }&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Need to do Chi-Square test? There is no better way than {ggbarstats} function from {ggstatsplot} package 📦. Here is [R demo on how to conduct, visualize and interpret Chi-Square test &amp;amp; pairwise post-hoc tests] (&lt;a href="https://youtu.be/8Tj0-yMPO64" class="uri"&gt;https://youtu.be/8Tj0-yMPO64&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="further-readings-and-watchings"&gt;Further readings and watchings&lt;/h2&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;a href="http://www.sthda.com/english/wiki/one-sample-t-test-in-r" class="uri"&gt;http://www.sthda.com/english/wiki/one-sample-t-test-in-r&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.instantr.com/2012/12/29/performing-a-one-sample-t-test-in-r/" class="uri"&gt;http://www.instantr.com/2012/12/29/performing-a-one-sample-t-test-in-r/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Funder, D. C., &amp;amp; Ozer, D. J. (2019). Evaluating effect size in psychological research: sense and nonsense. Advances in Methods and Practices in Psychological Science.&lt;/li&gt;
&lt;li&gt;Gignac, Gilles E, and Eva T Szodorai. 2016. “Effect Size Guidelines for Individual Differences Researchers.” Personality and Individual Differences 102: 74–78.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>ad2dcf8028cf2b8022b134c6631af623</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others</guid>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>R demo | Chi-Square Test | how to conduct, visualize &amp; interpret | + pairwise post-hoc tests</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-12-14-how-to-conduct-chi-square-test-in-r</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;p&gt;I recommend to watch a video first, because I highlight things I talk about. It’s ca. 5 minutes long.&lt;/p&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/8Tj0-yMPO64" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;Understanding &lt;a href="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/"&gt;hypothesis testing&lt;/a&gt; and &lt;a href="https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation/"&gt;p-values&lt;/a&gt; would be very helpful.&lt;/p&gt;
&lt;h2 id="how-to-conduct-chi-squared-test-in-r"&gt;How to conduct Chi-Squared test in R&lt;/h2&gt;
&lt;p&gt;If you have installed and loaded {ggstatsplot} package, you can use {ggbarstats} function to conduct and visualize Chi-Square test of independence between two categorical variables, where variables can have two or more categories. Within this function we need to specify only four arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;our data&lt;/strong&gt;, e.g. let’s take &lt;code&gt;mtcars&lt;/code&gt;, which you already have in R, so you don’t need to look for it&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt; - as one of your categorical variables, for example &lt;em&gt;Transmission&lt;/em&gt; of the car (with 0 being automatic, and 1 being manual transmission)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;y&lt;/strong&gt; - would be your second variable, let’s take the &lt;em&gt;Number of cylinders&lt;/em&gt; (4, 6 or 8) and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the label&lt;/strong&gt; argument - which displays &lt;strong&gt;both&lt;/strong&gt; numbers and percentages of observations in each category.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This simple command results in a statistically rich and publication ready plot! Now, let’s interpret the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;ggstatsplot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ggstatsplot)

ggbarstats(
  data  = mtcars, 
  x     = am, 
  y     = cyl, 
  label = &amp;quot;both&amp;quot;, 
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="filec6bf72c7535c_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2 id="interpretation"&gt;Interpretation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Chi-Square statistics&lt;/strong&gt; was previously used to manually calculate p-value, but nowadays, since p-values are always calculated by computers, we can safely ignore it&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;P-value&lt;/strong&gt; in our test can be seen as the probability of independence between two variables, low p-value (usually p &amp;lt; 0.05), like in our example, indicates that number of cylinders and transmission of cars &lt;em&gt;are dependent on each other&lt;/em&gt;. In other words - &lt;em&gt;there is a relationship between them&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Indeed, the plot shows that the number of cars using automatic transmission (am = 0) increases with increasing number of cylinders. The opposite is true for cars with manual transmission (am = 1), their frequency declines as number of cylinders increases.&lt;/p&gt;
&lt;p&gt;So, we can conclude, that &lt;em&gt;the relationship between transmission and number of cylinders exists&lt;/em&gt;. However, p-value doesn’t say &lt;em&gt;how strong this relationship is&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;That’s why we have &lt;strong&gt;V Cramer&lt;/strong&gt; value with its 95% confidence intervals as &lt;strong&gt;the effect size&lt;/strong&gt; next to p-value. Our effect size of 0.46 indicates a &lt;strong&gt;relatively strong relationship&lt;/strong&gt;, which supports the conclusion made by the p-value. The confidence intervals do not make much sense though, since &lt;em&gt;V Cramer&lt;/em&gt; goes from 0 to 1 anyway.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-14-how-to-conduct-chi-square-test-in-r/v_cramer.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;However, &lt;code&gt;ggbarstats&lt;/code&gt; also provides a second &lt;strong&gt;Bayesian V Cramer effect size&lt;/strong&gt;, which delivers much more useful 95% Highest Density Intervals. The interpretation of the Bayesian effect size is the same, so the relationship between our variables is &lt;strong&gt;relatively strong&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If that’s not enough, we can look at the &lt;strong&gt;Bayes Factor&lt;/strong&gt; (Jeffreys, 1961), which tests both null and alternative hypotheses at the same time. Bayes Factor of - 2.82 in our example indicates a &lt;strong&gt;strong evidence for the alternative hypothesis&lt;/strong&gt; - that the relationship exists, which IS in line with the frequentists statistics on the top of the plot.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-12-14-how-to-conduct-chi-square-test-in-r/bf_interpretation.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can also see &lt;strong&gt;Proportion Tests&lt;/strong&gt; for transmissions in each cylinder. They show whether proportions inside every cylinder differ. Our Null Hypothesis (&lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;) here is that there are equal proportions of different transmissions in a particular category of a cylinder. Which is the case for cylinders 4 and 6. While, our Alternative Hypothesis (&lt;span class="math inline"&gt;\(H_alt\)&lt;/span&gt;) is that the proportions differ, which is the case for the cylinder 8.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="pairwise-proportion-tests-or-post-hoc-tests"&gt;Pairwise Proportion Tests … or post-hoc tests&lt;/h2&gt;
&lt;p&gt;If you find a significant relationship between variables and you have more then two categories in any of your variables, like in our example, you might be interested to compare proportions of cylinders with each other, namely 4 with 6, 4 with 8 and 6 with 8. Such simple pairwise comparisons is often called with an unnecessary fancy name - &lt;strong&gt;post-hoc tests&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The easiest was to make pairwise proportions tests is to use {pairwise_prop_test} function from {rstatix} package. Thus, first, install and load {rstatix} package, then use {table} function for a contingency table of your variables. And finally, simply apply {pairwise_prop_test} function to your contingency table. The results show two kinds of p-values, normal and adjusted for multiple comparisons. Always use the adjusted ones. So, we see that there is a significant association between cylinders 4 and 8, where cylinder 4 has more cars with manual transmission, while cylinder 8 has more cars with automatic transmission.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;rstatix&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(rstatix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;contingency_table &amp;lt;- table(mtcars$cyl, mtcars$am)
contingency_table
pairwise_prop_test(contingency_table) &lt;/code&gt;&lt;/pre&gt;
&lt;template id="7f47d948-6096-4441-9cbd-c8cead8c79e5"&gt;&lt;style&gt;
.tabwid table{
  border-spacing:0px !important;
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
&lt;/style&gt;&lt;div class="tabwid"&gt;&lt;style&gt;.cl-73e9e61e{}.cl-73e55f18{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-73e56c38{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-73e56c42{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-73e59636{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-73e5964a{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-73e5964b{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-73e59654{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-73e5965e{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-73e5965f{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-73e59660{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-73e59668{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-73e59672{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-73e59673{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-73e5967c{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-73e5967d{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}&lt;/style&gt;&lt;table class='cl-73e9e61e'&gt;
&lt;thead&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-73e59673"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;group1&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e59673"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;group2&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e5967c"&gt;&lt;p class="cl-73e56c42"&gt;&lt;span class="cl-73e55f18"&gt;p&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e5967c"&gt;&lt;p class="cl-73e56c42"&gt;&lt;span class="cl-73e55f18"&gt;p.adj&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e5967d"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;p.adj.signif&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-73e59636"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;4&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e59636"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;6&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e5964a"&gt;&lt;p class="cl-73e56c42"&gt;&lt;span class="cl-73e55f18"&gt;0.4400&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e5964a"&gt;&lt;p class="cl-73e56c42"&gt;&lt;span class="cl-73e55f18"&gt;0.7300&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e5964b"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;ns&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-73e59654"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;4&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e59654"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;8&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e5965e"&gt;&lt;p class="cl-73e56c42"&gt;&lt;span class="cl-73e55f18"&gt;0.0108&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e5965e"&gt;&lt;p class="cl-73e56c42"&gt;&lt;span class="cl-73e55f18"&gt;0.0324&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e5965f"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;*&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style="overflow-wrap:break-word;"&gt;&lt;td class="cl-73e59660"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;6&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e59660"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;8&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e59668"&gt;&lt;p class="cl-73e56c42"&gt;&lt;span class="cl-73e55f18"&gt;0.3650&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e59668"&gt;&lt;p class="cl-73e56c42"&gt;&lt;span class="cl-73e55f18"&gt;0.7300&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class="cl-73e59672"&gt;&lt;p class="cl-73e56c38"&gt;&lt;span class="cl-73e55f18"&gt;ns&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;/template&gt;
&lt;div class="flextable-shadow-host" id="c1ccb5ef-3496-4c0f-8de3-b0a1d091c606"&gt;&lt;/div&gt;
&lt;script&gt;
var dest = document.getElementById("c1ccb5ef-3496-4c0f-8de3-b0a1d091c606");
var template = document.getElementById("7f47d948-6096-4441-9cbd-c8cead8c79e5");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
&lt;/script&gt;

&lt;h2 id="whats-next"&gt;What’s next?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;first of all, never forget to cite this amazing package!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;citation(&amp;quot;ggstatsplot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  Patil, I. (2021). Visualizations with statistical details:
  The &amp;#39;ggstatsplot&amp;#39; approach. Journal of Open Source Software,
  6(61), 3167, doi:10.21105/joss.03167

A BibTeX entry for LaTeX users is

  @Article{,
    doi = {10.21105/joss.03167},
    url = {https://doi.org/10.21105/joss.03167},
    year = {2021},
    publisher = {{The Open Journal}},
    volume = {6},
    number = {61},
    pages = {3167},
    author = {Indrajeet Patil},
    title = {{Visualizations with statistical details: The {&amp;#39;ggstatsplot&amp;#39;} approach}},
    journal = {{Journal of Open Source Software}},
  }&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;learn more about &lt;a href="https://yury-zablotski.netlify.app/post/chi-square-2/"&gt;Chi-Squared Test&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;have a look at the &lt;a href="https://yury-zablotski.netlify.app/post/goodness-of-fit/"&gt;Chi-Squared Test Goodness of Fit&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="further-readings-and-watchings"&gt;Further readings and watchings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Jeffreys, H. 1961. Theory of Probability. 3rd ed. Oxford: Oxford University Press.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>9c36df85059d7ab5d0aba54cfa61bab2</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-12-14-how-to-conduct-chi-square-test-in-r</guid>
      <pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-12-14-how-to-conduct-chi-square-test-in-r/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>R package reviews {dlookr} diagnose, explore and transform your data</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data</link>
      <description>Raw data need to be diagnosed for existing problems, explored for new hypotheses and repaired in order to increase data quality and output. The {dlookr} package makes these steps fast and easy. {dlookr} generates automated reports and performs compex operations, like imputing missing values or outliers, with simple functions. Moreover, {dlookr} collaborates perfectly with {tidyverse} packages, like {dplyr} and {ggplot2} to name just a few!</description>
      <category>EDA</category>
      <category>videos</category>
      <category>data wrangling</category>
      <category>R package reviews</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data</guid>
      <pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data/dlookr_thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Deep Exploratory Data Analysis (EDA) in R</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress</link>
      <description>Exploratory Data Analysis is an important first step on the long way to the final result, be it a statistical inference in a scientific paper or a machine learning algorithm in production. This long way is often bumpy, highly iterative and time consuming. However, EDA might be the most important part of data analysis, because it helps to generate hypothesis, which then determine THE final RESULT. Thus, in this post I'll provide the simplest and most effective ways to explore data in R, which will significantly speed up your work. Moreover, we'll go one step beyond EDA by starting to test our hypotheses with simple statistical tests.</description>
      <category>EDA</category>
      <category>videos</category>
      <category>data wrangling</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress/DEDA_thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>How to impute missing values with Machine Learning in R</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r</link>
      <description>Imputation simply means - replacing a missing value with a value that makes sense. But how can we get such values? Well, we'll use Machine Learning algorithms, because they have a high prediction power. So, in this post we'll learn how to impute missing values easily and effectively.</description>
      <category>videos</category>
      <category>data wrangling</category>
      <category>visualization</category>
      <category>machine learning</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r/thumbnail_impute_na.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Null Hypothesis, Alternative Hypothesis and Hypothesis Testing</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good</link>
      <description>Hypothesis testing is one of the most important concepts in (frequentiest) statistics and science. However, most people who test hypotheses are scientists, but not statisticians. That's why scientists often do not test hypotheses properly, without any bad intensionс. So, in this blog-post we'll break down hypothesis testing in small parts and try to properly understand every of them.</description>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>What is p-value and why we need it</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation</link>
      <description>


&lt;h2 id="this-post-as-a-video"&gt;This post as a video&lt;/h2&gt;
&lt;div class="vembedr"&gt;
&lt;div&gt;
&lt;iframe src="https://www.youtube.com/embed/Dldutns6Tig" width="533" height="300" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="previous-topics"&gt;Previous topics&lt;/h2&gt;
&lt;p&gt;Since p-values are mostly used for testing hypothesis, you should definitely check out the previous post - &lt;a href="https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/"&gt;hypothesis testing&lt;/a&gt;, where I superficially introduced the p-value already.&lt;/p&gt;
&lt;h2 id="p-value-definition-n1-its-a-probability"&gt;P-value definition N°1: it’s a probability&lt;/h2&gt;
&lt;p&gt;The most intuitive explanation of p-values I have found came from Andrew Vickers’ book “What is a p-value anyway? 34 Stories to Help You Actually Understand Statistics”: &lt;strong&gt;“P-value is the probability of the toothbrush being dry if you’ve just cleaned your teeth.”&lt;/strong&gt; Let’s start with that.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;P&lt;/strong&gt; stands for &lt;strong&gt;probability&lt;/strong&gt;. And it refers to &lt;strong&gt;the probability to observe the results we did just by chance&lt;/strong&gt;. In order to understand this definition a little better, let’s have an example:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/coin.png" /&gt;&lt;/p&gt;
&lt;p&gt;If we throw a fair coin, we have only &lt;strong&gt;2 possible outcomes&lt;/strong&gt;: heads or tales. The chances to get &lt;strong&gt;tails are obviously 50%&lt;/strong&gt;. Then we throw our coin a second time. The chances to get &lt;strong&gt;2 tails in a row are 25%&lt;/strong&gt;, because we have 4 possible outcomes after two throws, with only one of those four outcomes having &lt;strong&gt;two tails in a row&lt;/strong&gt;, so 1/4 = 0.25 = 25%. It’s actually pretty likely to get 2 tails in a row. So, nothing unusual here. The coin must be fair. But 3 tails in a row starts to feel strange. The chances to get &lt;strong&gt;3 tails in a row are low, only 12.5%&lt;/strong&gt;, but still possible. However, if we get 3 tails in a row we start to doubt whether the coin is actually fair. And if we get &lt;strong&gt;6 tails in a row despite only 1.5% probability to get them&lt;/strong&gt;, the null hypothesis - &lt;em&gt;that this is a fair coin&lt;/em&gt; - would seem ridiculous and we’ll reject it, because it’s simply &lt;strong&gt;too unlikely to happen randomly (or by chance)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/6_tails.png" /&gt;&lt;/p&gt;
&lt;p&gt;This example, which defines the p-value as &lt;strong&gt;ONE particular&lt;/strong&gt; probability of having &lt;strong&gt;only tales&lt;/strong&gt;, is clear, intuitive, but &lt;strong&gt;only ONE sided&lt;/strong&gt;. It isn’t wrong, but &lt;strong&gt;ONE-sided p-values&lt;/strong&gt; coming from &lt;strong&gt;only ONE probability&lt;/strong&gt; is not what people use. The &lt;strong&gt;“real p-values”&lt;/strong&gt; are &lt;strong&gt;TWO sided&lt;/strong&gt;. Then why did we learned about one-sided p-values at all, you might ask??? Well, it was totally necessary for a better understanding of the second definition of p-values. And this is actually one of those definitions from the book, where people start to run away from statistics :) namely:&lt;/p&gt;
&lt;div id="hello" class="greeting message" style="color: blue;"&gt;
&lt;p&gt;P.S.: &lt;strong&gt;One-sided p-values are rare and dangerous&lt;/strong&gt;, so, please don’t use them to avoid confusion, especially if you just started to learn about p-values.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="p-value-definition-n2-three-probabilities-learned-from-josh-starmer"&gt;P-value definition N°2: three probabilities (learned from &lt;a href="https://www.youtube.com/watch?v=5Z9OIYA8He8&amp;amp;ab_channel=StatQuestwithJoshStarmer"&gt;Josh Starmer!&lt;/a&gt;)&lt;/h2&gt;
&lt;p&gt;P-value is the probability to get &lt;strong&gt;(1) the data we have collected, (2) as extreme or (3) more extreme data&lt;/strong&gt; just by chance, assuming our null hypothesis is true.&lt;/p&gt;
&lt;p&gt;Two tailed p-values are determined by adding up &lt;strong&gt;three probabilities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/3_probs.png" /&gt;&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;Remember our coin example? The probability of having &lt;em&gt;two tails&lt;/em&gt; in a row is 25% (or 0.25), because there are 4 equally possible outcomes after flipping a coin 2 times. These 4 possible outcomes is our &lt;em&gt;distribution.&lt;/em&gt; And &lt;strong&gt;the data we have got&lt;/strong&gt; - &lt;em&gt;two tails&lt;/em&gt; - is only one side of this distribution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Likewise, the probability of getting &lt;em&gt;two heads&lt;/em&gt; is also 0.25, and since &lt;em&gt;two heads&lt;/em&gt; are &lt;strong&gt;as extreme&lt;/strong&gt; as &lt;em&gt;two tales&lt;/em&gt;, we have to add the probability of &lt;em&gt;two tails&lt;/em&gt; to the probability of &lt;em&gt;two heads&lt;/em&gt;: 1/4 + 1/4 = 2/4 = 0.5. Now we have a &lt;strong&gt;two sided probability&lt;/strong&gt; but the p-values still need one last part.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start="3" style="list-style-type: decimal"&gt;
&lt;li&gt;The third and the last part of the p-value is - the probability of observing &lt;strong&gt;something rarer or more extreme&lt;/strong&gt;. In this case it is 0, because &lt;strong&gt;no other outcomes are rarer then two tails or two heads&lt;/strong&gt;. Thus, if we add this 0 to the probability of having either two tales or two heads, we’ll get a final p-value of 0.5. And since our p-value is way above the significance threshold of 0.05, we failed to reject the null hypothesis, so that our null hypothesis is still TRUE - our coin must be fare.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus: the p-value is the probability of getting the &lt;strong&gt;(1) data we collected (two tails = 0.25), as extreme (two heads = 0.25) or more extreme data (0) is&lt;/strong&gt;: 0.25 + 0.25 + 0 = 0.5.&lt;/p&gt;
&lt;p&gt;Now let’s calculate the p-value for a slightly more complicated outcome: &lt;strong&gt;3 heads and 1 tail&lt;/strong&gt;. This is one of the 16 possible outcomes, when we flip the coin 4 times. The null hypothesis would be that our data - 3 heads and 1 tail - is nothing special, but belongs to this exact distribution.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/3h1t.png" /&gt;&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;The probability we randomly get &lt;strong&gt;3 heads and 1 tail&lt;/strong&gt; is 4/16 = 0.25. That would be &lt;strong&gt;our data&lt;/strong&gt; and with that &lt;strong&gt;the first part of the p-value&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The probability that we randomly get something equally rare, namely &lt;strong&gt;3 tails and 1 head&lt;/strong&gt;, is also 4/16 = 0.25, which will be &lt;strong&gt;the second part of the p-values&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The probability that we randomly get something rarer or more extreme, which in our case are &lt;strong&gt;4 heads and 4 tails&lt;/strong&gt; is 2/16 = 0.125, and are &lt;strong&gt;the last part of our p-value&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Adding them all together would result into a p-value of 0.625, so we would fail to reject the null hypothesis - that our data (3 heads and 1 tail) is nothing special, and would need to conclude that 3 heads and 1 tail belong to this distribution.&lt;/p&gt;
&lt;h2 id="p-value-definition-n3-cumulative-probability-area-under-the-curve"&gt;P-value definition N°3: cumulative probability (area under the curve)&lt;/h2&gt;
&lt;p&gt;Calculating a p-value from discrete numbers is kind of easy, but how do we do it for continuous numbers? Plotting our discrete data will help to understand that. If we plot our values from the last example, we would find the &lt;strong&gt;most probable&lt;/strong&gt; outcomes (&lt;span style="color: green;"&gt;green&lt;/span&gt;) in the middle, a bit &lt;strong&gt;less probable&lt;/strong&gt; (black and &lt;span style="color: blue;"&gt;blue&lt;/span&gt;) on both sides of the middle, and finally &lt;strong&gt;the least probable&lt;/strong&gt; values (&lt;span style="color: red;"&gt;red&lt;/span&gt;) very far away from the middle. This will become &lt;strong&gt;the probability distribution of our discrete values&lt;/strong&gt;. We can also cover plotted values with a &lt;strong&gt;curve&lt;/strong&gt; to better visualize this distribution. Now, if we mark our discrete values with discrete numbers on the x-axis, we’ll see that we’ll be able to add any &lt;strong&gt;continuous value&lt;/strong&gt; in between those discrete numbers, e.g. 0.3 or 0.333. And for every of these &lt;strong&gt;continuous value&lt;/strong&gt; we can calculate &lt;strong&gt;probability&lt;/strong&gt; in the exact same way as we just did for &lt;em&gt;2 tails&lt;/em&gt;, or &lt;em&gt;3 heads and 1 tail&lt;/em&gt;. That’s how we arrive at the &lt;strong&gt;continuous probability&lt;/strong&gt;. The p-values for such continuous probability are also calculated in the same way. But instead of adding up only 3 discrete probabilities (our data, as extreme and more extreme data), we add up 2 discrete parts (our data and similarly extreme data) and 1 continuous part (more extreme data from both tails of the distribution):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;and since we calculate probabilities of many points &lt;strong&gt;under the curve&lt;/strong&gt; and&lt;/li&gt;
&lt;li&gt;add all of them together, or &lt;strong&gt;ACCUMULATE those probabilities under the curve&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;we’ll get the &lt;strong&gt;AREA under the curve&lt;/strong&gt; - which then &lt;strong&gt;IS&lt;/strong&gt; our p-value as a &lt;strong&gt;cumulative probability&lt;/strong&gt;, including all the values which are as extreme or more extreme than our data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/distribution.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;An example of such curve could be the weight of people. For instance, the average weight of German males is 80 kilos with 95% of them weighting between 70 and 90 kilos (these numbers are totally made up ;). 2.5% of them will weight &amp;lt;70 and 2.5% will weight &amp;gt;90 kilos. If we get a new data point - namely a weight of a new person, we can calculate the p-value for that point. Our null hypothesis is - that the persons weight is &lt;strong&gt;NOT different&lt;/strong&gt; from our distribution. If this person weights 75 kilos, the area under the curve (or a p-value!), which includes all the values &lt;strong&gt;as extreme or more extreme&lt;/strong&gt; than 75, will be ca. 0.6 (or 60%), which is fairly big. So, we would fail to reject the null hypothesis, concluding that the person could be a German male, or is at least not different from German males. However, if the weight is 65 kilos, the area under the curve is quite small, let’s say 2%. Here, we can reject the null hypothesis and conclude that this person must belong to a different population, e.g. women, or a man from a different country.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So you see the p-value is not a simple probability, but 3 probabilities (parts) added together!&lt;/strong&gt; A simple probability is the number of outcomes of interest, divided by the total number of outcomes (it’s not even the one-sided p-value). While &lt;strong&gt;a p-value is the probability that random chance generated the&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;data we observed, or the data that is&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;equally rare or 3. rarer for &lt;strong&gt;discrete&lt;/strong&gt; values, or&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;equally extreme or 3. more extreme for &lt;strong&gt;continuous&lt;/strong&gt; values,&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;if the null hypothesis is true.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="p-value-definition-n4-original-fishers-definition"&gt;P-value definition N°4: original Fisher’s definition&lt;/h2&gt;
&lt;p&gt;All right, &lt;strong&gt;p-value is a cumulative probability&lt;/strong&gt;. And since a probability is a &lt;strong&gt;continues&lt;/strong&gt; measure from 0 to 1 (or from 0% to 100%), the &lt;strong&gt;p-value also IS any number between 0 &amp;amp; 1&lt;/strong&gt;. If we look for a difference between two groups, we can see a p-value &lt;strong&gt;as a measure of similarity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/similarity.png" /&gt;&lt;/p&gt;
&lt;p&gt;For examples if two samples are identical, there is &lt;strong&gt;100% similarity and 0% difference&lt;/strong&gt;. So, our p-value is equal to 1. If &lt;strong&gt;similarity is only 60%, then the difference is 40%&lt;/strong&gt;, which is much bigger then 0, but is still small. But &lt;strong&gt;if the similarity drops to 5% or below, then the difference of 95% is often considered significant&lt;/strong&gt;, and we can confidently reject the null hypothesis - that there is NO difference between samples, in favour of the alternative hypothesis - that such difference exists.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But wait!!!&lt;/strong&gt;, does it make any sense to &lt;strong&gt;dichotomize a continuous number&lt;/strong&gt; (from 0 to 1) into only two categories - &lt;strong&gt;significant and not-significant? Of coarse, not!&lt;/strong&gt; The p-value is not black and white, but everything in between … like 50 shades of gray …&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/Fifty_Shades_of_Grey_poster.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;… ups, sorry, not that 50 shades of gray … that one :) &lt;span class="citation"&gt;(&lt;a href="#ref-Sterne2001" role="doc-biblioref"&gt;Sterne, Smith, and Cox 2001&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/interpretation.png" /&gt;&lt;/p&gt;
&lt;p&gt;Hmm, but if dichotomizing continuous p-values into two categories - &lt;strong&gt;significant and not-significant&lt;/strong&gt; - doesn’t make much sense, &lt;strong&gt;why is the whole world doing exactly that&lt;/strong&gt;? “The basic explanation is neither philosophical nor scientific, but sociologic - &lt;strong&gt;everyone uses them&lt;/strong&gt;.” &lt;span class="citation"&gt;(&lt;a href="#ref-Goodman2019" role="doc-biblioref"&gt;Goodman 2019&lt;/a&gt;)&lt;/span&gt;. But in order to answer this question more thoroughly, we’d need to briefly explore the history of p-values …&lt;/p&gt;
&lt;p&gt;Ronald Fisher, the father of modern statistics, saw the &lt;strong&gt;P value as an index measuring the strength of evidence against the null hypothesis&lt;/strong&gt; (see the picture above). Fisher himself proposed: &lt;strong&gt;“if P-value is between 0.1 and 0.9 there is certainly no reason to suspect the hypothesis tested. If it’s below 0.02 it is strongly indicated that the hypothesis fails to account for the whole of the facts.”&lt;/strong&gt; He sometimes used the threshold for the p-value of 0.05 (&amp;lt; 5%) himself to reject the null hypothesis, but &lt;strong&gt;without any clear reason&lt;/strong&gt;. Moreover, he recommended to treat a &lt;strong&gt;p-value around 0.05 as inconclusive, where we’d need to repeat the experiment.&lt;/strong&gt; The reason Fisher used p-values at all was - to reduce the probability of the &lt;strong&gt;type I error&lt;/strong&gt; - the probability to find something what is not there - &lt;strong&gt;false positive&lt;/strong&gt;, e.g. that somebody has cancer, while being totally healthy. The most typical type I error happens when we measure a difference between two groups and find this difference to be significant, while both samples came from the same population:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/type_1_errors.png" /&gt;&lt;/p&gt;
&lt;p&gt;Having a stiff threshold of 0.05 can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Good&lt;/strong&gt;: because from all H0 you test, in the long run you will falsely reject at most 5% of the correct ones, but it’s also&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;BAD&lt;/strong&gt;: because it statistically guarantees that 1 in 20 healthy people, will “have” cancer just by chance, or in 1 out of 20 statistical tests we’ll find nonsense just by chance.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, &lt;strong&gt;lowering the p-value to, let’s say 0.01, reduces the probability of finding nonsense and ensures we find something what is really there&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Along the &lt;strong&gt;type I error&lt;/strong&gt;, Neyman and Pearson also advocated the &lt;strong&gt;type II error&lt;/strong&gt;, that could be made in interpreting the results of an experiment. The type II error is the probability to miss something what is there - &lt;strong&gt;false negative&lt;/strong&gt;, e.g. failing to detect cancer, when cancer is already in the body.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/type_2_errors.png" /&gt;&lt;/p&gt;
&lt;p&gt;Neyman and Pearson’s idea of &lt;strong&gt;hypothesis testing&lt;/strong&gt; was actually really good, because it supposed to reduce the number of mistakes by &lt;strong&gt;keeping the rates of both type I and type II errors low&lt;/strong&gt;. However, only the easy part of their approach — that the null hypothesis can be rejected if P &amp;lt; 0.05 (type I error rate of 5%) — has been widely adopted and stuck in the medical research, causing current dichotomy of results into &lt;strong&gt;significant&lt;/strong&gt; or &lt;strong&gt;not significant&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Two serious consequences of this are that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;potentially clinically important differences observed in small studies are denoted as non-significant and ignored, while&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;all significant findings are assumed to result from real treatment effects.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Well, how do we deal with type II errors? &lt;strong&gt;Increasing the sample size will increase the power of the experiment and therefore decrease the probability of type II error&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So, now we know that hypothesis testing can produce two types of mistakes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The type I error IS an ERROR, because p-values help to maintain nonsense&lt;/strong&gt;. While&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The type II error IS an ERROR, because p-values help to miss something important&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, &lt;strong&gt;we should stop dichotomising our results into significant and not significant&lt;/strong&gt;! But, if after reading this you still want to continue dichotomising your results into &lt;strong&gt;significant&lt;/strong&gt; or &lt;strong&gt;not significant&lt;/strong&gt;, please, answer a following question: &lt;strong&gt;is the difference between a P-value of 0.051 and 0.049 statistically significant?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The answer is - a clear NO! However, if using a p-value continuously totally blows your mind, and you still need some pragmatic decision making tool, consider using a hybrid approach, which singles out 5 instead of only 2 categories of &lt;strong&gt;the strength of the evidence against the null hypothesis in favor of the alternative:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P &amp;gt; 0.10 The data shows &lt;strong&gt;NO evidence&lt;/strong&gt; against the null hypothesis&lt;/li&gt;
&lt;li&gt;0.05 &amp;lt; P &amp;lt; 0.10 &lt;strong&gt;Weak evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as a dot: ‘.’&lt;/li&gt;
&lt;li&gt;0.01 &amp;lt; P &amp;lt; 0.05 &lt;strong&gt;Moderate evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as one star: ‘*’&lt;/li&gt;
&lt;li&gt;0.001 &amp;lt; P &amp;lt; 0.01 &lt;strong&gt;Strong evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as two stars: ‘**’&lt;/li&gt;
&lt;li&gt;P &amp;lt; 0.001 &lt;strong&gt;Very strong evidence&lt;/strong&gt; against the null hypothesis in favor of the alternative. It is sometimes reported as three stars: ‘***’&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="dont-follow-any-cut-off-the-cut-off-should-follow-you"&gt;Don’t follow any cut-off, the cut-off should follow you!&lt;/h2&gt;
&lt;p&gt;Any cut-off splits the probability into two parts:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level of confidence&lt;/strong&gt; - beta (β), e.g. 95%, tells how confident are we in our decision and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Level of significance&lt;/strong&gt; - alpha (⍺), e.g. 5%, tells us when to &lt;strong&gt;reject or fail to reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;&lt;/strong&gt;. Namely, (1) if p-value &amp;lt;= ⍺, we reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;; (2) if p-value &amp;gt; ⍺, we fail to reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both &lt;strong&gt;⍺&lt;/strong&gt; &amp;amp; &lt;strong&gt;β&lt;/strong&gt; add up to 1 and tell you the same thing - how sure are you about your decision:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for 95% confidence: ⍺ = 1 - β = 1 - 0.95 = 0.05&lt;/li&gt;
&lt;li&gt;for 99% confidence: ⍺ = 1 - β = 1 - 0.99 = 0.01&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;cut-off itself is up to you&lt;/strong&gt; and is &lt;strong&gt;highly dependent on the experiment&lt;/strong&gt;. It might sound fuzzy, but it actually gives you a freedom to adjust the decision-making to reality. On the other hand, clinging to the cut-off of 0.05 may actually increase the rates of both type I and type II errors. Let’s have a look at two examples:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;Imagine you study a new treatment for a deadly disease and your p-value for the difference between control and treatment groups is 0.15. If you strictly follow the 0.05 cut-off, you’ll fail to reject the &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; and would conclude that the treatment does not work. The p-value of 0.15 makes you “&lt;em&gt;only&lt;/em&gt;” 85% sure that such difference exists, making this difference “&lt;em&gt;not significant enough&lt;/em&gt;.” However, if you don’t &lt;span style="color: red;"&gt;blindly&lt;/span&gt; follow the 0.05, you could &lt;span style="color: blue;"&gt;look&lt;/span&gt; at your experiment in a completely new way by saying: &lt;strong&gt;I am 85% confident that new treatment works and we found a new way of treating a deadly disease&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Similarly, if I was 85% confident of winning the lottery I would definitely play! However, if I’d strictly follow the cut-off of 0.05, I would “&lt;em&gt;be not confident enough&lt;/em&gt;” that I will win (which is ridiculous) and I won’t play.&lt;/p&gt;
&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;On the opposite side, if an airline company welcomes you aboard with a catchy phrase: “We are 95% confident that you’ll survive, so your probability of crashing with us is below 5% (p &amp;lt; 0.05),” would you go aboard? I personally would run away! The airline company would chase me though and scrim: "Stop running away! We &lt;strong&gt;successfully rejected the Null Hypothesis that you will die!!!&lt;/strong&gt; But I would only be willing to fly .. if my level of confidence in survival increases to 99.9999% or if my chances to die would become 1 to a million (⍺ = 0,000001) flights.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, you see?, the freedom to set up your own level of significance (the cut-off) depending on the situation makes you a better scientist, while blindly following the cut-off of 0.05 would either keep you pure or just kill you.&lt;/p&gt;
&lt;div id="hello" class="greeting message" style="color: blue;"&gt;
&lt;p&gt;P.S.: the dramatic effect of the last sentence serves solely educational purposes … while hoping to significantly (p &amp;lt; 0.05 😉) increase the &lt;strong&gt;probability&lt;/strong&gt; that you’ll remember the material :)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Does it mean that any threshold is evil? Well, no! &lt;strong&gt;The threshold of 0.05 means that if there is no difference between Group 1 and Group 2, and if we did this exact experiment 100 times, then only 5 of these experiments would result in a wrong decision.&lt;/strong&gt; These would be 5 &lt;strong&gt;false positives&lt;/strong&gt; (type I error). Lowering the threshold to let’s say 0.001 would reduce the number of false positives to 1 out of 1000 experiments. &lt;strong&gt;But the costs of doing 1000 experiments are often much higher then it is worth.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="a-single-article-you-need-to-read-moving-to-a-world-beyond-p0.05"&gt;A single article you need to read: “Moving to a World Beyond “p&amp;lt;0.05” "&lt;/h2&gt;
&lt;p&gt;If you want to know what statisticians themselves think about p-value, read the following article: &lt;a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913"&gt;Moving to a World Beyond “p&amp;lt;0.05”&lt;/a&gt; &lt;span class="citation"&gt;(&lt;a href="#ref-Wasserstein2019" role="doc-biblioref"&gt;Wasserstein, Schirm, and Lazar 2019&lt;/a&gt;)&lt;/span&gt;. Here I would just sum up some recommendations the authors provided. These recommendations would enable you to find &lt;strong&gt;fewer false alarms (false positives, type I error), and to overlook fewer discoveries (false negatives, type II error)&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id="what-to-do"&gt;What to do:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;report precise continuous p-values&lt;/strong&gt; without reference to arbitrary thresholds. For example, P = 0.023 rather than P &amp;lt; 0.05.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;lower the 0.05 “&lt;em&gt;statistical significance&lt;/em&gt;” threshold for claims of novel discoveries to a 0.005 threshold and refer to p-values between 0.05 and 0.005 as “&lt;em&gt;suggestive&lt;/em&gt;.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;remember, &lt;strong&gt;the p-value as only one aspect of a complete data analysis&lt;/strong&gt;. Thus, supplement the p-value with visualizations of confidence intervals on effect sizes or likelihood ratios, justify the adequacy of the sample size and &lt;strong&gt;the reasons various statistical methods were employed&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;clarify objectives, invest into careful planning &amp;amp; design, invest into producing solid data, use more descriptive stats, use more of the regularized, robust, nonlinear and nonparametric methods for exploratory research, check the robustness of your methods and use several different data analysis techniques for testing the same hypothesis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;look for both (1) a small p-value and (2) a large effect size before declaring a result “significant,” instead of dichotomized p-values alone.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;interpret the p-value in the context of sample size. A large study can detect a small, clinically unimportant finding. The larger the sample the more likely a difference to be detected. Lower the p-value threshold (e.g. to 0.001) for large studies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;reduce unplanned and uncontrolled modeling/testing (p-hacking). &lt;strong&gt;Examining 20 associations will produce one result that is “significant at P = 0.05” by chance alone.&lt;/strong&gt; Thus, testing multiple variables, using long questionnaires with hundreds of variables and measuring a wide range of potential outcomes, would &lt;strong&gt;guarantee several false positive findings&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Accept uncertainty and embrace variation in effects” &lt;span class="citation"&gt;(&lt;a href="#ref-McShane2019" role="doc-biblioref"&gt;McShane et al. 2019&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="what-not-to-do"&gt;What not to do:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Don’t make decisions based solely on some arbitrary threshold such as p &amp;lt; 0.05. Dichotomizing p-values into “significant” and “non significant” one loses information. A P-value is not black and white, but ≈ 50 shades of grey 😂&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t treat &lt;strong&gt;“p = 0.051” and “p = 0.049” as being categorically different. 🙂 As &lt;span class="citation"&gt;&lt;a href="#ref-Gelman2006" role="doc-biblioref"&gt;Gelman and Stern&lt;/a&gt; (&lt;a href="#ref-Gelman2006" role="doc-biblioref"&gt;2006&lt;/a&gt;)&lt;/span&gt; famously observed, the difference between “significant” and “not significant” is not itself statistically significant.&lt;/strong&gt; Don’t discard a well-designed, excellently conducted, thoughtfully analyzed, and scientifically important experiment only because it failed to cross the threshold of 0.05 (e.g. p-value = 0.09).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t believe that an association or effect exists just because it was “statistically significant.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t believe that an association or effect is absent just because it was not “statistically significant.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t conclude anything about practical importance based on “statistical significance” (or lack thereof).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In fact - don’t say “statistically significant” at all … whether expressed in words or by asterisks in a table.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t believe that your p-value gives the probability that your test hypothesis is TRUE. &lt;strong&gt;P-value is the probability of the data, not of (any) hypothesis!&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="what-about-bayesian-statistical-inference"&gt;What about Bayesian statistical inference?&lt;/h2&gt;
&lt;p&gt;The switch to the use of Bayesian statistical inference in medical research was proposed for several decades, but is barely possible. A major reason is that &lt;strong&gt;prior knowledge can be difficult to quantify&lt;/strong&gt;. The major reason for that is that science suppose to produce NEW knowledge. Thus, repeated experiments are necessary, but few scientist want to do them. Moreover, if the priors are flat (a wide range of values is considered to be equally likely), then the &lt;em&gt;Frequentist&lt;/em&gt; and &lt;em&gt;Bayesian&lt;/em&gt; results are similar, while computing power required for Bayesian analysis is “significantly” ;) higher. Two approaches will give different results, however, if our prior opinion is not vague. Besides, we can’t ignore the fact that &lt;em&gt;Bayesian statistics&lt;/em&gt; is even less intuitive to the majority of scientists then &lt;em&gt;Frequentists statistics&lt;/em&gt;. Otherwise everybody would have already used it. Thus, until an alternative, simple and widely accepted approach to the p-values exists, banning p-values is of no help to anyone.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Most of the scientist can’t tell you what the p-value is. Most of the statisticians can, but can’t explain it to intuitively. So, don’t give yourself a hard time if you still a bit confused. It’s normal. Instead, I’d suggest we stop caring about understanding the p-value, but start caring to use it properly. And remember, &lt;strong&gt;you don’t need to understand how the car works, you only need to know how to drive&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://yuzar-blog.netlify.app//posts/2021-07-31-p-value-intuitive-explanation/impossible-parking.gif" /&gt;&lt;/p&gt;
&lt;h2 id="whats-next-to-learn"&gt;What’s next to learn?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;misinterpretations of p-values (only if you are interested)&lt;/li&gt;
&lt;li&gt;effect size and&lt;/li&gt;
&lt;li&gt;power analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;If you think, I missed something, please comment on it, and I’ll improve this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you for learning!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="further-readings-and-watchings"&gt;Further readings and watchings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;that’s an amazing video: &lt;a href="https://www.youtube.com/watch?v=tLM7xS6t4FE&amp;amp;ab_channel=SciShow" class="uri"&gt;https://www.youtube.com/watch?v=tLM7xS6t4FE&amp;amp;ab_channel=SciShow&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="http://www.stat.ualberta.ca/~hooper/teaching/misc/Pvalue.pdf" class="uri"&gt;http://www.stat.ualberta.ca/~hooper/teaching/misc/Pvalue.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-Gelman2006" class="csl-entry"&gt;
Gelman, Andrew, and Hal Stern. 2006. &lt;span&gt;“&lt;span class="nocase"&gt;The difference between "significant" and "not significant" is not itself statistically significant&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;American Statistician&lt;/em&gt; 60 (4). &lt;a href="https://doi.org/10.1198/000313006X152649"&gt;https://doi.org/10.1198/000313006X152649&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-Goodman2019" class="csl-entry"&gt;
Goodman, Steven N. 2019. &lt;span&gt;“&lt;span class="nocase"&gt;Why is Getting Rid of P-Values So Hard? Musings on Science and Statistics&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;American Statistician&lt;/em&gt; 73 (sup1). &lt;a href="https://doi.org/10.1080/00031305.2018.1558111"&gt;https://doi.org/10.1080/00031305.2018.1558111&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-McShane2019" class="csl-entry"&gt;
McShane, Blakeley B., David Gal, Andrew Gelman, Christian Robert, and Jennifer L. Tackett. 2019. &lt;span&gt;“&lt;span&gt;Abandon Statistical Significance&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;American Statistician&lt;/em&gt; 73 (sup1). &lt;a href="https://doi.org/10.1080/00031305.2018.1527253"&gt;https://doi.org/10.1080/00031305.2018.1527253&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-Sterne2001" class="csl-entry"&gt;
Sterne, Jonathan A. C., George Davey Smith, and D. R. Cox. 2001. &lt;span&gt;“&lt;span class="nocase"&gt;Sifting the evidence—what’s wrong with significance tests?&lt;/span&gt;”&lt;/span&gt; &lt;em&gt;BMJ&lt;/em&gt; 322 (7280). &lt;a href="https://doi.org/10.1136/bmj.322.7280.226"&gt;https://doi.org/10.1136/bmj.322.7280.226&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-Wasserstein2019" class="csl-entry"&gt;
Wasserstein, Ronald L, Allen L Schirm, and Nicole A Lazar. 2019. &lt;span&gt;“&lt;span class="nocase"&gt;Moving to a World Beyond "p &lt;span&gt;&amp;lt;&lt;/span&gt; 0.05"&lt;/span&gt;.”&lt;/span&gt; &lt;a href="https://doi.org/10.1080/00031305.2019.1583913"&gt;https://doi.org/10.1080/00031305.2019.1583913&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5>fbe358502d16d4b3c983e5217d5bc6d2</distill:md5>
      <category>videos</category>
      <category>statistics</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation</guid>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>R package reviews {DataExplorer} explore your data!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data</link>
      <description>What is the best way to explore the data quick? I think it's visualization. And what it the best way to visualize the data quick? I think it's - {DataExplorer} package, because it can visualize all your data in seconds using only one function! Check this out...</description>
      <category>R package reviews</category>
      <category>EDA</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data</guid>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data/2.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Survival analysis 2: parametric survival models</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models</link>
      <description>The non-parametric Kaplan-Meier method (KM) can not describe survival probability by a smooth function, which means it can not predict anything. The parametric models (e.g. Exponential, Weibull etc.) can! Besides, in case where parametric models are appropriate, they are more exact, more effective and more informative than KM or Cox. However, unfortunately, this step is often left out due to the rear use of parametric models. In this post we’ll try to close this gap.</description>
      <category>survival analysis</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models</guid>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-06-survival-analysis-2-parametric-survival-models/thumbnail_survival_2.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {performance} check how good your model is! </title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is</link>
      <description>There are several indicators of model quality, e.g. $R^2$ or AIC, and several assumption for every model which supposed to be checked, e.g. normality of residuals, multicollinearity etc.. R provides solutions for every indicator or assumption you can imagine. However, they are usually spread around different packages and functions. {performance} package brings all of quality indicators and all of the assumption under one roof. Thus, for me it became the one-stop solution for modelling.</description>
      <category>R package reviews</category>
      <category>videos</category>
      <category>visualization</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is</guid>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/14.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>Survival analysis 1: a gentle introduction into Kaplan-Meier Curves</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves</link>
      <description>Survival time analysis is necessary in any study which investigates the time to a particular outcome of interest. Cancer studies in the medicine and the first failure of the car in the engineering field (failure time analysis) are good examples. The outcome of interest could be death, remission to relapse, progression, or failure. Point in time of reaching that outcome is generally called the event. Thank goodness, not every “event” is fatal 😃, but can sometimes even be a favorable outcome such as discharge from hospital. And thus, survival analysis is also a generic term, because it is not only about survival.</description>
      <category>survival analysis</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves</guid>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves/thumbnail_survival_1.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>R package reviews {janitor} clean your data!</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data</link>
      <description>Data Scientists spend up to 80% of their time cleaning and preparing data for analysis. " Happy families are all alike; every unhappy family is unhappy in its own way" — Leo Tolstoy. "Like families, tidy datasets are all alike but every messy dataset is messy in its own way" - Hadley Wickham. Thats when "janitor" helps to clean the mess.</description>
      <category>R package reviews</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data</guid>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-02-r-package-reviews-janitor-clean-your-data/11.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>How to visualize models, their assumptions and post-hocs</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs</link>
      <description>A picture is worth a thousand words! This article shows how to visualize results of 16 different models in R: from a simple linear model to a multiple-additive-non-linear-mixed-effects model. Among them are logistic, multinomial, additive and survival models with and without interactions. **Goal: minimum R code &amp; maximum output!** We'll also go a bit beyond only model visualization. So, don't miss the bonuses 😉.</description>
      <category>visualization</category>
      <category>videos</category>
      <category>models</category>
      <guid>https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs</guid>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/thumbnail_visualize_models.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
    <item>
      <title>How to create a blog or a website in R with {Distill} package</title>
      <dc:creator>Yury Zablotski</dc:creator>
      <link>https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package</link>
      <description>If you're not online, you don't exist. A personal webpage or a blog became the business card of the digital century. It shows who you are and what you are capable of. Thus: show, don't tell.</description>
      <category>R &amp; the Web</category>
      <category>videos</category>
      <guid>https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package</guid>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <media:content url="https://yuzar-blog.netlify.app/posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/images/thumbnail.png" medium="image" type="image/png" width="1920" height="1080"/>
    </item>
  </channel>
</rss>
