---
title: "Deep Exploratory Data Analysis (EDA) in R"
description: |
  EDA is an important first step on the long way to the final result, be it a statistical inference in a published scientific paper or a Machine Learning Algorithm in production. This long way is often bumpy, highly iterative and time consuming. However, EDA might be the most important part of data analysis, because it helps to generate hypothesis, which then determine THE final RESULT. Thus, in this video I'll provide the simplest and most effective ways to explore data in R, which will signifiantly speed up your work. Moreover, we'll go one step beyond the EDA by starting to test our hypotheses by simple statistical tests.
author:
  - name: Yury Zablotski
    url: https://yury-zablotski.netlify.app/
date: 01-09-2021
categories:
  - EDA
  - videos
  - data wrangling
  - visualization
preview: 2.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 6
    code_download: true
---

```{r setup, include=FALSE, warning=F, message=F}
knitr::opts_chunk$set(echo = T)
```

You can run all packages at once, in order to avoid interruptions. You'd need to install some of them if you still aren't, using *install.packages("name_of_the_package")* command.

```{r}
library(DataExplorer)     # exploratory data analysis
library(tidyverse)        # data wrangling and visualization
library(SmartEDA)         # exploratory data analysis
library(dlookr)           # exploratory data analysis
library(flextable)        # beautifying tables
library(funModeling)      # some quick statistics
library(skimr)            # descriptive stats
library(pastecs)          # regularization, decomposition and analysis of space-time series
library(summarytools)     # descriptive stats and summaries
library(psych)
library(gtsummary)

library(datasets)         # getting the data 
library(ISLR)             # for data


library(PerformanceAnalytics)
library(ggstatsplot)
library(performance)
library(dlookr)
library(visdat)
library(car)
library(explore)
library(knitr)
library(fastStat)
```

I love R, because it is reach and generous. Having more than 17000 packages allows me to solve any data science problem. However, such abundance can be overwhelming, especially because one task can be accomplished by **different functions** from **different packages** with **different levels of effectiveness**. So, looking for the most effective way can be very time consuming! Thus, I hope that this collection of functions will save you some time. And if you know better functions or packages for EDA, please let me know in the comments below and let us together create **the one-stop solution for EDA in R**. If one of the variables is of a particular importance for you, you can specify it and get much richer report. Simply execute the code below and see it for yourself.

## Creating visualised reports of the whole dataset with only one function!

The most effective way to explore the data quick is the creation of **automated reports**. We'll have a look at three packages which are able to do this. [DataExplorer package](https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data/) package creates the best report in my opinion. [SmartEDA](https://daya6489.github.io/SmartEDA/) and [dlookr](https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data/) packages are also good choices. **Three** functions you are going to see in a moment will cover all the basics of EDA in a few seconds.

### {DataExplorer}

{DataExplorer} report will deliver **basic info** about your dataset, like number of rows and columns, number of categorical and numeric variables, number of missing values and number of complete rows. It will also show you a **missing data profile**, where percentages of missing values in every variable are displayed. It plots the **histograms** and **Quantile-Quantile plots** for every numeric variable and **bar-plots** for every categorical variable. It finally explores the combinations of different variables, by conducting **correlation analysis, principal component analysis, box and even scatter plots**.

```{r eval=F}
library(DataExplorer) # for exploratory data analysis
library(tidyverse)    # for data, data wrangling and visualization

# report without a response variable
create_report(diamonds)

# report with a response variable
create_report(diamonds, y = "price")
```

### {SmartEDA}

In addition to the similar results, {SmartEDA} report also delivers **descriptive statistics** for every numeric variable with all important metrics you could need, like **number of negative values, number of zeros, mean, median, standard deviation, IQR, bunch of different quantiles and even the number of outliers**. {SmartEDA} also displays the **density** of every numeric variables instead of histograms. And while {DataExplorer} package can visualize density too, density plots are not part of the automated {DataExplorer} report.

What I found particularly useful in {SmartEDA} report is that **it provides the code** responsible for a particular result. For instance, if I don't need the whole report, but wanna see descriptive stats, I just copy the code, change the name of my dataset instead of generic "data" and get the same table I see in the report without looking for such code in the documentation. **It saves time!** Moreover, in {SmartEDA} package, you can give your report a name and save it in the directory of your choice.

```{r eval=F}
library(SmartEDA) # for exploratory data analysis

ExpReport(diamonds, op_file = 'smarteda.html')
```

### {dlookr}

One of the most amazing features of {dlookr} package is that {dlookr} perfectly collaborates with {tidyverse} packages, like {dplyr} and {ggplot2}. This elevates the output of {dlookr} on another level. We'll see some examples here. Another advantage of *{dlookr}* package is that you can choose the output to be a PDF (by default) or HTML files. Moreover, it also separates between three kinds of reports: **diagnose report, EDA report and transformation report**. 

The **diagnose report** delivers:

- the number of missing and unique values,  
- counts, proportions and ranks of categorical variables,
- descriptive stats, number of zeros, negative values and number of outliers of numeric variables and finally
- visualizes every numeric variables with and without outliers

```{r eval=F}
library(dlookr) # for exploratory data analysis

# report without a response variable
diagnose_report(diamonds) # pdf by default
```

Similarly to the {DataExplorer} report, we can get much richer **EDA report** from {dlookr} by specifying the target variable. Let's export this report in the HTML format. This **EDA report** delivers:

- **visual normality tests** of all numerical variables with **a histogram and a Quantile-Quantile plot** plus two **histograms** of the most common **transformations of data**, namely **log- and square-root**. This is quite useful, because we can immediately see whether we need to transform the data and which type of transformation is more useful;
- this report also provides **correlation coefficients and plots** for all possible combinations of numeric variable, then
- since our target variable is categorical, EDA report provides **descriptive statistics for every category of our target variable vs. every numeric variable**, and **contingency tables for with categorical variables**;
- it also visualizes the distribution of every other variable vs. the target variables using **box-plots and density plots, with and without outliers**.

```{r eval=F}
# report with a response variable and some dplyr sintax
diamonds %>%
  eda_report(
    target        = cut, 
    output_format = "html", 
    output_file   = "EDA_diamonds.html")
```



Finally the **transformation report**, which is my **absolute favorite**:

- **imputes missing values** with multiple methods simultaneously, so that you can compare the distribution of the data after different imputation techniques and choose the best imputation method for every particular dataset,
- **imputes outliers**, also with different methods,
- **resolve skewness** of the data with different methods and even
- is able to **categorize numeric variables**, if needed.


```{r eval=F}
# example with missing values
transformation_report(airquality, target = Temp)

# example with outliers
transformation_report(diamonds, target = price)
```


## Big Picture of your data and some quick 

Big reports might be overwhelming though, and we often need only a particular aspect of data exploration. Fortunately, you can get any part of the big report separately. For instance, the basic description for *airquality* dataset can be reached via functions *introduce()* and *plot_intro()* from {DataExplorer} package. 

### {DataExplorer}

```{r cache=T}
introduce(airquality) %>% t() 

plot_intro(airquality)
```


### {funModeling}

{funModeling} package provides a similar function with some useful metrics, like **number of zeros, NAs or unique values** for every variable. 

```{r cache=T}
library(funModeling)
status(airquality) %>% flextable()
```


## Explore categorical (discrete) variables

### {DataExplorer}

Simple bar plots with frequency distribution of all numeric variables are already quite useful, because they provide a quick overview about the meaningfulness of the categorization, and whether there are some typing mistakes in the data. However, plotting a target discrete variable by another discrete variable is even more useful. It is some sort of a visual contingency table (see the second plot below).

```{r cache=TRUE}
plot_bar(diamonds)
plot_bar(diamonds, by = "cut")
```


### {SmartEDA}

*ExpCatViz* from *{SmartEDA}* package also plots each categorical variable with a bar plot, but displays percents instead of counts. 

```{r fig.width=10}
ExpCatViz(diamonds, Page = c(1,3))
```

And here we finally come to the **DEEP** part of EDA. The plot below nearly "scrims" the hypothesis that education level is strongly associated with the job. Namely, the more educated we get, the more likely we'll end up working with information (e.g. with data ;) and the less likely we'll end up working in a factory. However, without a proper statistical test and a p-value this hypothesis can not be tested and remain, well ... a speculation. 

```{r cache=T}
## Stacked bar graph
library(ISLR)      # for the Wage dataset
ExpCatViz(
  Wage %>% 
    select(education, jobclass), 
  target="education")
```

### {ggstatsplot}

Fortunately, the {ggstatsplot} package does all the above in one line of code and goes one step further. Namely:

- it **counts and calculates percentages** for every category,
- it **visualizes** the "frequency table" in the form of **stacked bar plots** and
- **provides numerous statistical details (including p-value) in addition to visualization**, which allows us to make a conclusion or inference already in the exploratory phase of the project!

```{r cache=TRUE, fig.width=8}
library(ggstatsplot)
ggbarstats(
  data  = Wage, 
  x     = jobclass, 
  y     = education, 
  label = "both")
```


## Descriptive stats

### {dlookr}

Descriptive statistics is usually needed for either a whole numeric variable, or for a numeric variable separated in groups of some categorical variable, like *control & treatment*. Three functions from {dlookr} package, namely *describe()*, *univar_numeric()* and *diagnose_numeric()* do totally nail it. Be careful with the *describe()* function though, because it also exists in {Hmisc} and {psych} packages too. Thus, in order to avoid the confusion, simply write **dlookr::** in front of *describe()*. Here, we can see how useful can be the collaboration of *{dlookr}* with *{tidyverse}* packages, like *{dplyr}* and its *group_by()* function! {dlookr} provides the most common descriptive stats, like **N, NAs, mean, sd, se_mean, IQR, skewness, kurtosis and 17 quantiles**. 


```{r, layout="l-screen-inset"}
library(flextable)        # beautifying tables
dlookr::describe(iris) %>% flextable()
```

If you don't need such a monstrous table, but only want to have the *median()* instead of all those quantiles, use *univar_numeric()* function.

```{r}
iris %>% 
  group_by(Species) %>% 
  univar_numeric() %>% 
  knitr::kable()
```

*diagnose_numeric()* function reports the usual 5-number-summary (which is actually a box-plot in a table form) and the **number of zeros, negative values and outliers**.

```{r}
iris %>% 
  diagnose_numeric() %>% 
  flextable()
```


### {SmartEDA}

{SmartEDA} with its *ExpNumStat()* function provides richest and most comprehensive descriptive statistics table. Moreover we can choose to describe the whole variables, grouped variables, or even both at the same time. If we call the argument "**by =**" with a big letter **A**, we'll get statistics for every numeric variable in the dataset. The big **G** delivers descriptive stats per **GROUP**, but we'll need to specify a group in the next argument "**gr =**". Using **GA**, as shown below, would give you both. We can specify the quantiles we need and identify the lower hinge, upper hinge and number of outliers. 



```{r, layout="l-screen-inset"}
ExpNumStat(iris, by="GA", gp="Species", Outlier=TRUE, Qnt = c(.25, .75), round = 2) %>% flextable()
```

### {summarytools} and {psych}

{summarytools} and {psych} packages also provide useful tables with descriptive stats, but since they do not offer anything dramatically new compared to functions presented above, I'll just provide the code but would not display the results. By the way, **summarytools** sounds like a good topic for the next chapter...

```{r eval=FALSE}
library(summarytools)
iris %>% 
  group_by(Species) %>% 
  descr()

library(psych)
describeBy(iris,
           iris$Species)
```


## Summary tools

This topic can be singled out because functions presented below give you a quick overview about the whole dataset and some of them also test hypothesis with simple statistical tests.

### {summarytools}

For instance, *dfSummary()* function from {summarytools} package provides some basic descriptive stats for numeric and counts with proportions for categorical variables. It even tries to somehow plot the distribution of both, but I found it very weird and not too useful. What is useful though, is that *dfSummary()* provides a number of duplicates and missing values.

```{r}
dfSummary(diamonds)
```

### {skimr}

{skimr} is another useful package which provides both some basic descriptive stats of numeric variables, and counts for categorical variables. Besides, it is also able to use {dplyr's} *group_by()* function (not shown).

```{r}
library(skimr)
skim(diamonds)
```



### {gtsummary}

First of all, *tbl_summary()* function from {gtsummary} package summarizes all categorical variables by **counts and percentages**, while all numeric variables by **median and IQR**. The argument "**by =**" inside of *tbl_summary()* specifies a grouping variable. The *add_p()* function then **conducts statistical tests with all variables and provides p-values**. For numeric variables it uses the **non-parametric Wilcoxon rank sum test** for comparing two groups and the **non-parametric Kruskal-Wallis rank sum test** for more then two groups. Categorical variables are checked with **Fisher's exact test**, if number of observations in any of the groups is below 5, or with **Pearson's Chi-squared test** for more data.

```{r}
library(gtsummary)

mtcars %>% 
  select(mpg, hp, am, gear, cyl) %>% 
  tbl_summary(by = am) %>% 
  add_p()
```


```{r, layout="l-screen-inset"}
Wage %>%
  select(age, wage, education, jobclass) %>% 
  tbl_summary(by = education) %>% 
  add_p()
```






## Explore distribution of numeric variables


{DataExplorer} package provides very intuitive functions for getting histogram and density plots of all continuous variables at once, namely *plot_histogram()* and *plot_density()*. {SmartEDAs} function *ExpNumViz()* provides nice looking density plots with **Skewness** and **Kurtosis**. Why do need to see the distribution. Well, for example to check the normality of distribution. Namely, if the density curve looks like a bell, the data is normally distributed and we can use parametric tests, like t-test or ANOVA. If, however, the curve looks not like a bell, the data is probably not-normally distributed and we'd need non-parametric tests, like Mann-Whitney or Kruskal-Wallis. Even better way to check for normality of data is to look at the Quantile-Quantile plot.

### {DataExplorer}

```{r cache=TRUE}
plot_histogram(iris)

plot_density(iris)
```



### {SmartEDA}

```{r}
ExpNumViz(iris, Page = c(2,2))
```

## Check the normality of distribution

{DataExplorer} provides a simple and elegant *plot_qq()* function, which produces **Quantile-Quantile plots** either for all continuous variables in the dataset, or, even for every group of a categorical variable, if the argument **by = ** is specified.

### {DataExplorer}

```{r cache=TRUE}
plot_qq(iris)

plot_qq(iris, by = "Species")
```


### {ggpubr}

{ggpubr} package goes one step further and shows the confidence intervals, which help to decide whether the deviation from normality is big or not. However, in order to properly check the normality, we'd need to actually do a statistical test for normality, which is in most cases a Shapiro-Wilk test.

```{r cache=TRUE}
library(ggpubr)
ggqqplot(iris, "Sepal.Length", facet.by = "Species")
```


### {dlookr}

*normality()* function from {dlookr} package performs Shapiro-Wilk normality tests with every numeric variable in the dataset. Moreover, via the collaboration with {dplyr} and therefore *group_by()* function we cat conduct ca. 2000 normality tests in seconds:

```{r}
normality(iris) %>% flextable()
```


```{r cache=TRUE}
diamonds %>%
  group_by(cut, color, clarity) %>%
  normality()
```

Moreover, **plot_normality()** function from {dlookr} package **visualizes** not only **Histogram** and **Quantile-Quantile of the original data**, but also **two of the most common transformations of data**, namely **log transformation & square root**, in case the normality assumption wasn't met. This allows us to see, whether transformation actually improves something or not. Here we could again use *group_by()* function in order to quickly visualize several groups.

```{r cache=TRUE}
iris %>%
  group_by(Species) %>%
  plot_normality(Petal.Length)
```



## Visualize correlation

### {DataExplorer}

```{r cache=TRUE}
## View overall correlation heatmap
plot_correlation(na.omit(airquality), type = "c")
```

### {dlookr}

```{r cache=TRUE}
correlate(airquality, Ozone)

plot_correlate(airquality, method = "kendall")

diamonds %>%
  filter(cut %in% c("Premium", "Ideal")) %>% 
  group_by(cut) %>%
  plot_correlate()
```

### {ggstatsplot}

```{r cache=TRUE}
library(ggstatsplot)

ggcorrmat(airquality)

ggcorrmat(
  data = airquality,
  type = "np",
  output = "dataframe"
) %>% flextable()
```


### {PerformanceAnalytics}

Correlation coefficients and correlation color are amazing, they give us an idea about where there is a potentias for further exploration. However, they are quite limited though. Firstly, we don't really know whether these correlations are significant, so there are no tests behind it. Secondly, we don't even know what correlation method produced these coeffitients in the first place, was it the Pearson correaltion, or Spearman. Finally we don't see how data are scattered, so that some relationships might be very non-linear and correlation analysis would be not appropriate at all. The solution for all three problems is provided by the *PerformanceAnalytics* package, which offers a *chart.Correlation()* function. It produces histograms for every particular numeric variable and the scatterplots for every combination of numeric variables. I found the significance stars particularly helpful. Besides, we can easily choose the method we measure the correlation by, for instance, in the example below we use a robust non-parametric *kendall* correlation which is more appropriate for non-normally and non-too-linearly distributed values with some outliers. If we do not specify the **method**, we'll produce a **parametric (usual) Pearson correlation** which is only appropriate for perfect data, which ... rarely happens.

```{r cache=TRUE}
PerformanceAnalytics::chart.Correlation(airquality %>% select(-Month), method = "kendall") 
```



## Visualize box plots

### {DataExplorer}

```{r cache=TRUE}
## View bivariate continuous distribution based on `cut`
plot_boxplot(airquality, by = "Month")
```

### {SmartEDA}

```{r cache=TRUE}
ExpNumViz(iris, target = "Species")
```

### {ggstatsplot}

```{r cache=TRUE}
ggbetweenstats(
  data = iris, 
  x    = Species, 
  y    = Sepal.Width, 
  type = "np")
## I did 3 videos about these fancy box plots with statistics
```



## Visualize outlier

The *performance* package provide an easy way to visualize outliers.

```{r cache=TRUE}
check_outliers(airquality$Ozone)
plot(check_outliers(airquality$Ozone, method = "iqr"))
```

### {dlookr}

```{r  cache=TRUE}
diagnose_outlier(airquality)

plot_outlier(airquality, Ozone)

airquality %>% 
  dplyr::select(Ozone, Wind) %>% 
  plot_outlier()

# Visualize variables with a ratio of outliers greater than 3%
diamonds %>%
  plot_outlier(diamonds %>%
      diagnose_outlier() %>%
      filter(outliers_ratio > 3) %>%
      select(variables) %>%
      pull())
```

## Missing values

I found this topic actually so cool, that I produced a separate video and [article](https://yuzar-blog.netlify.app/posts/2021-03-04-how-to-impute-missing-values-in-r/) on it. Here I just give you the glimpse on it. For instance 

```{r}
# “knn” : K-nearest neighbors
plot(imputate_na(airquality, Ozone, Temp, method = "knn"))
```


## Impute outliers

*{dlookr}* package also provides an opportunity to impute outliers using several methods:

- "mean" : arithmetic mean
- "median" : median
- "mode" : mode
- "capping" : Impute the upper outliers with 95 percentile, and Impute the bottom outliers with 5 percentile.

Moreover, we can compare the results of a column before and after imputation via a plain table or via a plot.

```{r cache=TRUE}
bla <- imputate_outlier(diamonds, carat, method = "capping")
summary(bla)
plot(bla)
```





## Scatter plots

```{r cache=TRUE}
plot(iris)
```

### {SmartEDA}

```{r cache=TRUE}
airquality %>% 
  select(1:3) %>% 
ExpNumViz(scatter=TRUE)
```

### {dlookr}

The *compare_numeric()* function examines the relationship between numerical variables with the help of (Pearson's) correlation and simple linear models.

```{r cache=TRUE}
compare_numeric(iris) %>% plot()

compare_numeric(iris)
```


### fastStat

```{r cache=TRUE}
iris %>% select_if(is.numeric) %>% cor_sig_star(method = "kendall")
```


## Exploratory modelling

```{r cache=TRUE}
ggplot(airquality, aes(Solar.R, Temp))+
  geom_point()+
  geom_smooth()+
  facet_wrap(~Month)
  

ggplot(diamonds, aes(price, carat))+
  geom_point()+
  geom_smooth()+
  facet_grid(cut ~ clarity)
```


## Data transformation




### Dummify

```{r cache=TRUE}
## Dummify diamonds dataset
glimpse(dummify(diamonds, select = "cut"))
glimpse(dummify(diamonds))
```


### Normalize the variables

```{r eval=F}
tr <- transform(airquality$Ozone, method = "log+1")
summary(tr)
plot(tr)

transformation_report(airquality)
```


### funModeling

Convert a numeric vector into a scale from 0 to 1 with 0 as the minimum and 1 as the maximum.

```{r cache=TRUE}
range01(airquality$Ozone) %>% densityplot()
```



### Categorize numeric variables

The binning() converts a numeric variable to a categorization variable. Choose from "quantile", "equal", "equal", "pretty", "kmeans" and "bclust". The "quantile" sets breaks with quantiles of the same interval. The "equal" sets breaks at the same interval. The "pretty" chooses a number of breaks not necessarily equal to nbins using base::pretty function. The "kmeans" uses stats::kmeans function to generate the breaks. The "bclust" uses e1071::bclust function to generate the breaks using bagged clustering. "kmeans" and "bclust" was implemented by classInt::classIntervals function.

```{r  cache=TRUE}
bin <- binning(airquality$Ozone, type = "bclust", nbins = 4,
              labels = c("LQ1", "UQ1", "LQ3", "UQ3"))

summary(bin)

plot(bin)
```



--------------------------------

If you think, I missed something, please comment on it, and I'll improve this tutorial.

**Thank you for learning!**

## Further readings and references

- One of the best places to learn R is R-Bloggers platform: http://www.R-bloggers.com 

- https://www.groundai.com/project/the-landscape-of-r-packages-for-automated-exploratory-data-analysis/1