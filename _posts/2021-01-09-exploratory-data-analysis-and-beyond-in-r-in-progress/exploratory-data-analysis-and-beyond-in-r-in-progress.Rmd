---
title: "Deep Exploratory Data Analysis (EDA) in R: or one-stop solution for EDA in R (in progress)"
description: |
  Exploratory Data Analysis (EDA) is an important first step on the long way to the final result, be it statistical inference in a published scientific paper or a machine learning algorithm in production. This way between EDA and final result is often rocky, bumpy, annoying and highly iterative. However, EDA is sometimes the most important and the most time consuming part of data analysis, because it helps to generate hypothesis, which then determine the final result. Unfortunatly EDA often remains unreported, simply because everyone focuses on the final result. So that we often don't know how EDA was conducted. Thus, in this article we'll provide the best (to my knowledge) ways to explore the data in R, which will signifiantly speed up and easy data exploration. Moreover we'll go one step beyond the EDA towards generating hypotheses by using some simple statistical tests. 
author:
  - name: Yury Zablotski
    url: https://yury-zablotski.netlify.app/
date: 01-09-2021
categories:
  - EDA
  - videos
  - data wrangling
  - visualization
preview: 2.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    code_download: true
---


```{r setup, include=FALSE, warning=F, message=F}
knitr::opts_chunk$set(echo = T)
```


```{r}
# install.packages("tidyverse")
# install.packages("DataExplorer")

library(tidyverse)        # for data wrangling and visualization
library(datasets)         # for getting the data 
library(ISLR)
library(DataExplorer)     # for exploratory data analysis
library(SmartEDA)         # for exploratory data analysis
library(skimr)            # for descriptive stats
library(PerformanceAnalytics)
library(ggstatsplot)
library(performance)
library(pastecs)
library(dlookr)
library(visdat)
library(summarytools)
library(explore)
library(knitr)
```

I love R, because it is reach and having ca. 17000 packages allows me to concur almost any data science problem. However, such abundance can be overwhelming, especially because one task can be accomplished by different functions from different packages with different levels of effectiveness. Looking for the most effective way can be very time consuming! So, I hope this collection will save you some time. And if you know better functions or packages for EDA, than those provided here, please let me know in the comments below and let us together create a one-stop solution for EDA in R.

## Creating visualised reports of the whole dataset with only one function!

The most effective way to explore the data fast is a creation of automated reports. [DataExplorer package](https://yuzar-blog.netlify.app/posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data/) creates the best reports in my opinion. [SmartEDA](https://daya6489.github.io/SmartEDA/) package is another good choice. 

### DataExplorer

```{r eval=F}
airquality <- airquality %>% 
  mutate(Month = factor(Month))

# report without a response variable
create_report(airquality)

# report with a response variable
create_report(diamonds, y = "price")
```

### SmartEDA

```{r eval=F}
ExpReport(airquality, op_dir  = 'other_docs/', op_file = 'smarteda.html')
```

### dlookr

The advantage of *dlookr* package is that you can choose to output to pdf (by default) and html files. Moreover, you can determine the output name, directory and even the font family.

```{r eval=F}
eda_report(airquality)

diamonds %>% 
  eda_report(price)
```


## Big Picture of your data, or some quick descriptive stats

Instead of running *create_report()*, you may also run each function individually for your analysis, e.g. view basic description for *airquality* data.

### DataExplorer

```{r}
introduce(airquality)

plot_intro(airquality)
```

### SmartEDA

```{r}
ExpData(data=diamonds,type=1)
ExpData(data=diamonds,type=2)
```
### dlookr

```{r}
diagnose(airquality) %>% 
  kable()
```


```{r eval=F}
diagnose_report(airquality)
```



### skimr

```{r}
skim(diamonds)
```

### pastecs

```{r}
stat.desc(airquality)
```


### summarytools

```{r}
library(summarytools)
dfSummary(diamonds)
freq(diamonds)
descr(airquality)
```

### psych

```{r}
library(psych)
describeBy(diamonds,
           diamonds$cut) # grouping variable
```

### dlookr

```{r}
dlookr::describe(airquality)

diamonds %>% 
  dplyr::select(price, cut, carat) %>% 
  group_by(cut) %>% 
  dlookr::describe() %>% 
  dplyr::select(variable, cut, n, na, mean, sd, IQR)

diamonds %>% 
  dplyr::select(price, cut, carat) %>% 
  group_by(cut) %>%
  univar_numeric()
```

### Hmisc

```{r}
Hmisc::describe(mpg)
```



### gtsummary

```{r}
library(gtsummary)
diamonds %>%
  tbl_summary(by = cut) %>% 
  bold_labels() %>% 
  add_p()
```


## View missing value distribution for airquality data

### DataExplorer

```{r}
plot_missing(airquality)
```

### dlookr

Plot The Combination Variables That Is Include Missing Value
Visualize the combinations of missing value across cases.

```{r}
plot_na_pareto(airquality)
plot_na_hclust(airquality)
plot_na_intersect(airquality)
```

### visdat

```{r}
vis_dat(airquality)

plotly::ggplotly(vis_miss(airquality))

vis_miss(airquality, 
         cluster = TRUE)

vis_guess(iris)
plotly::ggplotly(vis_guess(airquality))
```


## Quickly impute missing values

### dlookr

*Dlookr* package provide numerous methods to impute missing values. However, it's not recommenden to use any specific number, like mean or median. Two of the methods from *Dlookr* package are particularly useful,
- "rpart" : Recursive Partitioning and Regression Trees
- "mice" : Multivariate Imputation by Chained Equations
because they can be applied for both numerical and categorical variables, while additionally, the 
- "knn" : K-nearest neighbors 
method can be used for only numeric columns.

One thing I found particularly interesting: we can use several methods and compare them visually. This gives us a choice to use the method which does not change the distribution too much or too weirdly. The example with *median* shown below is a weard one, so that I'd prefer to use *rpart* or *mice*.



```{r}
bla <- imputate_na(airquality, Ozone, Temp, method = "rpart")
summary(bla)
plot(bla)

blup <- imputate_na(airquality, Ozone, Temp, method = "mice")
summary(blup)
plot(blup)

blip <- imputate_na(airquality, Ozone, Temp, method = "median")
summary(blip)
plot(blip)
```



## Visualise distribution of categorical (discrete) variables

### DataExplorer

```{r}
plot_bar(diamonds)

plot_bar(diamonds, with = "price")

## View frequency distribution by a discrete variable
plot_bar(diamonds, by = "cut")
```

### dlookr

```{r}
compare_category(.data = diamonds, cut, clarity) %>% plot()

diagnose_category(diamonds)
```



### SmartEDA

```{r}
ExpCatViz(diamonds)

## Stacked bar graph
ExpCatViz(diamonds,target="cut",fname=NULL,clim=10,col=NULL,margin=2,Page = c(2,1),sample=2)

## Frequency or custom tables for categorical variables
ExpCTable(diamonds, Target = "price")

## Summary statistics of categorical variables
ExpCatStat(diamonds, Target="price",result = "Stat")

## Inforamtion value and Odds value
ExpCatStat(diamonds, Target="price",result = "IV")

## Variable importance graph using information values
ExpCatStat(Carseats, Target="Urban", result = "Stat", clim=10, nlim=5, bins=10, Pclass="Yes", plot=TRUE, top=10, Round=2)
```

### ggstatsplot

```{r}
ggbarstats(data = diamonds, x = cut, y = clarity)
```


## Explore numeric variables

### dlookr

Besides the usual descriptive statistics, *diagnose_numeric()* reports the number of zeros, negative values and outliers.

```{r}
diagnose_numeric(airquality)
```


### DataExplorer

```{r}
## View histogram of all continuous variables
plot_histogram(airquality)

## View estimated density distribution of all continuous variables
plot_density(airquality)

## View quantile-quantile plot of all continuous variables
plot_qq(airquality)

## View quantile-quantile plot of all continuous variables by feature `cut`
plot_qq(airquality, by = "Month")
```

### SmartEDA

```{r}
## View estimated density distribution of all continuous variables
ExpNumViz(airquality)

## View quantile-quantile plot of all continuous variables
ExpOutQQ(airquality)
```


### ggpubr

```{r}
library(ggpubr)
ggqqplot(airquality$Temp)
ggqqplot(airquality$Temp[airquality$Month == 5])
```

### car

```{r}
car::qqPlot(mtcars$mpg, groups = mtcars$cyl)
```

### dlookr

Shapiro-Wilk normality test is performed.

```{r}
normality(airquality)

# amazingly it supports dplyr and therefore group_by
mtcars %>%
  group_by(am, cyl) %>%
  normality(mpg, wt) %>% 
  arrange(p_value)

# get ca. 2000 tests in seconds
diamonds %>% 
  group_by(cut, color, clarity) %>% 
  normality()

plot_normality(airquality, Ozone, Wind)

airquality %>% 
  filter(Month %in% c(7, 9)) %>% 
  group_by(Month) %>%
  plot_normality(Ozone)

method = c("zscore", "minmax", "log", "log+1", "sqrt", "1/x", "x^2", "x^3")
transform(airquality$Ozone, method = "log+1") %>% plot()
transform(airquality$Ozone, method = "log+1") %>% summary()
```


## Visualize correlation

### DataExplorer

```{r}
## View overall correlation heatmap
plot_correlation(na.omit(airquality), type = "c")
```

### dlookr

```{r}
correlate(airquality, Ozone)

plot_correlate(airquality, method = "kendall")

diamonds %>%
  filter(cut %in% c("Premium", "Ideal")) %>% 
  group_by(cut) %>%
  plot_correlate()
```

### PerformanceAnalytics

Correlation coeffitients and correlation color are amazing, they give us an idea about where there is a potentias for further exploration. However, they are quite limited though. Firstly, we don't really know whether these correlations are significant, so there are no tests behind it. Secondly, we don't even know what correlation method produced these coeffitients in the first place, was it the Pearson correaltion, or Spearman. Finally we don't see how data are scattered, so that some relationships might be very non-linear and correlation analysis would be not appropriate at all. The solution for all three problems is provided by the *PerformanceAnalytics* package, which offers a *chart.Correlation()* function. It produces histograms for every particular numeric variable and the scatterplots for every combination of numeric variables. I found the significance stars particularly helpful. Besides, we can easily choose the method we measure the correlation by, for instance, in the example below we use a robust non-parametric *kendall* correlation which is more appropriate for non-normally and non-too-linearly distributed values with some outliers. If we do not specify the methos, we'll produce a parametric (usual) Pearson correlation which is only appropriate for perfect data, which ... rarely happens.



```{r}
PerformanceAnalytics::chart.Correlation(airquality %>% select(-Month)) 
# method = "kendall" or "spearman"
PerformanceAnalytics::chart.Correlation(airquality %>% select(-Month), method = "kendall") 
```



## Visualize box plots

### DataExplorer

```{r}
## View bivariate continuous distribution based on `cut`
plot_boxplot(airquality, by = "Month")
```

### SmartEDA

```{r}
ExpNumViz(airquality, target = "Month")
```

### ggstatsplot

```{r}
ggstatsplot::ggbetweenstats(data = airquality, x = Month, y = Ozone, type = "np")
## I did 3 videos about these fancy box plots with statistics

```

## Visualize outlier

The *performance* package provide an easy way to visualize outliers.

```{r}
check_outliers(airquality$Ozone)
plot(check_outliers(airquality$Ozone, method = "iqr"))
```

### dlookr



```{r}
diagnose_outlier(diamonds)

plot_outlier(airquality, Ozone)

airquality %>% 
  dplyr::select(Ozone, Wind) %>% 
  plot_outlier()

# Visualize variables with a ratio of outliers greater than 3%
diamonds %>%
  plot_outlier(diamonds %>%
      diagnose_outlier() %>%
      filter(outliers_ratio > 3) %>%
      select(variables) %>%
      pull())
```


## Impute outliers

*Dlookr* package also provides an opportunity to impute outliers using several methods:

- "mean" : arithmetic mean
- "median" : median
- "mode" : mode
- "capping" : Impute the upper outliers with 95 percentile, and Impute the bottom outliers with 5 percentile.

Moreover, we can compare the results of a column before and after imputation via a plain table or via a plot.

```{r}
bla <- imputate_outlier(diamonds, carat, method = "capping")
summary(bla)
plot(bla)
```



## Scatter plots

```{r}
plot(iris)
```

### SmartEDA

```{r}
airquality %>% 
  select(1:3) %>% 
ExpNumViz(scatter=TRUE)
```

### dlookr

The *compare_numeric()* function examines the relationship between numerical variables with the help of (Pearson's) correlation and simple linear models.

```{r}
compare_numeric(iris) %>% plot()

compare_numeric(iris)
```


## Exploratory modelling

```{r}
ggplot(airquality, aes(Solar.R, Temp))+
  geom_point()+
  geom_smooth()+
  facet_wrap(~Month)
  

ggplot(diamonds, aes(price, carat))+
  geom_point()+
  geom_smooth()+
  facet_grid(cut ~ clarity)
```


## Data transformation

### Dummify

```{r}
## Dummify diamonds dataset
glimpse(dummify(diamonds, select = "cut"))
glimpse(dummify(diamonds))
```


### Normalize the variables

```{r eval=F}
tr <- transform(airquality$Ozone, method = "log+1")
summary(tr)
plot(tr)

transformation_report(airquality)
```

### Categorize numeric variables

The binning() converts a numeric variable to a categorization variable. Choose from "quantile", "equal", "equal", "pretty", "kmeans" and "bclust". The "quantile" sets breaks with quantiles of the same interval. The "equal" sets breaks at the same interval. The "pretty" chooses a number of breaks not necessarily equal to nbins using base::pretty function. The "kmeans" uses stats::kmeans function to generate the breaks. The "bclust" uses e1071::bclust function to generate the breaks using bagged clustering. "kmeans" and "bclust" was implemented by classInt::classIntervals function.

```{r}
bin <- binning(airquality$Ozone, type = "bclust", nbins = 4,
              labels = c("LQ1", "UQ1", "LQ3", "UQ3"))

summary(bin)

plot(bin)
```



--------------------------------

If you think, I missed something, please comment on it, and I'll improve this tutorial.

**Thank you for learning!**

## Further readings and references

- One of the best places to learn R is R-Bloggers platform: http://www.R-bloggers.com 

- https://www.groundai.com/project/the-landscape-of-r-packages-for-automated-exploratory-data-analysis/1