---
title: "4 Reasons Non-Parametric Bootstrapped Regression is Better then Ordinary"
description: |
  If the assumptions of parametric models can be satisfied, it's better to use parametric models. However, there are many assumptions and they are rarely all satisfied. So, data trasformation or using non-parametric methods are the solutions. In this post we'll learn the Non-Parametric Bootstrapped Regression as an alternative for the Ordinary Linear Regression in case when assumptions are violated.
author:
  - name: Yury Zablotski
    url: https://yuzar-blog.netlify.app/
date: "`r format(Sys.time(), '%B %d, %Y')`"
categories:
  - videos
  - statistics
  - visualization
  - models
#preview: thumbnail_sjPlot.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 6
    code_download: true
bibliography: /Users/zablotski/Documents/library.bib
#csl: american-political-science-association.csl
biblio-style: apalike
link-citations: yes
linkcolor: blue
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```


If we want to compare salaries of people with different education, we could create a simple linear model and plot the results. But we can not trust these results, without checking the assumptions of our model, because if assumptions are violated, the results might be wrong. However, the problem with assumptions is that there are too many, and most of the time we can't satisfy them all. 


```{r}
library(tidymodels)   # for everything good in R ;)
library(ISLR)         # for "Wage" dataset

set.seed(9999)        # makes sure we get the same 100 lines
salary <- Wage %>%
  sample_n(100)

m <- lm(wage ~ education, salary)

library(sjPlot)      # I have a video on "sjPlot" ðŸ“¦
theme_set(theme_test())
plot_model(m, type = "pred", show.data = TRUE, jitter = TRUE)
```


For instance, our model has not-normally distributed residuals, the variances between groups differ and we have heteroskedasticity. So, we can't trust our results, without fixing them, but when we start fixing assumptions, we might (1) lose data (if we remove some influential points), (2) reduce interpretability (when we log-transform data) or screw up other assumptions even more, because data manipulation constantly changes the **parameters** of our - **parametric** model. And here is where **non-parametric bootstrap** comes into play. 


```{r}
library(performance) # I have a video on "performance" ðŸ“¦ 

check_normality(m)          # Shapiro-Wilk normality test
check_normality(m) %>% plot()
check_homogeneity(m)        # Bartlett test 
check_homogeneity(m) %>% plot()
check_heteroscedasticity(m) # Breusch-Pagan test
check_heteroscedasticity(m) %>% plot()
```


## Why should we nonparametrically bootstrap regressions?

```{r fig.height=9}
check_model(m)
```



The 4 reasons a bootstrapped model is better then usual linear model, is that bootstrapping:

1. does not have any distributional assumptions (such as normally distributed residuals or equal variances among groups)

2. bootstrapped models provide more accurate inferences, e.g. confidence intervals [^1], which we'll soon prove on two examples, one with categorical and one with numeric predictors

3. it works better for small samples and finally 

4. it describes variation in the data much better than traditional linear models

Therefore, having bootstrapped models in your tool-box will definitely make you a better data-scientist! So ...

## What is - a bootstrap?

In order to better understand how bootstrap models work, we need to understand what the bootstrap actually is. A bootstrapp is the little loop on the back of a boot to help you pool it on. And the phrase "pulling oneself up by oneâ€™s bootstraps" means to succeed without any external help. For data it simply means - resampling.

```{r}
library(ggstatsplot) # I made many videos on "ggstatsplot" ðŸ“¦
ggbetweenstats(data = salary, x = education, y = wage)
```

So, lets (1) take a very small dataset, (2) produce model coefficients & 95% CIs with the **bootstrap** method which **does not rely on assumptions** and (3) compare them to the coefficients of a **usual linear model** which totally **relies on assumptions**, in order to find out whether bootstrapping result is indeed better!

## 4 steps to compute bootstrapped regression

There are only 4 steps to conduct and visualize bootstrapped regression. 

### 1. Bootstrap the data with resampling

First, we bootstrap the data, which simply means we take 1000 samples from our 100 data rows. Every of those 1000 samples is of the same size as the original data set, namely 100 rows, but is made using **replacements**, which results into multiple replicates of some of the original rows of the data. **Replacements are necessary, because we would otherwise simply reproduce the original sample**. (The assessment set contains the rows of the original data that were not included in the bootstrap sample.) In order to distinguish bootstrapped samples from the original sample let's call our bootstrapped samples - splits. 


```{r}
set.seed(123)
# 1000 samples are OK, but with 2000-10000 you are save!
boot_data <- bootstraps(salary, times = 1000) 
boot_data
boot_data$splits[[1]]
```

### 2. Produce 1000 models

Then we **fit a linear model** to every **split** using the first `map()` function and tidy up model coefficients with the second `map()` function. Let's use "0 +" in the model formula to remove the Intercept, because we are interested in estimates of the salary and not the slope of change from that Intercept.

```{r}
boot_models <- boot_data %>% 
  mutate(model = map(splits, ~lm(wage ~ 0 + education, data = .)), 
         coefs = map(model, tidy) ) # optional "conf.int = TRUE"

boot_models
```


### 3. Plot estimates distribution of 1000 estimates

```{r}
boot_coefs <- boot_models %>%
  unnest(coefs) %>% 
  select(term, estimate)

boot_coefs
```

Now, we not only have 1000 models, which are nested in the column "model", but also the results of those models which are laready nested in the column "coefficients". And if we `unnest()` the coefficients, we'll see 5 estimates for every of our 1000 models. It's like we have done 1000 experiments, which we can immediately visualize as a distribution.

```{r fig.width=9}
boot_coefs %>%
  ggplot(aes(x = estimate)) +
  geom_histogram() +
  facet_wrap(~term, scales = "free")+
  theme_test()
```
First of all, the distribution seems normal, which is amazing ;). But the coolest thing about this distribution is that we can easily get not only classic 95% Confidence Intervals, **but any intervals we want**. Which is kind of hard to get out of the ordinary model and are less trusted then Bayesian methods. Thus, we have a fifth - bonus ðŸ¥³ - advantage of the bootstrapped model. 

For example:

- let's find out where are 50% of possible salaries for every group, 
- visualize the distribution as a density curve instead of histogram, 
- add the median as our central line, 
- plus 50% and 95% confidence intervals. 

Beautiful, isn't it? Here we can clearly see that people who did not finish a high school, have a 50% change to earn around 95 thousand dollars, while folks who finished advanced degree would have 50% chance to earn around 150 thousands. Waw!

```{r fig.width=9}

boot_meds <- boot_coefs %>% 
  group_by(term) %>% 
  # term here is useless, but you'll need for >1 predictor
  summarise(
    med_est   = median(estimate),
    conf.low  = quantile(estimate, 0.025),
    conf.high = quantile(estimate, 0.975),
    conf.25   = quantile(estimate, 0.25 ),
    conf.75   = quantile(estimate, 0.75 ))

boot_meds

boot_coefs %>%
  ggplot(aes(x = estimate)) +
  geom_rect(aes(x = med_est, xmin = conf.low, xmax = conf.high, ymin = 0, ymax = Inf), data = boot_meds, alpha = 0.1, fill = "green") +
  geom_rect(aes(x = med_est, xmin = conf.25, xmax = conf.75, ymin = 0, ymax = Inf), data = boot_meds, alpha = 0.3, fill = "green") +
  geom_density() +
  geom_vline(data = boot_meds, aes(xintercept = med_est))+
  facet_wrap(~term, scales = "free")+
  theme_test()
```
But what blew my mind even more the first time I learned to bootstrap regression estimates, is that I can even get a distribution of p-values :). For that we just (1) remodel 1000 ... ahhh, what a hell, let's remodel 10.000 models with the Intercept, (2) unnest coefficients again, (3) get median p-values (not average p-values!!!) for every group and (4) compare them to p.values from the ordinary models.

```{r fig.width=9}
boot_models <- bootstraps(salary, times = 10000) %>% 
  mutate(model = map(splits, ~lm(wage ~ education, data = .)), 
         coefs = map(model, tidy) ) # optional "conf.int = TRUE"

boot_coefs <- boot_models %>%
  unnest(coefs) %>% 
  select(term, p.value)

boot_coefs %>%
  ggplot(aes(x = p.value)) +
  geom_histogram() +
  facet_wrap(~term, scales = "free")+
  theme_test()
```


```{r}
boot_coefs %>% 
  group_by(term) %>% 
  summarise(
    med_boot_p.value  = median(p.value)
    ) %>% 
  left_join(tidy(m) %>% select(term, p.value)) %>% 
  mutate_if(is.numeric, ~round(., 4))
```

And the results are ... only slightly different. However, I intuitively trust the bootstrapped p.values more, because they were produced in a statistically different fashion I know - we did not violate any assumptions.

### 4. Get and visualize predictions

Finally, here is the moment we've been waiting for: 

- let's use the augment command to produce predictions for every of our 1000 models, 
- then calculate a median and 95% CIs from our predictions and 
- compare them to the predictions made by the ordinary parametric linear regression:

```{r}
# get predictions
boot_aug <- boot_models %>% 
  mutate(augmented = map(model, augment)) %>% 
  unnest(augmented) %>% 
  select(-splits, -model)

# get median estimates, median 95% CIs
nonpar_med_boot_preds <- boot_aug %>% 
  group_by(education) %>% 
  summarise(
    predicted = median(.fitted ),
    conf.low  = quantile(.fitted, 0.025),
    conf.high = quantile(.fitted, 0.975)) %>% 
  mutate(model = "bootstrapped") %>% 
  select(model, everything())

# get mean estimates from the classic model
m <- lm(wage ~ education, salary)  # just a reminder
par_avg_preds <- ggeffects::ggeffect(m, terms = "education") %>% 
  tibble() %>% 
  mutate(model = "linear") %>% 
  select(model, education = x, predicted, conf.low, conf.high)

# combine two datasets
preds <- rbind(nonpar_med_boot_preds, par_avg_preds)

# plot row data and predicted CIs from both models
ggplot() +
  geom_jitter(data = salary, aes(education, wage), width = .2, alpha = 0.2, size = 2)+
  geom_pointrange(data = preds, aes(x=education, y=predicted, 
                                    ymin=conf.low, ymax=conf.high, color = model),  
                  position=position_dodge(width=.6), size = 1) + 
  theme_test()
```

Interestingly, the center of the bootstrap distribution is identical to the mean of the sample, because the bootstrap distributions are centered around statistics calculated from the data (e.g., a regression slope). However, the bootstrapped confidence intervals are slimmer where the variance is low and wider, where the variance is big. So, as mentioned in the beginning, **bootstrapped result describe the variation in the data better and produce therefore more accurate and realistic inferences without violating any assumptions and especially for small samples.**

## Bootstrapping numeric predictors

Bootstrapping with numeric predictors works in the same way, and it's an good way to repeat all we learned so far, namely: 

- we use already created 1000 resamples of the data,
- but `map()` over it to fit new 1000 models, with the numeric predictor "age"
- then `map()` over every model and use `augment()` function to extract fitted data for every model in an new nested column, with 100 fitted values for every model, because every resample in boot_data has 100 observations.
- 1000 models with 100 fitted values each result in 100.000 fitted values, which we see when we unnest them for plotting, and will be visualized as 1000 lines on the plot


```{r}
# bootstrap new regressions and unnest all fits
boot_models <- boot_data %>% 
  mutate(model     = map(splits, ~lm(wage ~ age, data = .)), 
         augmented = map(model, augment) )

boot_models

boot_aug <- boot_models %>% 
  unnest(augmented) %>% 
  select(-splits, -model)

boot_aug

ggplot(data = boot_aug, aes(x = age, y = wage)) +
  geom_line(aes(y = .fitted, group = id), col = "grey",  size = 0.25) 
```

- we then create new age data, from 22 to 71 years old, to evaluate predictions
- use `map()` to predict salaries for the new age data by every model
- and then summarize all 1000 predictions by median and calculate 95 and 50% confidence intervals (CIs),
- and plot them on top our our 1000 fitted models

```{r}
# get new data for predictions
new <- data.frame(age = seq(22, by = 3, 71))
new

few_preds <- boot_models %>% 
  mutate(few_preds = map(model, predict, new)) %>% 
  unnest(few_preds) %>% 
  select(-splits, -model) %>% 
  mutate(age = rep( seq(22, by = 3, 71) , 1000)) %>% 
  group_by(age) %>% 
  summarise(wage   = median(few_preds),
            LCL_50 = quantile(few_preds, 0.25),
            UCL_50 = quantile(few_preds, 0.775),
            LCL    = quantile(few_preds, 0.025),
            UCL    = quantile(few_preds, 0.975),
  )

few_preds

ggplot(data = boot_aug, aes(x = age, y = wage)) +
  geom_line(aes(y = .fitted, group = id), col = "grey",  size = 0.25) +
  geom_errorbar(data = few_preds, aes(ymin = LCL, ymax = UCL), width = 1)+
  geom_errorbar(data = few_preds, aes(ymin = LCL_50, ymax = UCL_50))
```

- plot the original 100 data points in green and
- plot the results of an ordinary linear regression in blue with their 95% CIs in red and finally

```{r}
ggplot(data = boot_aug, aes(x = age, y = wage)) +
  geom_point(data = salary, aes(age, wage), colour = "green", shape = 1) +
  geom_line(aes(y = .fitted, group = id), 
            col = "grey",  size = 0.25) +
  geom_errorbar(data = few_preds, aes(ymin = LCL, ymax = UCL), width = 1)+
  geom_errorbar(data = few_preds, aes(ymin = LCL_50, ymax = UCL_50))+geom_smooth(aes(x = age, y = wage), data = salary, method = "lm",
              colour = "blue", fill = "red", alpha = 0.25)
```


As we can see again, low variance in salaries in the beginning of professional life is described with narrower CIs by the bootstrapped predictions as compared to the ordinary linear regression, while larger variance in salaries after age of 45 is described by the bootstrapped predictions with wider CIs as compared to the ordinary linear regression. So, similarly to the categorical predictor, we see that **bootstrapped result describe the variation in the data better and produce therefore more accurate and realistic inferences.**


## Problems

So, is bootstrap method perfect? Of coarse no! While bootstrap more accurately describes variance of a sample, for example here I really want to include the high salaries into my variance, to see what is possible to earn with this education, if you have real outliers or very influential observations in your data, they will be given more weight then you might wont to. In this case you would use a **robust regression**, and if you wanna became a more complete data scientist in the next five minutes, watch [this video](in progress).  


```{r}
library(robustbase)
rm <- lmrob(wage ~ education, salary)
rob_avg_preds <- ggeffects::ggeffect(rm, terms = "education") %>% 
  tibble() %>% 
  mutate(model = "robust2") %>% 
  select(model, education = x, predicted, conf.low, conf.high)

# combine two datasets
preds <- rbind(nonpar_med_boot_preds, par_avg_preds, rob_avg_preds)

ggplot() +
  geom_jitter(data = salary, aes(education, wage), width = .2, alpha = 0.2, size = 2)+
  geom_pointrange(data = preds, aes(x=education, y=predicted, 
                                    ymin=conf.low, ymax=conf.high, color = model),  
                  position=position_dodge(width=.6), size = 1) + 
  theme_test()
```


## Further readings and references


[^1]: Tim Hesterberg (2015), What Teachers Should Know about the Bootstrap: Resampling in the Undergraduate Statistics Curriculum, The American Statistician 69(4) 371-386, DOI: 10.1080/00031305.2015.1089789


---

If you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.

**Thank you for learning!**
