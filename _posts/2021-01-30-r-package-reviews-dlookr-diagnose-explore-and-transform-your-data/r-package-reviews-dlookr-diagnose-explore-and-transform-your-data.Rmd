---
title: "R package reviews {dlookr} diagnose, explore and transform your data"
description: |
  Raw data need to be diagnosed for existing problems, explored for new hypotheses and repaired in order to increase data quality and output. The {dlookr} package makes these steps fast and easy. {dlookr} generates automated reports and performs compex operations, like imputing missing values or outliers, with simple functions. Moreover, {dlookr} collaborates perfectly with {tidyverse} packages, like {dplyr} and {ggplot2} to name just a few!
author:
  - name: Yury Zablotski
    url: https://yury-zablotski.netlify.app/
date: 01-30-2021
categories:
  - EDA
  - videos
  - data wrangling
  - R package reviews
  - visualization
  
preview: dlookr_thumbnail.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 5
    code_download: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```


```{r}
library(tidyverse) # for almost everything ;)
library(flextable) # for beautifying tables
library(dlookr)    # for the main event of the evening ;)

citation("dlookr")
```


## Chapter 1: DIAGNOSE your data

### Generall diagnosis

We can think of **diagnosing** our data similarly to **diagnosing** a disease - we just try to figure out what is wrong. First of all, we can check whether a **type** of variables is correct. For instance, if we see a **text** or a **factor** for a clearly numeric variable - Temperature, we would be alarmed and would know that we need to fix it. Then, we can immediately see which variable has **missing values** and **how many**. Lastly, we can see how many **unique values** do we have in every variable, which is particularly useful for categorical variables, for instance, we could immediately see if some of values were misspelled and are now in several instead of only one category.


```{r}
diagnose(airquality) %>% flextable()
```

### Categorical variables

Speaking of categorical variables: using **diagnose_category()** function we can **diagnose** all categorical variables from our dataset at once. Such **diagnosis** reveals **names** of categories, their **counts** and **percentages** and even **ranking number** from the biggest category to the smallest.


```{r}
diagnose_category(diamonds) %>% flextable()
```

### Numeric variables

Diagnosing all numeric variables at once is similarly easy. **diagnose_numeric()** function calculates not only most common descriptive statistics, like **minimum, first Quartile, average, median, third Quartile** and **maximum**, but also gives you the number of **zeros, negative values** and even the **number of potential outliers** for every numeric variable.

```{r}
diagnose_numeric(diamonds) %>% flextable()
```

As mentioned before, {dlookr} happily collaborates with two of the most used {tidyverse} packages: {dplyr} and {ggplot2}, which is simply amazing!

```{r}
diagnose_numeric(diamonds) %>% 
  filter(minus > 0 | zero > 0) %>% 
  select(variables, median, zero:outlier) %>% 
  flextable()
```

### Outliers

By the way **outliers**, how would you **diagnose** them? Of coarse you would **count** them and get their **percentages** in every variable, right? Sure! Well {dlookr} does it for you. Moreover, it calculates three different averages: the **mean** of every variable **with outliers, without outliers** and the **mean of the outliers themselves**. So, you can see how strong the influence of outliers for every variable is. For instance the variable "depth" in "diamonds" data has over 2500 outliers. That's a lot! However, the **means with and without outliers** are almost identical. Besides, the **average of the outliers themselves** is very similar to the original average of the whole data. In contrast, the variable "price" with over 3500 outliers is heavily influenced by them. The **average of the outliers** is almost 5 times higher, than the **average without them**. That's deep enough diagnoses of outliers already. And if that weren't enough, {dlookr} can visualize the distribution of data with and without outliers. Let's plot them!

```{r}
diagnose_outlier(diamonds) %>% flextable()
```


First of all we'll quickly find the variables (or columns) with outliers using **find_outliers()** function. It gives you the index of the column. For example in "iris" dataset the second column contains outliers. Using the argument **index = FALSE** provides the name of the column, so that you can select only columns with outliers and plot them by a simple **plot_outlier()** function. If we don't specify any columns, **plot_outlier()** function will plot each numeric variable from your dataset. This plot displays the distribution of data **with and without outliers** in the form of box plots and histograms, so that we directly **see how our data changes if we remove outliers**.

```{r}
find_outliers(iris)
find_outliers(iris, index = F)

iris %>% 
  select(Sepal.Width) %>% 
plot_outlier()

plot_outlier(ISLR::Default)
```

If we remove outliers, they kind of become **missing values**, despite the fact that they weren't missing in the beginning. And if we want to diagnose our data properly, we need to deal with **missing values** too. And as always, the best way to deal with any data problem is to **visualize it**. 

### Missing Values

Well amazingly, {dlookr} not only can visualize outliers, but can also do this in **three different ways**. The first one - is the **Pareto chart**. 

```{r fig.height=5}
plot_na_pareto(airquality)
```

In a **Pareto Chart counts and proportions of missing values** are represented in descending order by **bars**, and the **cumulative total** is represented by the **line**. It even tells you what the amount of missing values **means**, namely, missing around 24% of observations is still **OK**, while missing more then 40% of values would be **bad**. If you have a lot of variable and wanna display only the ones with missing values, use **only_na = TRUE** argument. The only problem with pareto plot is that we don't know whether missing values in different columns belong to the same observation.


```{r fig.height=5}
airquality %>% 
  plot_na_pareto(only_na = TRUE)

airquality %>% 
  plot_na_pareto(only_na = TRUE, plot = FALSE) %>% flextable()
```

But that's where the second type of visualization of missing values comes into play - **plot_na_hclust()** - which shows you how missing values are **distributed** and whether there are **overlapping** of variables between then. The only which concerns me here was that overlapping itself could be hard to see if we'll get many variables with missing values, some with few and some with a lot of missing values.

```{r}
plot_na_hclust(airquality)
```

Third method of plotting missing values solves this problem. **plot_na_intersect()** function visualizes the **combinations of missing value across columns**. The **x-axis** shows the variables with missing values and the counts of missing values on the top of the plot as a bar graph. The **y-axis** represents the **combination of variables and their frequencies**. For instance, we see, that two of the observations are missing in both variables. We can also display complete cases if we want to.

```{r}
plot_na_intersect(airquality)	
```

### Reporting

**diagnose_report()** function combines most of what we just learned (but not all!) into one **PDF** or **HTML document** in seconds. Just run the line of code below and explore it. But the way - **exploring** - is the second thing {dlookr} package absolutely nails! So, let's get straight to it.

```{r eval=F}
diagnose_report(airquality) # pdf or html
```



## Chapter 2: EXPLORE your data

### Describtive statistics

```{r}
# all numeric variables
describe(iris) %>% flextable()

# selected numeric variables
# describe(airquality, Ozone, Temp)
```

**describe()** function provides **descriptive statistics of ALL numeric variables at once** in your dataset, if you don't specify any. Among them are **standard deviation,  interquartile range, skewness and many quantiles**. However, **describe()** function becomes even more useful by **collaborating with {dplyr}** package. Particularly, we can **group the data by any categorical variable** and get **descriptive stats for each separate category**.

```{r}
# per category
iris %>% 
  group_by(Species) %>% 
  select(Sepal.Length) %>% 
  describe() %>% flextable()
```



### Normality Test



```{r}
normality(iris) %>% flextable()
```


**normality()** function performs a **Shapiro-Wilk normality test on ALL numeric variables at once**, if non are specified. When the number of observations is greater than 5000, then 5000 observations are randomly selected. The ability of this function to work with {dplyr} helps here too, since we always need to **check normality of groups**, not only of the whole variable. The code you see below consists of **just 4 words**, but conducts 12 normality tests, namely for 3 categories in 4 numeric variables. Now, imagine a dataset with 100 numeric variables and 100 categories you wanna check the normality for. It would still take only 4 words, but would conduct 10.000 tests.

```{r}
iris %>% 
  group_by(Species) %>% 
  normality() %>% 
  flextable()
```

For comparison, here is the simplest example of the code I used for the same task before {dlookr}. Here I would need to:

- transform the data first
- "nest" the values
- use "map" function two times, which has nothing to do with "normality"
- use strange symbols like "~" (*tilde*) and ".x$"
- "tidy" up the result
- "unnest" the result and even 
- get rid of some useless columns to get the same table we have seen above

Don't get me wrong, I am a huge fan of {dplyr}! But some tasks can be simplified even beyond {dplyr} and that's just the beautiful.

```{r }
iris %>% 
  pivot_longer(cols = 1:4) %>% 
  nest(value) %>% 
  mutate(
    test   = map(data, ~ shapiro.test(.x$value)),
    tidied = map(test, broom::tidy)
  ) %>% 
  unnest(tidied, .drop = TRUE) %>% 
  select(-data, -test) %>% 
  flextable() %>% 
  width(j = "method", width = 2)
```



Moreover, **plot_normality()** function visualizes the normality of numeric data and two most common transformations of data in case the normality assumption was not met. Particularly, we see:

- Histogram of original data
- Q-Q plot of original data
- Histogram of log transformed data and finally
- Histogram of square root transformed data

Now we see whether transformation brings something or not.


```{r}
airquality %>%
  plot_normality(Ozone)
```

### Correlation

In order to quickly **check the relationship between numeric variables** we can use **correlate()** function. If we don't specify any target variables, **Pearson's correlation between ALL variables** will be calculated pairwisely. But let's have a look at only one variable - Ozone. Nice, but **plot_correlate()** function is even more useful, because it **visualizes these relationships**. We can of course determine the **method of calculations**, be it a default "pearson", or a non-parametric "kendall" or "spearman" correlation. The **shape** of each subplot shows the **strength of the correlation**, while the **color** shows the **direction**, where blue is **positive** and red is **negative correlation**.

```{r}
correlate(airquality, Ozone)

plot_correlate(iris, method = "kendall")
```

Here again, adding some {dplyr} code, we can check as many correlations as we want. 

```{r}
diamonds %>%
  filter(cut %in% c("Premium", "Ideal")) %>% 
  group_by(cut) %>%
  plot_correlate(method = "spearman")
```

### Relation

{dlookr} can also check out **other kinds of relationships between two particular**, for example between two categorical, **variables**. You just need to specify the response (or target) variable with a function **target_by()** and the predictor with the function **relate()**. 

#### categorical to categorical

For instance, the influence of predictor "clarity" on our target "cut" in "diamonds" dataset, we'll be analyzed by a **Chi-Square Test for independence**. Plotting them will produce a **mosaic plot** with frequencies of both categorical variables. 

```{r}
diamonds %>% 
  target_by(cut) %>%      # similar to "group_by" only for one variable
  relate(clarity) %>% 
  summary() 


diamonds %>% 
  target_by(cut) %>%      
  relate(clarity) %>% 
  plot()
```


#### categorical to numeric

If our target variable is categorical and our predictor is numeric, simple descriptive stats and density plots will be displayed.

```{r}
iris %>% 
  target_by(Species) %>%      
  relate(Sepal.Length) %>% 
  plot()
```


#### numeric to categorical

If the response variable is numeric, while the predictor is categorical or numeric, **simple linear regression** will be applied. Using **summary()** or **tab_model()** would give you the **model output**, which **coefficients and p-values**. Plotting the categorical predictor would produce a pretty boring **box-plot**, while visualization of the numeric variable would produce a nice **model fit with with confidence intervals**.

```{r}
iris %>% 
  target_by(Sepal.Length) %>%      
  relate(Species) %>% 
  summary()

iris %>% 
  target_by(Sepal.Length) %>%      
  relate(Species) %>% 
  plot()
```
#### numeric to numeric

```{r}
airquality %>% 
  target_by(Ozone) %>%      
  relate(Temp) %>% 
  #summary() 
  sjPlot::tab_model()

airquality %>% 
  target_by(Ozone) %>%      
  relate(Temp) %>% 
  plot()
```


### Reporting

**eda_report()** will produce a **report**, similar to the diagnose report. However, if we here specify the **target variable**, we'll get much richer report. Let's use the **HTML output format** this time.

```{r eval=F}
airquality %>%
  eda_report(
    target        = Temp, 
    output_format = "html", 
    output_file   = "EDA_airquality.html")
```


## Chapter 3: TRANSFORM your data

This chapter is my favorite! To be honest with you, missing values and outliers are like parasites on my data and I just wanna get rid of them. Now, thanks to {dlookr}, I can do it easily and very effective! Check this out.


### Imputation

**Imputation** simply means **replacing a missing value** with a value which makes sense. There are **two kinds of imputation**:

#### 1. Predictor is numeric

- imputation with a **single value**, like 
  - mean 
  - median or 
  - mode, or 
- imputation with by a **machine learning algorithm**, like
  - knn - K-nearest neighbors
  - rpart - Recursive Partitioning and Regression Trees or
  - mice - Multivariate Imputation by Chained Equations (random seed must be set)

**imputate_na()** function needs 4 arguments: 
- data
- name of the variable with missing  values
- name of the predictor variable and 
- the **method** of imputation


Let's use the "airquality" dataset again, and fill out 37 missing values in the variable "Ozone" via the variable "Temperature" using the **average** as a method of imputation.

Just run all the methods available and peak the best for your particular dataset. Because there is no "best" imputation method, just the best method for your individual data. *imputate_na()* support both numeric and categorical variables, and supports the following method. If we sage the output of the function **imputate_na()** in some meaningful name, like "bla", we can **first** have a look at the **summary**. The summary compares some metrix of the Original and New dataset, before and after the imputation accordingly. Particularly, we see that the standard deviation and the standard error got smaller, which is a good sign! Secondly, let's plot both datasets and compare them. The plot reveals that **distribution** of a new dataset changed dramatically, which is not a good sing at all, because we just made up some data which is far away from the original data, and the original data is the best guess on reality we have. 

So, does imputation makes sense at all? Well, let's use one of the machine learning methods, for example K-nearest neighbors. This plot looks much better, because the distribution did not change much, but some aspects of it were a bit more emphasized. But does this mean, that imputation by a single value is useless? I've heard this opinion before. But I found that usefulness of the imputation strongly depends on the dataset and the best way to use it **in my opinion** is just to compare all imputation methods with each other and take the best, even if it is a single value method. If we run this whole chunk of code, we'll be able to visually choose among methods. For instance, “rpart” - Recursive Partitioning and Regression Trees, does the best job for "Ozone" columns in "airquality" dataset.

Needless to say, the plots can be pimped with some **ggplot2** syntax.


```{r}
# mean
bla <- imputate_na(airquality, xvar = Ozone, yvar = Solar.R, method = "mean")
#summary(bla)
plot(bla)

# median
blup <- imputate_na(airquality, Ozone, Temp, method = "median")
# summary(blup)
plot(blup)+theme_minimal()

# “knn” : K-nearest neighbors
blap <- imputate_na(airquality, Ozone, Temp, method = "knn")
#summary(blap)
plot(blap)

# “rpart” : Recursive Partitioning and Regression Trees
blah <- imputate_na(airquality, Ozone, Temp, method = "rpart")
#summary(blah)
plot(blah)+theme_classic()+theme(legend.position = "top")

# “mice” : Multivariate Imputation by Chained Equations
blup <- imputate_na(airquality, Ozone, Temp, method = "mice", seed = 999)
# summary(blup)
plot(blup)+theme_void()
```

#### 2. Predictor is categorical variable

Only three methods are available for imputation of categorical variables:

- mode - mode
- rpart - Recursive Partitioning and Regression Trees
- mice - Multivariate Imputation by Chained Equations (random seed must be set)

Let's take the **diamonds** dataset, make it a bit smaller, add some NAs and use the **mice** imputation. The plot of the results compares the frequencies of each category, so that we can see which values were imputed. The summary command would give you the **exact counts and proportions of categories before and after imputation**. And already familiar **plot_na_pareto()** function ensures that we have missing values in the old "cut" variable and no missing values in the "new_cut" variable.

```{r}
d <- diamonds %>% 
  sample_n(1000) 

d[sample(seq(NROW(d)), 50), "cut"] <- NA

d2 <- imputate_na(d, cut, price, method = "mice", seed = 999)

plot(d2)

summary(d2)
```


```{r fig.height=5}
d %>% 
  mutate(new_cut = d2) %>% 
  select(cut, new_cut) %>% 
  plot_na_pareto()
```


#### 3. Imputation of outliers

We have seen that {dlookr} package can effectively find outliers and impute missing values. If the outliers are nasty, they can be treated like missing values. Therefore, if desired, the outliers can be easily replaced by the  **imputate_outlier()** function. This function supports the following imputation methods:

- mean
- median
- mode and
- capping - which imputes the upper outliers with 95th percentile, and the bottom outliers with 5th percentile. Here, I would also recommend to compare all 4 methods before deciding which method is the best. And if we look at the plots, I thing a simple average is the best.

```{r}
bla <- imputate_outlier(diamonds, carat, method = "mean")
plot(bla)

bla <- imputate_outlier(diamonds, carat, method = "median")
plot(bla)

bla <- imputate_outlier(diamonds, carat, method = "mode")
plot(bla)

bla <- imputate_outlier(diamonds, carat, method = "capping")
plot(bla)

```



### Binning

*binning()* transforms a numeric variable into a categorical one. The following types of binning are supported:

- quantile, which categorizes using quantiles 
- equal - categorizes to have equal length segments
- pretty - categorizes into moderately good segments (whatever it it :)
- kmeans - categorization using K-means clustering
- bclust - categorization using clustering technique with **bagging**, where **bagging** is an abbreviation for **Bootstrap Aggregating** - a machine learning meta-algorithm designed to improve predictive performance.

The **bclust** sound most fancy, so, let's try this out first. The distribution of the Ozone variable provides 3 intuitive groups:

- the parabola from 0 to 60
- short flat area from 60 to 80 and
- steady decline till the end of the plot

thus, let's set the number of **bins** to 3. Plotting the result shows that our first intuition was correct, the parabola was singled out into a category. However, the third category is kind of small, and has only 7 observations, which we can see using **summary()** command. And we wanna know the exact numbers were the borders of categories were drawn, don't use argument "labels" in the **binning()** command:

```{r}
DataExplorer::plot_density(airquality$Ozone)

# the fanciest one
bin <- binning(airquality$Ozone, type = "bclust", nbins = 3,
              labels = c("cat1", "cat2", "cat3"))

plot(bin)

summary(bin)

binning(airquality$Ozone, type = "bclust", nbins = 3)
```

If we check out all the other categorizations, we can choose the one we feel the best about. I would take the **kmeans**, because it has relatively similar counts of data in every category, while still describing the distribution with three traits of the plot we have seen, namely parabola at the beginning of the plot, plateau in the middle and decline till the end.

```{r}
plot(binning(airquality$Ozone, type = "quantile", nbins = 3,
              labels = c("cat1", "cat2", "cat3")))

plot(binning(airquality$Ozone, type = "equal", nbins = 3,
              labels = c("cat1", "cat2", "cat3")))

plot(binning(airquality$Ozone, type = "pretty", nbins = 3))

plot(binning(airquality$Ozone, type = "kmeans", nbins = 3,
              labels = c("cat1", "cat2", "cat3")))
```
Using a simple {dplyr} syntax we can easily add a new categorized Ozone variable into our dataset.

```{r}
airquality %>% 
  mutate(
    binned_Ozone = binning(airquality$Ozone, type = "quantile", nbins = 3, labels = c("cat1", "cat2", "cat3"))
    ) %>% head() %>% flextable()
```



### Standardization and Resolving Skewness

**transform()** function performs both **Standardization and Resolving Skewness**.

#### 1. Standardization

Standardization is the process of putting different variables on the same scale, so that we can better compare and model them. In order to standardize variables, we first calculate the mean and standard deviation for a variable. Then, for each observed value of the variable, we subtract the mean and divide by the standard deviation. {dlookr} provides two methods for standardization, namely:

- zscore - z-score transformation. (x - mu) / sigma
- minmax - minmax transformation. (x - min) / (max - min)


```{r}
bla <- transform(airquality$Solar.R)

plot(bla)
```




#### 2. Resolving Skewness

...and 6 methods for resolving skewness:

- log - transformation, which is great, but has problem with zeros.
- log+1 - transformation, which is amazing for variables that contain zeros.
- sqrt - square root transformation.
- 1/x - transformation
- x^2 - squared transformation and
- x^3 - power of three transformation

**log+1** is my favorite, because it handle zeros, which produce "Infinities" with a simple **logarithm**.

```{r}
find_skewness(mtcars, index = F)

plot( transform(mtcars$hp, method = "log+1") )
```


### Reporting

Similarly to **diagnosis** and **exploratory reporting**, we can produce a **transformation report** using a single intuitive command.

```{r eval = F}
transformation_report(airquality, target = Temp)
```


## Conclusion

I hope {dlookr} package provides some value for you. It certainly made my life easier, that's why I wanted to share it with you. I am super curious what R package you find the most useful? All right, that's it for today. If you liked this video and want to see more, be sure to like and subscribe. I wanna thank you guys for watching and I'll catch you again soon in the next video.

---------------

If you think, I missed something, please comment on it below, and I’ll improve this tutorial.

# Useful ressources

- The best place to start is actually the Distill website itself: <https://rstudio.github.io/distill>.
