---
title: "R package reviews {emmeans} squizze all the knowledge out of the models!"
description: |
  {emmeans} is one of the most capable, but at the same time one of the most mysterious and therefore underrated R packages. Let's demistify {emmeans} and uncover it's power!
author:
  - name: Yury Zablotski
    url: https://yuzar-blog.netlify.app/
date: "`r format(Sys.time(), '%B %d, %Y')`"
categories:
  - videos
  - statistics
  - models
preview: thumbnail_gtsummary.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 6
    code_download: true
bibliography: /Users/zablotski/Documents/library.bib
#csl: american-political-science-association.csl
biblio-style: apalike
link-citations: yes
linkcolor: blue
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

```{r}
library(tidyverse)       # for everything good in R
theme_set(theme_test())  # beautifies plots 
library(emmeans)         # multiply power of your results!
```

# Why do we need {emmeans}?

# Understanding "emmeans"

"emmeans" is an abbreviation for estimated marginal means. And in a linear regression it calculates means of a numeric predictor. However, since it also calculates probabilities, counts etc., the name - {emmeans} is, I think, just an artifact from the old times, where this package wasn't that powerful.

"estimated" is part of the name, since any results of {emmmeans} is estimated from models, not from data

"marginal" - could be seen as average of averages. In a contingency table, the "marginal means" of one variable are the means for that variable averaged across every level of the other variable. For example, a marginal mean of mpg for cyl = 4 is 25.4 averaged over 0 and 1 levels of "am". The same is with a average of mpg for am = 0, which is 19, which was averages over the means of all three types of cylinders. 

```{r}
d <- mtcars %>% 
  mutate(cyl = factor(cyl),
         am  = factor(am),
         gear= factor(gear))

with(d, tapply(mpg, list(cyl, am), mean)) %>% 
  addmargins(FUN = mean)
```
This is called reference grid:

- For each predictor that is a factor, use its levels.
- For each numeric predictor (covariate), use its average

Estimated marginal means.Once the reference grid is established, we can consider using the model to estimate the mean at each point in the reference grid.

```{r}
m <- lm(mpg ~ cyl + am + hp, d)
ref_grid(m)
mean(d$hp)
```

But that's just an example, let's take it slowly one step at the time

# 1. one categorical predictor

```{r}
m <- lm(mpg ~ cyl, d)
library(sjPlot) # I made a video on this ðŸ“¦
plot_model(m, type = "pred")

ref_grid(m)
emmeans::emmeans(m, pairwise ~ cyl)
```


# 2. one numeric predictor


```{r}
m <- lm(mpg ~ hp, d)

plot_model(m, type = "pred")
tab_model(m)

ref_grid(m)
emmeans(m, ~ hp)
```

# Altering the reference grid

while an average mpg estimate per cyl category is nice,
a single average mpg for a single average predictor value hp
is not that informative, however, if we use at least two of
them, for example the lowest and the highest, we see the slope
we see on the plot and see why it is significant, it differst a lot.


```{r}
ref_grid(m, cov.reduce = range)
emmeans(m, ~ hp, cov.reduce = range)
emmeans(m, ~ hp, cov.reduce = range) %>% plot()
```

moreover, instead of a range we can specify any
particular values of predictor

```{r}
ref_grid(m, at = list(hp = c(100, 200, 300)))
emmeans(m, ~ hp, at = list(hp = c(100, 200, 300)))
emmeans(m, ~ hp, at = list(hp = c(100, 200, 300))) %>% 
  plot()
```

which can be very useful for non-linear models. Let me show
you an example, if we have a closer look at the data, we'll
notice a non-linear, namely quadratic, pattern.
plot_model(m, type = "pred", show.data = T)

which we can depict using second polynomial degree: 

```{r}
m1 <- lm(mpg ~ poly(hp, 2), d) 
plot_model(m1, type = "pred", show.data = T)
```


the significance of both degrees tells us that we are right

```{r}
tab_model(m1)
```


now we can use three particular values to take three slices
of hp and since the CIs of two last would overlap, we can 
can assume those are not different.

```{r}
emmeans(m1, ~ hp, at = list(hp = c(100, 200, 300))) %>% 
  plot()
```


using emmeans and "specs" argument, we can compare those emmeans
and see that there is no difference between hp = 200 and 300

```{r}
emmeans(m1, specs = pairwise ~ hp, 
        at = list(hp = c(100, 200, 300)))
```



# 3. two categorical predictors without interactions

```{r}
m <- lm(mpg ~ am + cyl, d)
ref_grid(m)
```


Results are averaged over the levels of: am

```{r}
emmeans::emmeans(m, pairwise ~ cyl)
```


contrasts are identical, but emmeans not

```{r}
emmeans(m, pairwise ~ cyl, at = list(am = "0"))
emmeans(m, pairwise ~ cyl, at = list(am = "1"))

emmeans(m, pairwise ~ cyl, at = list(am = "0"))$emmeans
emmeans(m, pairwise ~ cyl, at = list(am = "1"))$emmeans
```


Now I understand this: Results are averaged over the levels of: am
(24.8+27.4)/2 = 26.1
(18.6+21.2)/2 = 19.9
(14.7 +17.3)/2= 16






# 4. one categorical + one numeric predictor without interactions

In models with covariates, EMMs are often called adjusted means.

```{r}
m <- lm(mpg ~ cyl + hp, d)
ref_grid(m)
```


Results are for the average hp = 146.69

```{r}
emmeans(m, pairwise ~ cyl)
```


contrasts are identical, but emmeans not
here is the prove

```{r}
emmeans(m, pairwise ~ cyl, at = list(hp = 146.69))$emmeans
```


Results are averaged over the levels of: hp 
we can't determine the levels of covariate, it'll be averaged

```{r}
emmeans(m, pairwise ~ cyl, at = list(hp = c(46.69, 246.69)))$emmeans
```


but we can determine one value of covariate

```{r}
emmeans(m, pairwise ~ cyl, at = list(hp = 500))$emmeans
```



# 5. multiple predictors without interactions

```{r}
m <- lm(mpg ~ cyl + hp + am + disp + gear, d)

ref_grid(m)
ref_grid(m)@grid
```


Results are averaged over the levels of: am, gear 

```{r}
emmeans::emmeans(m, pairwise ~ cyl)
library(gtsummary) # I made a video on this ðŸ“¦
tbl_regression(m, add_pairwise_contrasts = T, 
               pvalue_fun   = ~style_pvalue(.x, digits = 3))

```

determine different values then reg_grid using "at"

```{r}
emmeans(m, pairwise ~ cyl, 
        at = list(
          hp = 100, am = "1", disp = 100, gear = "5")
        )$emmeans

```

contrasts still stay the same







# 6. two categorical predictors WITH interactions

```{r}
m <- lm(mpg ~ am * cyl, d)
ref_grid(m)

emmip(m, cyl ~ am, CIs = TRUE)
```


not cool

```{r}
emmeans(m, pairwise ~ cyl)
```


NOTE: Results may be misleading due to involvement in interactions

cooler
determine different values then reg_grid using "at"


```{r}
emmeans(m, pairwise ~ cyl | am, 
        at = list(am = "0")
)

emmeans(m, pairwise ~ cyl | am, 
        at = list(am = "1")
)
```

very cool

```{r}
emmeans(m, pairwise ~ cyl | am)
emmeans(m, pairwise ~ am | cyl)

```





# 7. Interactions with covariates

```{r}
m <- lm(mpg ~ cyl * hp, d)
ref_grid(m)
emtrends(m, pairwise ~ cyl, var = "hp")
emtrends(m, pairwise ~ cyl, var = "hp")[[1]] %>% plot()

# plotting and getting contrasts
plot_model(m, type = "int")
emmeans(m, pairwise ~ hp|cyl,  cov.reduce = range)

emmeans(m, pairwise ~ cyl|hp,  cov.reduce = range)

ref_grid(m, cov.reduce = range) %>% plot(by = "hp")

emmip(m, cyl ~ hp, CIs = TRUE, 
      cov.reduce = range, dodge = 50) 
```


# 8. one interaction + more predictors

```{r}
m <- lm(mpg ~ am * cyl + hp + gear + disp, d)
ref_grid(m)

# cooler determine different values then reg_grid using "at"
emmeans(m, pairwise ~ cyl | am, 
        at = list(am = "0", hp = 146.6875, disp = 230.7219)
)$emmeans

emmeans(m, pairwise ~ cyl | am, 
        at = list(am = "1", hp = 146.6875, disp = 230.7219)
)$emmeans

# contrasts here are amazingly still stay the same ;)

# very cool
emmeans(m, pairwise ~ cyl | am)$emmeans

# determine different values then reg_grid using "at"

emmeans(m, pairwise ~ cyl | am, 
        at = list(am = "1", hp = 300, disp = c(100, 200, 300), gear = "5")
)$emmeans %>% plot()

emmip(m, cyl ~ am, CIs = TRUE) 

mtcars.rg <- ref_grid(m,
                      at = list(hp = 300, 
                                disp = c(100, 200, 300)))
mtcars.rg

# plotting 
plot(mtcars.rg, by = "disp")

```




# 9. Interactions with covariates + other predictors

```{r}
library(ISLR)
m <- lm(wage ~ age*education + jobclass, data = Wage)

ref_grid(m)

emtrends(m, pairwise ~ education, var = "age")$emtrends

emmip(m, education ~ age, CIs = T, 
      cov.reduce = range)

# nothing changes on the 5 slopes, doesn't matter what jobclass level I take, and whether I take one

emtrends(m, pairwise ~ education, var = "age", 
        at = list(jobclass = "1. Industrial")
)$emtrends
```



# 10. several interactions

```{r}
m <- lm(mpg ~ am * cyl + hp * gear, d)
ref_grid(m)

emmeans(m, pairwise ~ cyl | am)
emmip(m, am ~ cyl, CIs = T)

emtrends(m, ~ gear, var = "hp", cov.reduce = range)
emmip(m, gear ~ hp, CIs = T, cov.reduce = range)

```


# 11. plotting p-values

```{r}
m <- lm(mpg ~ cyl * am, data = d)

m_emmeans <- emmeans(m, pairwise ~ cyl | am)
m_emmeans

# This matrix shows the EMMs along the diagonal, P values in the 
# upper triangle, and the differences in the lower triangle.

pwpm(m_emmeans[[1]])

# 28.1-20.6

pwpm(m_emmeans[[1]], adjust = "fdr")

```

# ??? Effect size: Cohen effect sizes are close cousins of pairwise differences

```{r}
eff_size(
  m_emmeans, 
  sigma = sqrt(mean(sigma(m)^2)), 
  edf   = df.residual(m) )
```


# Graphical comparisons

```{r}

plot(m_emmeans, comparisons = TRUE)

pwpp(m_emmeans[[1]], type = "response")+ # by = "am"
  geom_vline(xintercept = 0.05, linetype = 2) 
```













This plot illustrates, as much as anything else, how silly it is 
to try to predict mileage for a 4-cylinder car having high 
displacement, or an 8-cylinder car having low displacement. 
The widths of the intervals give us a clue that we are extrapolating.




# ??? Using weights?  HOW???

```{r}
# same
m <- lm(mpg ~ am + cyl, d)
sapply(c("equal", "prop", "outer", "cells", "flat"), function(w)
  emmeans(m, ~ cyl * am, weights = w, type = "response")) %>% 
  as.data.frame() %>% 
  select(equal.cyl, equal.am, contains("emmean")) # prob

# differ
m <- lm(mpg ~ am + cyl + hp + gear, d)
sapply(c("equal", "prop", "outer", "cells", "flat"), function(w)
  emmeans(m, ~ cyl * am, weights = w, type = "response")) %>% 
  as.data.frame() %>% 
  select(equal.cyl, equal.am, contains("emmean")) # prob

# same
m <- lm(mpg ~ am * cyl, d)
sapply(c("equal", "prop", "outer", "cells", "flat"), function(w)
  emmeans(m, ~ cyl * am, weights = w, type = "response")) %>% 
  as.data.frame() %>% 
  select(equal.cyl, equal.am, contains("emmean")) # prob

# differ
m <- lm(mpg ~ am * cyl + hp * gear, d)
sapply(c("equal", "prop", "outer", "cells", "flat"), function(w)
  emmeans(m, ~ cyl * am, weights = w, type = "response")) %>% 
  as.data.frame() %>% 
  select(equal.cyl, equal.am, contains("emmean")) # prob
```



# ??? 3 way interactions 

```{r}
noise.lm <- lm(noise/10 ~ size * type * side, data = auto.noise)
anova(noise.lm)

ref_grid(noise.lm)

# WRONG
emmeans(noise.lm, pairwise ~ size)
emmip(noise.lm, type ~ size | side)

# ALSO WRONG
emm_s.t <- emmeans(noise.lm, pairwise ~ size | type)

# Use lots of plots, and think about the results.
car::Anova(noise.lm)

joint_tests(noise.lm)
joint_tests(noise.lm, by = "side")






ref_grid(m, at = list(hp = median(d$hp)))
emmeans(m, ~ hp, at = list(hp = median(d$hp)))
```

# Exotic models (probably extra post)

## Median based regression ;) 

is very useful when hp is not normally distributed.

```{r}
library(quantreg)
mm <- rq(mpg ~ hp, data = d)

emmeans(mm, ~ hp, at = list(hp = median(d$hp)))
```

## Mixed-effects models

## Multinomila models




